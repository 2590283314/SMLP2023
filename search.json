[
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing ProgressMeter\nusing Random\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\nProvide a short alias for AlgebraOfGraphics.\nconst AOG = AlgebraOfGraphics;"
  },
  {
    "objectID": "kb07.html#data-set-and-model",
    "href": "kb07.html#data-set-and-model",
    "title": "Bootstrapping a fitted model",
    "section": "Data set and model",
    "text": "Data set and model\nThe kb07 data (Kronmüller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\n\ncontrasts = merge(\n  Dict(nm =&gt; EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm =&gt; Grouping() for nm in (:subj, :item)),\n);\n\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 883   Time: 0:00:01 ( 1.38 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6729\n77.2879\n28.23\n&lt;1e-99\n301.7871\n362.0938\n\n\nspkr: old\n67.7484\n18.2976\n3.70\n0.0002\n43.1536\n40.6843\n\n\nprec: maintain\n-333.9212\n47.1387\n-7.08\n&lt;1e-11\n62.0318\n246.8048\n\n\nload: yes\n78.7702\n19.5373\n4.03\n&lt;1e-04\n65.1901\n42.3299\n\n\nspkr: old & prec: maintain\n-21.9655\n15.8058\n-1.39\n0.1646\n\n\n\n\nspkr: old & load: yes\n18.3844\n15.8058\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5340\n15.8058\n0.29\n0.7742\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6073\n15.8058\n1.49\n0.1353\n\n\n\n\nResidual\n668.4871\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91075.4527\n301.7871\n\n\n\n\n\n\nspkr: old\n1862.2358\n43.1536\n+0.78\n\n\n\n\n\nprec: maintain\n3847.9447\n62.0318\n-0.59\n+0.03\n\n\n\n\nload: yes\n4249.7506\n65.1901\n+0.36\n+0.82\n+0.53\n\n\nitem\n(Intercept)\n131111.9379\n362.0938\n\n\n\n\n\n\nspkr: old\n1655.2134\n40.6843\n+0.44\n\n\n\n\n\nprec: maintain\n60912.6143\n246.8048\n-0.69\n+0.35\n\n\n\n\nload: yes\n1791.8243\n42.3299\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446875.0375\n668.4871\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7126\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.436\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n27\n20\n0.1431\n\n\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, kbm02, to the more complex, kbm01."
  },
  {
    "objectID": "kb07.html#a-bootstrap-sample",
    "href": "kb07.html#a-bootstrap-sample",
    "title": "Bootstrapping a fitted model",
    "section": "A bootstrap sample",
    "text": "A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\n\nRandom.seed!(1234321)\nhide_progress = true\nkbm02samp = parametricbootstrap(2000, kbm02; hide_progress);\n\nOne of the uses of such a sample is to form “confidence intervals” on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nDataFrame(shortestcovint(kbm02samp))\n\n9×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n2028.01\n2337.92\n\n\n2\nβ\nmissing\nspkr: old\n38.431\n99.5944\n\n\n3\nβ\nmissing\nprec: maintain\n-439.321\n-245.864\n\n\n4\nβ\nmissing\nload: yes\n46.0262\n107.511\n\n\n5\nσ\nitem\n(Intercept)\n261.196\n448.51\n\n\n6\nσ\nitem\nprec: maintain\n175.489\n312.051\n\n\n7\nρ\nitem\n(Intercept), prec: maintain\n-0.897984\n-0.445594\n\n\n8\nσ\nsubj\n(Intercept)\n228.099\n357.789\n\n\n9\nσ\nresidual\nmissing\n655.249\n701.497\n\n\n\n\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure 1.\n\n\nCode\ndraw(\n  data(kbm02samp.β) * mapping(:β; color=:coefname) * AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\ndraw(\n  data(\n    filter(\n      :column =&gt; ==(Symbol(\"(Intercept)\")), DataFrame(kbm02samp.σs)\n    ),\n  ) *\n  mapping(:σ; color=:group) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(filter(:type =&gt; ==(\"ρ\"), DataFrame(kbm02samp.allpars))) *\n  mapping(:value =&gt; \"Correlation\"; color=:names) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 3: Density plot of correlation parameters in bootstrap sample from model kbm02"
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "",
    "text": "The speed of MixedModels.jl relative to its predecessors makes the parametric bootstrap much more computationally tractable. This is valuable because the parametric bootstrap can be used to produce more accurate confidence intervals than methods based on standard errors or profiling of the likelihood surface.\nThis page is adapted from the MixedModels.jl docs"
  },
  {
    "objectID": "bootstrap.html#the-parametric-bootstrap",
    "href": "bootstrap.html#the-parametric-bootstrap",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "The parametric bootstrap",
    "text": "The parametric bootstrap\nBootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values. Bootstrapping also belongs to a larger family of procedures called resampling, which are based on creating new samples of data from an existing one, then computing statistics on the new samples, in order to examine the distribution of the relevant statistics.\nA parametric bootstrap is used with a parametric model, m, that has been fit to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.\nThe parameters of a LinearMixedModel object are the fixed-effects parameters, β, the standard deviation, σ, of the per-observation noise, and the covariance parameter, θ, that defines the variance-covariance matrices of the random effects. A technical description of the covariance parameter can be found in the MixedModels.jl docs. Lisa Schwetlick and Daniel Backhaus have provided a more beginner-friendly description of the covariance parameter in the documentation for MixedModelsSim.jl. For today’s purposes – looking at the uncertainty in the estimates from a fitted model – we can simply use values from the fitted model, but we will revisit the parametric bootstrap as a convenient way to simulate new data, potentially with different parameter values, for power analysis.\nFor example, a simple linear mixed-effects model for the Dyestuff data in the lme4 package for R is fit by\n\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\nusing Random\n\nusing AlgebraOfGraphics: AlgebraOfGraphics as AoG\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\nProgressMeter.ijulia_behavior(:clear);\n\nNote that the precise stream of random numbers generated for a given seed can change between Julia versions. For exact reproducibility, you either need to have the exact same Julia version or use the StableRNGs package."
  },
  {
    "objectID": "bootstrap.html#a-model-of-moderate-complexity",
    "href": "bootstrap.html#a-model-of-moderate-complexity",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "A model of moderate complexity",
    "text": "A model of moderate complexity\nThe kb07 data (Kronmüller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors.\n\ncontrasts = Dict(:spkr =&gt; EffectsCoding(),\n                 :prec =&gt; EffectsCoding(),\n                 :load =&gt; EffectsCoding(),\n                 :subj =&gt; Grouping(),\n                 :item =&gt; Grouping())\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :item =&gt; Grouping()\n  :spkr =&gt; EffectsCoding(nothing, nothing)\n  :load =&gt; EffectsCoding(nothing, nothing)\n  :prec =&gt; EffectsCoding(nothing, nothing)\n  :subj =&gt; Grouping()\n\n\nThe EffectsCoding contrast is used with these to create a ±1 encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\nWe can look at an initial fit of moderate complexity:\n\nform = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr + prec + load | subj) +\n                          (1 + spkr + prec + load | item))\nm0 = fit(MixedModel, form, kb07; contrasts)\n\nMinimizing 883   Time: 0:00:00 ( 0.97 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6729\n77.2879\n28.23\n&lt;1e-99\n301.7871\n362.0938\n\n\nspkr: old\n67.7484\n18.2976\n3.70\n0.0002\n43.1536\n40.6843\n\n\nprec: maintain\n-333.9212\n47.1387\n-7.08\n&lt;1e-11\n62.0318\n246.8048\n\n\nload: yes\n78.7702\n19.5373\n4.03\n&lt;1e-04\n65.1901\n42.3299\n\n\nspkr: old & prec: maintain\n-21.9655\n15.8058\n-1.39\n0.1646\n\n\n\n\nspkr: old & load: yes\n18.3844\n15.8058\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5340\n15.8058\n0.29\n0.7742\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6073\n15.8058\n1.49\n0.1353\n\n\n\n\nResidual\n668.4871\n\n\n\n\n\n\n\n\n\n\nThe default display in Quarto uses the pretty MIME show method for the model and omits the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(m0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91075.4527\n301.7871\n\n\n\n\n\n\nspkr: old\n1862.2358\n43.1536\n+0.78\n\n\n\n\n\nprec: maintain\n3847.9447\n62.0318\n-0.59\n+0.03\n\n\n\n\nload: yes\n4249.7506\n65.1901\n+0.36\n+0.82\n+0.53\n\n\nitem\n(Intercept)\n131111.9379\n362.0938\n\n\n\n\n\n\nspkr: old\n1655.2134\n40.6843\n+0.44\n\n\n\n\n\nprec: maintain\n60912.6143\n246.8048\n-0.69\n+0.35\n\n\n\n\nload: yes\n1791.8243\n42.3299\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446875.0375\n668.4871\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\n# formula f4 from https://doi.org/10.33016/nextjournal.100002\nform = @formula(rt_trunc ~ 1 + spkr * prec * load + (1 | subj) + (1 + prec | item))\n\nm1 = fit(MixedModel, form, kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n&lt;1e-99\n364.7286\n298.1109\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n&lt;1e-11\n252.6687\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nspkr: old & prec: maintain\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.917\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.080\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(m0, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 | subj) + (1 + prec | item)\n13\n28658\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n21\n16\n0.1649\n\n\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, m1, to the more complex, m0."
  },
  {
    "objectID": "bootstrap.html#bootstrap-basics",
    "href": "bootstrap.html#bootstrap-basics",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "Bootstrap basics",
    "text": "Bootstrap basics\nTo bootstrap the model parameters, first initialize a random number generator then create a bootstrap sample\n\nconst RNG = MersenneTwister(42)\nsamp = parametricbootstrap(RNG, 1_000, m1)\ndf = DataFrame(samp.allpars)\nfirst(df, 10)\n\n10×5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n2049.88\n\n\n2\n1\nβ\nmissing\nspkr: old\n71.6398\n\n\n3\n1\nβ\nmissing\nprec: maintain\n-268.333\n\n\n4\n1\nβ\nmissing\nload: yes\n75.1566\n\n\n5\n1\nβ\nmissing\nspkr: old & prec: maintain\n-17.8459\n\n\n6\n1\nβ\nmissing\nspkr: old & load: yes\n-1.90968\n\n\n7\n1\nβ\nmissing\nprec: maintain & load: yes\n13.1018\n\n\n8\n1\nβ\nmissing\nspkr: old & prec: maintain & load: yes\n37.19\n\n\n9\n1\nσ\nitem\n(Intercept)\n363.568\n\n\n10\n1\nσ\nitem\nprec: maintain\n199.692\n\n\n\n\n\n\nEspecially for those with a background in R or pandas, the simplest way of accessing the parameter estimates in the parametric bootstrap object is to create a DataFrame from the allpars property as shown above.\nWe can use subset to subset out relevant rows of a dataframe. A density plot of the estimates of σ, the residual standard deviation, can be created as\n\nσres = subset(df, :type =&gt; ByRow(==(\"σ\")), :group =&gt; ByRow(==(\"residual\")); skipmissing=true)\n\nplt = data(σres) * mapping(:value) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ\"))\n\n\n\n\nA density plot of the estimates of the standard deviation of the random effects is obtained as\n\nσsubjitem = subset(df, :type =&gt; ByRow(==(\"σ\")), :group =&gt; ByRow(!=(\"residual\")); skipmissing=true)\n\nplt = data(σsubjitem) * mapping(:value; layout=:names, color=:group) * AoG.density()\ndraw(plt; figure=(;supertitle=\"Parametric bootstrap estimates of variance components\"))\n\n\n\n\nThe bootstrap sample can be used to generate intervals that cover a certain percentage of the bootstrapped values. We refer to these as “coverage intervals”, similar to a confidence interval. The shortest such intervals, obtained with the shortestcovint extractor, correspond to a highest posterior density interval in Bayesian inference.\nWe generate these for all random and fixed effects:\n\nshortestcovint(samp)\n\n13-element Vector{NamedTuple{(:type, :group, :names, :lower, :upper)}}:\n (type = \"β\", group = missing, names = \"(Intercept)\", lower = 2028.4504682672964, upper = 2335.4980378747036)\n (type = \"β\", group = missing, names = \"spkr: old\", lower = 33.90584194560795, upper = 97.1017123385091)\n (type = \"β\", group = missing, names = \"prec: maintain\", lower = -420.9759791010719, upper = -246.92630826126546)\n (type = \"β\", group = missing, names = \"load: yes\", lower = 49.939734569214366, upper = 108.72371422004355)\n (type = \"β\", group = missing, names = \"spkr: old & prec: maintain\", lower = -52.84340405095695, upper = 8.707634571058161)\n (type = \"β\", group = missing, names = \"spkr: old & load: yes\", lower = -11.990358740715502, upper = 49.090842667896304)\n (type = \"β\", group = missing, names = \"prec: maintain & load: yes\", lower = -25.207699936710174, upper = 36.986540545649625)\n (type = \"β\", group = missing, names = \"spkr: old & prec: maintain & load: yes\", lower = -5.88517077964873, upper = 54.770344717114426)\n (type = \"σ\", group = \"item\", names = \"(Intercept)\", lower = 267.28218879001105, upper = 462.8175723410845)\n (type = \"σ\", group = \"item\", names = \"prec: maintain\", lower = 178.14890712185007, upper = 316.22873298202416)\n (type = \"ρ\", group = \"item\", names = \"(Intercept), prec: maintain\", lower = -0.9058596236131196, upper = -0.48139111311317206)\n (type = \"σ\", group = \"subj\", names = \"(Intercept)\", lower = 231.8760081324643, upper = 356.152081034666)\n (type = \"σ\", group = \"residual\", names = missing, lower = 653.5649122788086, upper = 700.7662328017541)\n\n\nand convert it to a dataframe:\n\nDataFrame(shortestcovint(samp))\n\n13×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n2028.45\n2335.5\n\n\n2\nβ\nmissing\nspkr: old\n33.9058\n97.1017\n\n\n3\nβ\nmissing\nprec: maintain\n-420.976\n-246.926\n\n\n4\nβ\nmissing\nload: yes\n49.9397\n108.724\n\n\n5\nβ\nmissing\nspkr: old & prec: maintain\n-52.8434\n8.70763\n\n\n6\nβ\nmissing\nspkr: old & load: yes\n-11.9904\n49.0908\n\n\n7\nβ\nmissing\nprec: maintain & load: yes\n-25.2077\n36.9865\n\n\n8\nβ\nmissing\nspkr: old & prec: maintain & load: yes\n-5.88517\n54.7703\n\n\n9\nσ\nitem\n(Intercept)\n267.282\n462.818\n\n\n10\nσ\nitem\nprec: maintain\n178.149\n316.229\n\n\n11\nρ\nitem\n(Intercept), prec: maintain\n-0.90586\n-0.481391\n\n\n12\nσ\nsubj\n(Intercept)\n231.876\n356.152\n\n\n13\nσ\nresidual\nmissing\n653.565\n700.766\n\n\n\n\n\n\n\ndraw(\n  data(samp.β) * mapping(:β; color=:coefname) * AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\nFor the fixed effects, MixedModelsMakie provides a convenience interface to plot the combined coverage intervals and density plots\n\nridgeplot(samp)\n\n\n\n\nOften the intercept will be on a different scale and potentially less interesting, so we can stop it from being included in the plot:\n\nridgeplot(samp; show_intercept=false, xlabel=\"Bootstrap density and 95%CI\")"
  },
  {
    "objectID": "bootstrap.html#singularity",
    "href": "bootstrap.html#singularity",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "Singularity",
    "text": "Singularity\nLet’s consider the classic dysetuff dataset:\n\ndyestuff = MixedModels.dataset(:dyestuff)\nmdye = fit(MixedModel, @formula(yield ~ 1 + (1 | batch)), dyestuff)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_batch\n\n\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n&lt;1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\n\nsampdye = parametricbootstrap(MersenneTwister(1234321), 10_000, mdye)\ndfdye = DataFrame(sampdye.allpars)\nfirst(dfdye, 10)\n\n┌ Warning: NLopt was roundoff limited\n└ @ MixedModels ~/.julia/packages/MixedModels/8zfgx/src/optsummary.jl:180\n┌ Warning: NLopt was roundoff limited\n└ @ MixedModels ~/.julia/packages/MixedModels/8zfgx/src/optsummary.jl:180\n┌ Warning: NLopt was roundoff limited\n└ @ MixedModels ~/.julia/packages/MixedModels/8zfgx/src/optsummary.jl:180\n┌ Warning: NLopt was roundoff limited\n└ @ MixedModels ~/.julia/packages/MixedModels/8zfgx/src/optsummary.jl:180\n┌ Warning: NLopt was roundoff limited\n└ @ MixedModels ~/.julia/packages/MixedModels/8zfgx/src/optsummary.jl:180\n\n\n10×5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n1509.13\n\n\n2\n1\nσ\nbatch\n(Intercept)\n14.312\n\n\n3\n1\nσ\nresidual\nmissing\n67.4315\n\n\n4\n2\nβ\nmissing\n(Intercept)\n1538.08\n\n\n5\n2\nσ\nbatch\n(Intercept)\n25.5673\n\n\n6\n2\nσ\nresidual\nmissing\n47.9831\n\n\n7\n3\nβ\nmissing\n(Intercept)\n1508.02\n\n\n8\n3\nσ\nbatch\n(Intercept)\n21.7622\n\n\n9\n3\nσ\nresidual\nmissing\n50.1346\n\n\n10\n4\nβ\nmissing\n(Intercept)\n1538.47\n\n\n\n\n\n\n\nσbatch = subset(dfdye, :type =&gt; ByRow(==(\"σ\")), :group =&gt; ByRow(==(\"batch\")); skipmissing=true)\n\nplt = data(σbatch) * mapping(:value) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ_batch\"))\n\n\n\n\nNotice that this density plot has a spike, or mode, at zero. Although this mode appears to be diffuse, this is an artifact of the way that density plots are created. In fact, it is a pulse, as can be seen from a histogram.\n\nplt = data(σbatch) * mapping(:value) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ_batch\"))\n\n\n\n\nA value of zero for the standard deviation of the random effects is an example of a singular covariance. It is easy to detect the singularity in the case of a scalar random-effects term. However, it is not as straightforward to detect singularity in vector-valued random-effects terms.\nFor example, if we bootstrap a model fit to the sleepstudy data\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\nmsleep = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)),\n             sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\nsampsleep = parametricbootstrap(MersenneTwister(666), 10_000, msleep);\ndfsleep = DataFrame(sampsleep.allpars);\nfirst(dfsleep, 10)\n\n10×5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n252.488\n\n\n2\n1\nβ\nmissing\ndays\n11.0328\n\n\n3\n1\nσ\nsubj\n(Intercept)\n29.6185\n\n\n4\n1\nσ\nsubj\ndays\n6.33343\n\n\n5\n1\nρ\nsubj\n(Intercept), days\n0.233383\n\n\n6\n1\nσ\nresidual\nmissing\n22.4544\n\n\n7\n2\nβ\nmissing\n(Intercept)\n260.763\n\n\n8\n2\nβ\nmissing\ndays\n8.55352\n\n\n9\n2\nσ\nsubj\n(Intercept)\n20.8099\n\n\n10\n2\nσ\nsubj\ndays\n4.3292\n\n\n\n\n\n\nthe singularity can be exhibited as a standard deviation of zero or as a correlation of ±1.\n\nDataFrame(shortestcovint(sampsleep))\n\n6×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n237.905\n264.231\n\n\n2\nβ\nmissing\ndays\n7.44545\n13.3516\n\n\n3\nσ\nsubj\n(Intercept)\n10.6757\n33.3567\n\n\n4\nσ\nsubj\ndays\n3.07033\n7.822\n\n\n5\nρ\nsubj\n(Intercept), days\n-0.411665\n1.0\n\n\n6\nσ\nresidual\nmissing\n22.6345\n28.5125\n\n\n\n\n\n\nA histogram of the estimated correlations from the bootstrap sample has a spike at +1.\n\nρs = subset(dfsleep, :type =&gt; ByRow(==(\"ρ\")), :group =&gt; ByRow(==(\"subj\")); skipmissing=true)\nplt = data(ρs) * mapping(:value) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap samples of correlation of random effects\"))\n\n\n\n\nor, as a count,\n\ncount(ρs.value .≈ 1)\n\n291\n\n\nClose examination of the histogram shows a few values of -1.\n\ncount(ρs.value .≈ -1)\n\n2\n\n\nFurthermore there are even a few cases where the estimate of the standard deviation of the random effect for the intercept is zero.\n\nσs = subset(dfsleep, :type =&gt; ByRow(==(\"σ\")), :group =&gt; ByRow(==(\"subj\")), :names =&gt; ByRow(==(\"(Intercept)\")); skipmissing=true)\ncount(σs.value .≈ 0)\n\n7\n\n\nThere is a general condition to check for singularity of an estimated covariance matrix or matrices in a bootstrap sample. The parameter optimized in the estimation is θ, the relative covariance parameter. Some of the elements of this parameter vector must be non-negative and, when one of these components is approximately zero, one of the covariance matrices will be singular.\nThe issingular method for a MixedModel object that tests if a parameter vector θ corresponds to a boundary or singular fit.\nThis operation is encapsulated in a method for the issingular function that works on MixedModelBootstrap objects.\n\ncount(issingular(sampsleep))\n\n300"
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule."
  },
  {
    "objectID": "sleepstudy.html#loading-the-data",
    "href": "sleepstudy.html#loading-the-data",
    "title": "Analysis of the sleepstudy data",
    "section": "Loading the data",
    "text": "Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrameMacros  # simplified dplyr-like data wrangling\nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\n\nProgressMeter.ijulia_behavior(:clear);\nCairoMakie.activate!(; type=\"svg\");\n\n\nThe sleepstudy data are one of the datasets available with the MixedModels package.\n\nsleepstudy = MixedModels.dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nFigure 1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nprint(xyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n))\n\"\"\";\n\n\n\n\n\nFigure 1: Average response time versus days of sleep deprivation by subject\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject’s data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic."
  },
  {
    "objectID": "sleepstudy.html#fitting-an-initial-model",
    "href": "sleepstudy.html#fitting-an-initial-model",
    "title": "Analysis of the sleepstudy data",
    "section": "Fitting an initial model",
    "text": "Fitting an initial model\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not “parameters” in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1)\n\n\n\n\n\nFigure 2: Prediction intervals on random effects for model m1\n\n\n\n\nFigure 2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope."
  },
  {
    "objectID": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "href": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "title": "Analysis of the sleepstudy data",
    "section": "A model with uncorrelated random effects",
    "text": "A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\n\nm2 = let\n  form = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n&lt;1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n&lt;1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\nAgain, the default display doesn’t show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\n\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nreaction ~ 1 + days + zerocorr(1 + days | subj)\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  DataFrame(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ∘ leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2×6 DataFrame\n\n\n\nRow\nmodel\npars\ngeomdof\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nm2\n5\n29.045\n1762.0\n1777.97\n1762.35\n\n\n2\nm1\n6\n28.6115\n1763.94\n1783.1\n1764.42\n\n\n\n\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a “smaller is better” scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of “degrees of freedom”, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We’re not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor."
  },
  {
    "objectID": "sleepstudy.html#some-diagnostic-plots",
    "href": "sleepstudy.html#some-diagnostic-plots",
    "title": "Analysis of the sleepstudy data",
    "section": "Some diagnostic plots",
    "text": "Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure 3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\nFigure 3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)"
  },
  {
    "objectID": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "href": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "title": "Analysis of the sleepstudy data",
    "section": "Assessing variability by bootstrapping",
    "text": "Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1; hide_progress=true)\nallpars = DataFrame(m1bstp.allpars)\n\n30000×5 DataFrame29975 rows omitted\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n260.712\n\n\n2\n1\nβ\nmissing\ndays\n9.84975\n\n\n3\n1\nσ\nsubj\n(Intercept)\n15.3314\n\n\n4\n1\nσ\nsubj\ndays\n6.40292\n\n\n5\n1\nρ\nsubj\n(Intercept), days\n-0.0259482\n\n\n6\n1\nσ\nresidual\nmissing\n23.4092\n\n\n7\n2\nβ\nmissing\n(Intercept)\n262.253\n\n\n8\n2\nβ\nmissing\ndays\n12.3008\n\n\n9\n2\nσ\nsubj\n(Intercept)\n16.3183\n\n\n10\n2\nσ\nsubj\ndays\n5.54687\n\n\n11\n2\nρ\nsubj\n(Intercept), days\n0.552608\n\n\n12\n2\nσ\nresidual\nmissing\n25.7047\n\n\n13\n3\nβ\nmissing\n(Intercept)\n253.149\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n29989\n4999\nβ\nmissing\n(Intercept)\n251.077\n\n\n29990\n4999\nβ\nmissing\ndays\n10.8061\n\n\n29991\n4999\nσ\nsubj\n(Intercept)\n31.6311\n\n\n29992\n4999\nσ\nsubj\ndays\n5.53413\n\n\n29993\n4999\nρ\nsubj\n(Intercept), days\n0.171692\n\n\n29994\n4999\nσ\nresidual\nmissing\n22.4943\n\n\n29995\n5000\nβ\nmissing\n(Intercept)\n249.945\n\n\n29996\n5000\nβ\nmissing\ndays\n9.25346\n\n\n29997\n5000\nσ\nsubj\n(Intercept)\n38.7082\n\n\n29998\n5000\nσ\nsubj\ndays\n4.68739\n\n\n29999\n5000\nρ\nsubj\n(Intercept), days\n-0.251217\n\n\n30000\n5000\nσ\nresidual\nmissing\n26.0088\n\n\n\n\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure 4, shows the normal distribution, “bell-curve”, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"),\n    @subset(allpars, :type == \"β\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    @subset(allpars, :type == \"β\" && :names == \"days\").value,\n  )\n  f1\nend\n\n\n\n\n\nFigure 4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-α shortestcovint to be the shortest interval that contains a proportion 1-α (defaults to 95%) of the bootstrap estimates of the parameter.\n\nDataFrame(shortestcovint(m1bstp))\n\n6×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n239.64\n265.228\n\n\n2\nβ\nmissing\ndays\n7.42347\n13.1607\n\n\n3\nσ\nsubj\n(Intercept)\n10.1722\n33.0877\n\n\n4\nσ\nsubj\ndays\n2.99474\n7.6612\n\n\n5\nρ\nsubj\n(Intercept), days\n-0.40135\n1.0\n\n\n6\nσ\nresidual\nmissing\n22.701\n28.5016\n\n\n\n\n\n\nThe intervals look reasonable except that the upper bound on the interval for ρ, the correlation coefficient, is 1.0 . It turns out that the estimates of ρ have a great deal of variability.\nEven more alarming, some of these ρ values are undefined (denoted NaN) because the way ρ is calculated can create a division by zero.\n\ndescribe(@select(@subset(allpars, :type == \"ρ\"), :value))\n\n1×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nFloat64\nNothing\nFloat64\nInt64\nDataType\n\n\n\n\n1\nvalue\nNaN\nNaN\n\nNaN\n0\nFloat64\n\n\n\n\n\n\nBecause there are several values on the boundary (ρ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure 5.\n\n\nCode\nhist(\n  @subset(allpars, :type == \"ρ\", isfinite(:value)).value;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\nFigure 5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure 6, show reasonable symmetry.\n\n\nCode\nbegin\n  σs = @subset(allpars, :type == \"σ\")\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual σ\"),\n    @subset(σs, :group == \"residual\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept σ\"),\n    @subset(σs, :group == \"subj\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope σ\"),\n    @subset(σs, :group == \"subj\" && :names == \"days\").value,\n  )\n  f2\nend\n\n\n\n\n\nFigure 6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, β₁ and β₂, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure 7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2×2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  vals = disallowmissing(\n    Array(\n      select(\n        unstack(DataFrame(m1bstp.β), :iter, :coefname, :β),\n        Not(:iter),\n      ),\n    ),\n  )\n  scatter(\n    vals;\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde(vals))\n  current_figure()\nend\n\n\n\n\n\nFigure 7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours"
  },
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, but one further motivation is that Julia is at a broader “technical computing” audience (like MATLAB or perhaps Python) and less at a “statistical analysis” audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. Pluto helpfully installs and manages for you, but the package-manager REPL mode (activated by typing ] at the julia&gt; prompt) is very useful."
  },
  {
    "objectID": "useful_packages.html#data-wrangling",
    "href": "useful_packages.html#data-wrangling",
    "title": "Useful packages",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nReading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather “column tables” – named tuples of column vectors.\n\n\nDataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R’s dplyr and much of the tidyverse. The DataFrames.jl documentation is the place to for looking at how to e.g. read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations.\nDataFrame.jl’s mini language can be a bit daunting, if you’re used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we’e had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or “pipe” together successive operations. It’s your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl."
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "Regression",
    "text": "Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don’t have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e. the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia &gt; lm(@formula(y ~ 1 + x), data)\njulia &gt; glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia &gt; fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia &gt; fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don’t recommend this as it’s easy to mess up in really subtle but very probelmatic ways.)\n\n@formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that’s because formulas are essentially their own domain-specific language (a variant of Wilkinson-Roger notation) and macros are used for manipulating the language itself – or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.\n\n\nExtensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the “extras” available in R, e.g. RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.\n\n\nStandardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e. on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g. centering new data around the mean computed during fitting the original data) correctly and automatically.\n\n\nEffects\nJohn Fox’s effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model’s overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we’re working on it!\n\n\nEstimated Marginal / Least Square Means\nEffects.jl provides a subset of the functionality (basic estimated-marginal means and exhaustive pairwise comparisons) of the R package emmeans package. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. 😃"
  },
  {
    "objectID": "useful_packages.html#hypothesis-testing",
    "href": "useful_packages.html#hypothesis-testing",
    "title": "Useful packages",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nClassical statistical tests such as the t-test can be found in the package HypothesisTests.jl."
  },
  {
    "objectID": "useful_packages.html#plotting-ecosystem",
    "href": "useful_packages.html#plotting-ecosystem",
    "title": "Useful packages",
    "section": "Plotting ecosystem",
    "text": "Plotting ecosystem\nThroughtout this course, we have used the Makie ecosystem for plotting, but there are several alternatives in Julia.\n\nMakie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g. MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Quarto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It’s a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly.\n\n\nPlots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX’s tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.\n\n\nGadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the “gg” in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).\n\n\nOthers\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite."
  },
  {
    "objectID": "useful_packages.html#connecting-to-other-languages",
    "href": "useful_packages.html#connecting-to-other-languages",
    "title": "Useful packages",
    "section": "Connecting to Other Languages",
    "text": "Connecting to Other Languages\nUsing Julia doesn’t mean you have to leave all the packages you knew in other languages behind. In Julia, it’s often possible to even easily and quickly invoke code from other languages from within Julia.\nRCall.jl provides a very convenient interface for interacting with R. JellyMe4.jl add support for moving MixedModels.jl and lme4 models back and forth between the languages (which means that you can use emmeans, sjtools, DHARMa, car, etc. to examine MixedModels.jl models!). RData.jl provides support for reading .rds and .rda files from Julia, while RDatasets.jl provides convenient access to many of the standard datasets provided by R and various R packages.\nPyCall.jl provides a very convenient way for interacting with Python code and packages. PyPlot.jl builds upon this foundation to provide support for Python’s matplotlib. Similarly, PyMNE.jl and PyFOOOF.jl provide some additional functionality to make interacting with MNE-Python and FOOOF from within Julia even easier than with vanilla PyCall. More recently, PythonCall.jl has proven to be a populat alternative to PyCall.jl.\nFor MATLAB users, there is also MATLAB.jl\nCxx.jl provides interoperability with C++. It also provides a C++ REPL mode, making it possible to treating C++ much more like a dynamic language than the traditional compiler toolchain would allow.\nSupport for calling C and Fortran is part of the Julia standard library."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "",
    "text": "This site provides materials for the Advanced frequentist methods stream of the Summer School on Statistical Methods to be held at the University of Potsdam, 11-15 September, 2023."
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.1 git",
    "text": "1.1 git\nWe will assume that you have git installed and are able to clone a repository from github. If not, Happy Git with R is a good place to learn about git for data science.\nThis website is built using quarto, described below, from the repository. Clone this repository with, e.g.\ngit clone https://github.com/RePsychLing/SMLP2023"
  },
  {
    "objectID": "index.html#julia-programming-language",
    "href": "index.html#julia-programming-language",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.2 Julia Programming Language",
    "text": "1.2 Julia Programming Language\nWe will use Julia v1.9.2 in the summer school. You can download the version appropriate for your setup from here: Julia Programming Language"
  },
  {
    "objectID": "index.html#visual-studio-code-vs-code",
    "href": "index.html#visual-studio-code-vs-code",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.3 Visual Studio Code (VS Code)",
    "text": "1.3 Visual Studio Code (VS Code)\nWe will use VS Code IDE, that is Julia : VS Code ~ R : RStudio. You can download the version appropriate for your setup from here: VS Code"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nThe web site and other documents for this course are rendered using a knitr-like system called Quarto. You can download the version appropriate for your setup from here: quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pkg.html",
    "href": "pkg.html",
    "title": "Package management and reproducible environments",
    "section": "",
    "text": "Julius Krumbiegel also has a great blog post with more details on Julia environments.\nJulia packages can be configured (in a file called Project.toml) on a per-project basis. The packaged sources and compiled versions are stored in a central location, e.g. ~/.julia/packages and ~/.julia/compiled on Linux systems, but the configuration of packages to be used can be local to a project. The Pkg package is used to modify the local project’s configuration. (An alternative is “package mode” in the read-eval-print-loop or REPL, which we will show at the summer school.) Start julia in the directory of the cloned SMLP2022 repository\n\nusing Pkg        # there's a package called 'Pkg' to manipulate package configs\nPkg.activate(\".\")# activate the current directory as the project\n\nIf you’ve recieved an environment from someone/somwhere else – such as this course repository – then you’ll need to first “instantiate” it (i.e., install all the dependencies).\n\nPkg.instantiate()# only needed the first time you work in a project\nPkg.update()     # get the latest package versions compatible with the project\n\n\nPkg.status()\n\nOccasionally the Pkg.status function call will give info about new versions being available but blocked by requirements of other packages. This is to be expected - the package system is large and the web of dependencies are complex. Generally the Julia package system is very good at resolving dependencies."
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ⁄ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\nDesign\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1–E7, R1–R3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\nTest schedule within days\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\nStatistical analyses\nThe authors analyzed response speed, that is (1/RT)*1000 – completely warranted according to a Box-Cox check of the current data – with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\nCurrent data\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#background",
    "href": "sleepstudy_speed.html#background",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ⁄ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\nDesign\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1–E7, R1–R3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\nTest schedule within days\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\nStatistical analyses\nThe authors analyzed response speed, that is (1/RT)*1000 – completely warranted according to a Box-Cox check of the current data – with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\nCurrent data\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#setup",
    "href": "sleepstudy_speed.html#setup",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Setup",
    "text": "Setup\nFirst we attach the various packages needed, define a few helper functions, read the data, and get everything in the desired shape.\n\n\nCode\nusing CairoMakie         # device driver for static (SVG, PDF, PNG) plots\nusing Chain              # like pipes but cleaner\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie   # plots specific to mixed-effects models using Makie\n\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "sleepstudy_speed.html#preprocessing",
    "href": "sleepstudy_speed.html#preprocessing",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe sleepstudy data are one of the datasets available with recent versions of the MixedModels package. We carry out some preprocessing to have the dataframe in the desired shape:\n\nCapitalize random factor Subj\nCompute speed as an alternative dependent variable from reaction, warranted by a ‘boxcox’ check of residuals.\nCreate a GroupedDataFrame by levels of Subj (the original dataframe is available as gdf.parent, which we name df)\n\n\ngdf = @chain MixedModels.dataset(:sleepstudy) begin\n  DataFrame\n  rename!(:subj =&gt; :Subj, :days =&gt; :day)\n  @transform!(:speed = 1000 / :reaction)\n  groupby(:Subj)\nend\n\nGroupedDataFrame with 18 groups based on key: SubjFirst Group (10 rows): Subj = \"S308\"\n\n\n\nRow\nSubj\nday\nreaction\nspeed\n\n\n\nString\nInt8\nFloat64\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n4.00705\n\n\n2\nS308\n1\n258.705\n3.86541\n\n\n3\nS308\n2\n250.801\n3.98723\n\n\n4\nS308\n3\n321.44\n3.111\n\n\n5\nS308\n4\n356.852\n2.80228\n\n\n6\nS308\n5\n414.69\n2.41144\n\n\n7\nS308\n6\n382.204\n2.61641\n\n\n8\nS308\n7\n290.149\n3.44651\n\n\n9\nS308\n8\n430.585\n2.32242\n\n\n10\nS308\n9\n466.353\n2.1443\n\n\n\n⋮\n\n\nLast Group (10 rows): Subj = \"S372\"\n\n\n\n\n\n\n\n\n\nRow\nSubj\nday\nreaction\nspeed\n\n\n\nString\nInt8\nFloat64\nFloat64\n\n\n\n\n1\nS372\n0\n269.412\n3.71179\n\n\n2\nS372\n1\n273.474\n3.65665\n\n\n3\nS372\n2\n297.597\n3.36025\n\n\n4\nS372\n3\n310.632\n3.21925\n\n\n5\nS372\n4\n287.173\n3.48223\n\n\n6\nS372\n5\n329.608\n3.03391\n\n\n7\nS372\n6\n334.482\n2.9897\n\n\n8\nS372\n7\n343.22\n2.91358\n\n\n9\nS372\n8\n369.142\n2.70899\n\n\n10\nS372\n9\n364.124\n2.74632\n\n\n\n\n\n\n\n\ndf = gdf.parent\ndescribe(df)\n\n4×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nSubj\n\nS308\n\nS372\n0\nString\n\n\n2\nday\n4.5\n0\n4.5\n9\n0\nInt8\n\n\n3\nreaction\n298.508\n194.332\n288.651\n466.353\n0\nFloat64\n\n\n4\nspeed\n3.46634\n2.1443\n3.46443\n5.14583\n0\nFloat64"
  },
  {
    "objectID": "sleepstudy_speed.html#estimates-for-pooled-data",
    "href": "sleepstudy_speed.html#estimates-for-pooled-data",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Estimates for pooled data",
    "text": "Estimates for pooled data\nIn the first analysis we ignore the dependency of observations due to repeated measures from the same subjects. We pool all the data and estimate the regression of 180 speed scores on the nine days of the experiment.\n\npooledcoef = simplelinreg(df.day, df.speed)  # produces a Tuple\n\n(3.9658119747831484, -0.110993592321997)"
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-effects",
    "href": "sleepstudy_speed.html#within-subject-effects",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Within-subject effects",
    "text": "Within-subject effects\nIn the second analysis we estimate coefficients for each Subj without regard of the information available from the complete set of data. We do not “borrow strength” to adjust for differences due to between-Subj variability and due to being far from the population mean.\n\nWithin-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] =&gt; simplelinreg =&gt; :coef)\n\n18×2 DataFrame\n\n\n\nRow\nSubj\ncoef\n\n\n\nString\nTuple…\n\n\n\n\n1\nS308\n(3.94806, -0.194812)\n\n\n2\nS309\n(4.87022, -0.0475185)\n\n\n3\nS310\n(4.90606, -0.120054)\n\n\n4\nS330\n(3.4449, -0.0291309)\n\n\n5\nS331\n(3.47647, -0.0498047)\n\n\n6\nS332\n(3.84436, -0.105511)\n\n\n7\nS333\n(3.60159, -0.0917378)\n\n\n8\nS334\n(4.04528, -0.133527)\n\n\n9\nS335\n(3.80451, 0.0455771)\n\n\n10\nS337\n(3.34374, -0.137744)\n\n\n11\nS349\n(4.46855, -0.170885)\n\n\n12\nS350\n(4.21414, -0.20151)\n\n\n13\nS351\n(3.80469, -0.0728582)\n\n\n14\nS352\n(3.68634, -0.144957)\n\n\n15\nS369\n(3.85384, -0.120531)\n\n\n16\nS370\n(4.52679, -0.215965)\n\n\n17\nS371\n(3.853, -0.0936243)\n\n\n18\nS372\n(3.69208, -0.113292)\n\n\n\n\n\n\nFigure 1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n┌ Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n│   caller = top-level scope at In[7]:8\n└ @ Core ./In[7]:8\n\n\n\n\n\nFigure 1: Reaction speed (s⁻¹) versus days of sleep deprivation by subject"
  },
  {
    "objectID": "sleepstudy_speed.html#basic-lmm",
    "href": "sleepstudy_speed.html#basic-lmm",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Basic LMM",
    "text": "Basic LMM\n\ncontrasts = Dict(:Subj =&gt; Grouping())\nm1 = let\n  form = @formula speed ~ 1 + day + (1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1056\n37.55\n&lt;1e-99\n0.4190\n\n\nday\n-0.1110\n0.0151\n-7.37\n&lt;1e-12\n0.0566\n\n\nResidual\n0.2698\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept which estimates the average speed on the baseline day of the experiment prior to sleep deprivation, and the slowing per day of sleep deprivation. In this case about -0.11/second.\nThe random effects represent shifts from the typical behavior for each subject.The shift in the intercept has a standard deviation of about 0.42/s.\nThe within-subject correlation of the random effects for intercept and slope is small, -0.18, indicating that a simpler model with a correlation parameter (CP) forced to/ assumed to be zero may be sufficient."
  },
  {
    "objectID": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "href": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "No correlation parameter: zcp LMM",
    "text": "No correlation parameter: zcp LMM\nThe zerocorr function applied to a random-effects term estimates one parameter less than LMM m1– the CP is now fixed to zero.\n\nm2 = let\n  form = @formula speed ~ 1 + day + zerocorr(1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1033\n38.38\n&lt;1e-99\n0.4085\n\n\nday\n-0.1110\n0.0147\n-7.53\n&lt;1e-13\n0.0550\n\n\nResidual\n0.2706\n\n\n\n\n\n\n\n\n\nLMM m2 has a slghtly lower log-likelihood than LMM m1 but also one fewer parameters. A likelihood-ratio test is used to compare these nested models.\n\n\nCode\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nspeed ~ 1 + day + zerocorr(1 + day | Subj)\n5\n125\n\n\n\n\n\nspeed ~ 1 + day + (1 + day | Subj)\n6\n125\n0\n1\n0.5192\n\n\n\n\n\nAlternatively, the AIC, AICc, and BIC values can be compared. They are on a scale where “smaller is better”. All three model-fit statistics prefer the zcpLMM m2.\n\n\nCode\nlet\n  mods = [m2, m1]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n2×5 DataFrame\n\n\n\nRow\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n5\n125.379\n135.379\n135.724\n151.344\n\n\n2\n6\n124.964\n136.964\n137.45\n156.122"
  },
  {
    "objectID": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "href": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Conditional modes of the random effects",
    "text": "Conditional modes of the random effects\nThe third set of estimates are their conditional modes. They represent a compromise between their own data and the model parameters. When distributional assumptions hold, predictions based on these estimates are more accurate than either the pooled or the within-subject estimates. Here we “borrow strength” to improve the accuracy of prediction."
  },
  {
    "objectID": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "href": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Caterpillar plots (effect profiles)",
    "text": "Caterpillar plots (effect profiles)\n\n\nCode\ncaterpillar(m2)\n\n\n\n\n\nFigure 2: Prediction intervals on the random effects in model m2"
  },
  {
    "objectID": "sleepstudy_speed.html#shrinkage-plot",
    "href": "sleepstudy_speed.html#shrinkage-plot",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m2)\n\n\n\n\n\nFigure 3: Shrinkage plot of the means of the random effects in model m2"
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\ndatadir = joinpath(@__DIR__, \"data\");"
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "Matrix notation for the sleepstudy model",
    "text": "Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n180×3 DataFrame155 rows omitted\n\n\n\nRow\nsubj\ndays\nreaction\n\n\n\nString\nInt8\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n\n\n2\nS308\n1\n258.705\n\n\n3\nS308\n2\n250.801\n\n\n4\nS308\n3\n321.44\n\n\n5\nS308\n4\n356.852\n\n\n6\nS308\n5\n414.69\n\n\n7\nS308\n6\n382.204\n\n\n8\nS308\n7\n290.149\n\n\n9\nS308\n8\n430.585\n\n\n10\nS308\n9\n466.353\n\n\n11\nS309\n0\n222.734\n\n\n12\nS309\n1\n205.266\n\n\n13\nS309\n2\n202.978\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n169\nS371\n8\n350.781\n\n\n170\nS371\n9\n369.469\n\n\n171\nS372\n0\n269.412\n\n\n172\nS372\n1\n273.474\n\n\n173\nS372\n2\n297.597\n\n\n174\nS372\n3\n310.632\n\n\n175\nS372\n4\n287.173\n\n\n176\nS372\n5\n329.608\n\n\n177\nS372\n6\n334.482\n\n\n178\nS372\n7\n343.22\n\n\n179\nS372\n8\n369.142\n\n\n180\nS372\n9\n364.124\n\n\n\n\n\n\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n──────────────────────────────────────────────────\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, β, has 2 elements and the fixed-effects model matrix, X, is of size 180 × 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ⋮\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.β\n\n2-element Vector{Float64}:\n 251.40510484848414\n  10.46728595959568\n\n\n\nm1.X\n\n180×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ⋮    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1’s.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The “estimates” (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158180863879005 -40.04844125547803 … 0.7232621646562781 12.118907819971193; 9.075511889252148 -8.644079482408275 … -0.9710526563521795 1.3106980518574347]\n\n\n\nfirst(m1.b)   # only one grouping factor\n\n2×18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   …  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180×36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.β + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22092293487205\n 273.7637207837199\n 293.3065186325677\n 312.84931648141554\n 332.3921143302634\n 351.93491217911117\n 371.477710027959\n 391.02050787680685\n 410.5633057256547\n 430.1061035745025\n 211.35666359300612\n 213.17987007019354\n 215.00307654738089\n   ⋮\n 328.0982334390884\n 337.5944667423319\n 263.5240126684553\n 275.3019966799085\n 287.0799806913615\n 298.8579647028147\n 310.63594871426784\n 322.4139327257209\n 334.19191673717404\n 345.96990074862714\n 357.74788476008024\n 369.5258687715334\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22092293487205\n 273.76372078371986\n 293.30651863256764\n 312.8493164814155\n 332.3921143302633\n 351.93491217911117\n 371.477710027959\n 391.02050787680685\n 410.56330572565463\n 430.10610357450247\n 211.35666359300612\n 213.17987007019354\n 215.0030765473809\n   ⋮\n 328.0982334390884\n 337.5944667423319\n 263.5240126684553\n 275.3019966799085\n 287.0799806913615\n 298.8579647028147\n 310.6359487142678\n 322.4139327257209\n 334.19191673717404\n 345.96990074862714\n 357.74788476008024\n 369.52586877153334\n\n\nIn symbols we would write the linear predictor expression as \\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\] where \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 × 2 and \\(\\bf Z\\) is of size 180 × 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 × 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 × 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.θ\n\n3-element Vector{Float64}:\n 0.9292213132463749\n 0.0181683611765396\n 0.2226448780526805\n\n\n\nλ = only(m1.λ)  # with multiple grouping factors there will be multiple λ's\n\n2×2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\n\nΣ = varest(m1) * (λ * λ')\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "Linear predictors in LMMs and GLMMs",
    "text": "Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as \\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\] may seem like over-mathematization (or “overkill”, if you prefer) relative to expressions like \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\] but this more abstract form is necessary for generalizations.\nThe way that I read the first form is\n\n\n\n\n\n\nThe conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\n\n\n\nSo the only things that differ in the distributions of the \\(y_i\\)’s are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "Generalized Linear Mixed Models",
    "text": "Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e. yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i’th response is again determined by the i’th element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of “success” for the i’th response and must be between 0 and 1. We can’t guarantee that the i’th component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren’t as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function, \\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\] (it’s called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic \\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\] This is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(η) = inv(increment(exp(-η)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don’t have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet “Beauty is truth, truth beauty - that is all ye know on earth and all ye need to know.”)\n\nAn example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n1934×5 DataFrame1909 rows omitted\n\n\n\nRow\ndist\nurban\nlivch\nage\nuse\n\n\n\nString\nString\nString\nFloat64\nString\n\n\n\n\n1\nD01\nY\n3+\n18.44\nN\n\n\n2\nD01\nY\n0\n-5.56\nN\n\n\n3\nD01\nY\n2\n1.44\nN\n\n\n4\nD01\nY\n3+\n8.44\nN\n\n\n5\nD01\nY\n0\n-13.56\nN\n\n\n6\nD01\nY\n0\n-11.56\nN\n\n\n7\nD01\nY\n3+\n18.44\nN\n\n\n8\nD01\nY\n3+\n-3.56\nN\n\n\n9\nD01\nY\n1\n-5.56\nN\n\n\n10\nD01\nY\n3+\n1.44\nN\n\n\n11\nD01\nY\n0\n-11.56\nY\n\n\n12\nD01\nY\n0\n-2.56\nN\n\n\n13\nD01\nY\n1\n-4.56\nN\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n1923\nD61\nN\n0\n-11.56\nY\n\n\n1924\nD61\nN\n3+\n1.44\nN\n\n\n1925\nD61\nN\n1\n-5.56\nN\n\n\n1926\nD61\nN\n3+\n14.44\nN\n\n\n1927\nD61\nN\n3+\n19.44\nN\n\n\n1928\nD61\nN\n2\n-9.56\nY\n\n\n1929\nD61\nN\n2\n-2.56\nN\n\n\n1930\nD61\nN\n3+\n14.44\nN\n\n\n1931\nD61\nN\n2\n-4.56\nN\n\n\n1932\nD61\nN\n3+\n14.44\nN\n\n\n1933\nD61\nN\n0\n-13.56\nN\n\n\n1934\nD61\nN\n3+\n10.44\nN\n\n\n\n\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n60×2 DataFrame35 rows omitted\n\n\n\nRow\ndist\nnrow\n\n\n\nString\nInt64\n\n\n\n\n1\nD01\n117\n\n\n2\nD02\n20\n\n\n3\nD03\n2\n\n\n4\nD04\n30\n\n\n5\nD05\n39\n\n\n6\nD06\n65\n\n\n7\nD07\n18\n\n\n8\nD08\n37\n\n\n9\nD09\n23\n\n\n10\nD10\n13\n\n\n11\nD11\n21\n\n\n12\nD12\n29\n\n\n13\nD13\n24\n\n\n⋮\n⋮\n⋮\n\n\n49\nD49\n4\n\n\n50\nD50\n19\n\n\n51\nD51\n37\n\n\n52\nD52\n61\n\n\n53\nD53\n19\n\n\n54\nD55\n6\n\n\n55\nD56\n45\n\n\n56\nD57\n27\n\n\n57\nD58\n33\n\n\n58\nD59\n10\n\n\n59\nD60\n32\n\n\n60\nD61\n42\n\n\n\n\n\n\nThe information recorded included woman’s age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, drawn using lattice graphics in R, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20’s to early 30’s) and low again for older women (late 30’s to early 40’s in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn’t mean that there is no “age effect”, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :numuse =&gt; \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist =&gt; Grouping(),\n  :urban =&gt; HelmertCoding(),\n  :livch =&gt; DummyCoding(), # default, but no harm in being explicit\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 214   Time: 0:00:00 ( 1.98 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n&lt;1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7021\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n&lt;1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n&lt;1e-08\n\n\n\nlivch: 1\n0.8151\n0.1622\n5.02\n&lt;1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n&lt;1e-06\n\n\n\nlivch: 3+\n0.9153\n0.1858\n4.93\n&lt;1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229094\n0.478638\n\n\n\n\n\nNotice that there is no “residual” variance being estimated. This is because the Bernoulli distribution doesn’t have a scale parameter.\n\n\nConvert livch to a binary factor\n\n@transform!(contra, :children = :livch ≠ \"0\")\n# add the associated contrast specifier\ncontrasts[:children] = EffectsCoding()\n\nEffectsCoding(nothing, nothing)\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 146   Time: 0:00:00 ( 1.02 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.84\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2351\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2×6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n2\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\nUsing urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 157   Time: 0:00:00 ( 0.93 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3×6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm3\n7\n2353.82\n2368.48\n2407.46\n2368.54\n\n\n2\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n3\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.84\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2351\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age² (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\nPredictions for some subgroups\nFor a “typical” district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e. centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children =&gt; [true, false], :urban =&gt; [\"Y\", \"N\"], :age =&gt; [0.0]\n)\npreds = effects(design, gm3)\n\n4×7 DataFrame\n\n\n\nRow\nchildren\nage\nurban\nuse: Y\nerr\nlower\nupper\n\n\n\nBool\nFloat64\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ntrue\n0.0\nY\n0.658575\n0.150531\n0.508045\n0.809106\n\n\n2\nfalse\n0.0\nY\n-0.554319\n0.230477\n-0.784795\n-0.323842\n\n\n3\ntrue\n0.0\nN\n-0.128618\n0.113018\n-0.241636\n-0.0156005\n\n\n4\nfalse\n0.0\nN\n-1.34151\n0.221575\n-1.56309\n-1.11994\n\n\n\n\n\n\nConverting these η values to probabilities yields\n\nlogistic.(preds[!, \"use: Y\"])\n\n4-element Vector{Float64}:\n 0.6589402981066934\n 0.3648630189943811\n 0.46788970324953494\n 0.20726147184793126"
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "Summarizing the results",
    "text": "Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  }
]