[
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, but one further motivation is that Julia is at a broader ‚Äútechnical computing‚Äù audience (like MATLAB or perhaps Python) and less at a ‚Äústatistical analysis‚Äù audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. Pluto helpfully installs and manages for you, but the package-manager REPL mode (activated by typing ] at the julia&gt; prompt) is very useful."
  },
  {
    "objectID": "useful_packages.html#reading-data",
    "href": "useful_packages.html#reading-data",
    "title": "Useful packages",
    "section": "1.1 Reading data",
    "text": "1.1 Reading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather ‚Äúcolumn tables‚Äù ‚Äì named tuples of column vectors."
  },
  {
    "objectID": "useful_packages.html#dataframes",
    "href": "useful_packages.html#dataframes",
    "title": "Useful packages",
    "section": "1.2 DataFrames",
    "text": "1.2 DataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R‚Äôs dplyr and much of the tidyverse. The DataFrames.jl documentation is the place to for looking at how to e.g.¬†read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations.\nDataFrame.jl‚Äôs mini language can be a bit daunting, if you‚Äôre used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we‚Äôe had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or ‚Äúpipe‚Äù together successive operations. It‚Äôs your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl.\nThere‚Äôs also Tidier.jl, which is ‚Äúa 100% Julia implementation of the R tidyverse mini-language in Julia‚Äù. Phillip and Doug aren‚Äôt huge fans of this package because it encourages patterns from R that they consider problematic, but each user can choose their own path. üòÉ"
  },
  {
    "objectID": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "href": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "title": "Useful packages",
    "section": "2.1 @formula, macros and domain-specific languages",
    "text": "2.1 @formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that‚Äôs because formulas are essentially their own domain-specific language (a variant of Wilkinson-Roger notation) and macros are used for manipulating the language itself ‚Äì or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan."
  },
  {
    "objectID": "useful_packages.html#extensions-to-the-formula-syntax",
    "href": "useful_packages.html#extensions-to-the-formula-syntax",
    "title": "Useful packages",
    "section": "2.2 Extensions to the formula syntax",
    "text": "2.2 Extensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the ‚Äúextras‚Äù available in R, e.g.¬†RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R."
  },
  {
    "objectID": "useful_packages.html#standardizing-predictors",
    "href": "useful_packages.html#standardizing-predictors",
    "title": "Useful packages",
    "section": "2.3 Standardizing Predictors",
    "text": "2.3 Standardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e.¬†on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g.¬†centering new data around the mean computed during fitting the original data) correctly and automatically."
  },
  {
    "objectID": "useful_packages.html#effects",
    "href": "useful_packages.html#effects",
    "title": "Useful packages",
    "section": "2.4 Effects",
    "text": "2.4 Effects\nJohn Fox‚Äôs effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model‚Äôs overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we‚Äôre working on it!"
  },
  {
    "objectID": "useful_packages.html#estimated-marginal-least-square-means",
    "href": "useful_packages.html#estimated-marginal-least-square-means",
    "title": "Useful packages",
    "section": "2.5 Estimated Marginal / Least Square Means",
    "text": "2.5 Estimated Marginal / Least Square Means\nEffects.jl provides a subset of the functionality (basic estimated-marginal means and exhaustive pairwise comparisons) of the R package emmeans package. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. üòÉ"
  },
  {
    "objectID": "useful_packages.html#makie",
    "href": "useful_packages.html#makie",
    "title": "Useful packages",
    "section": "4.1 Makie",
    "text": "4.1 Makie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g.¬†MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Quarto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It‚Äôs a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly."
  },
  {
    "objectID": "useful_packages.html#plots.jl",
    "href": "useful_packages.html#plots.jl",
    "title": "Useful packages",
    "section": "4.2 Plots.jl",
    "text": "4.2 Plots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX‚Äôs tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem."
  },
  {
    "objectID": "useful_packages.html#gadfly",
    "href": "useful_packages.html#gadfly",
    "title": "Useful packages",
    "section": "4.3 Gadfly",
    "text": "4.3 Gadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the ‚Äúgg‚Äù in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021)."
  },
  {
    "objectID": "useful_packages.html#others",
    "href": "useful_packages.html#others",
    "title": "Useful packages",
    "section": "4.4 Others",
    "text": "4.4 Others\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite."
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule.\n\n\n\n\n\n\nNote\n\n\n\nThis description is inaccurate. In fact the first two days were acclimatization, the third was a baseline and sleep deprivation was only enforced after day 2. To allow for comparison with earlier analyses of these data we retain the old data description for this notebook only.\n\n\n\n1 Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrameMacros  # simplified dplyr-like data wrangling\nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\nusing SMLP2023\nusing SMLP2023: dataset\n\nProgressMeter.ijulia_behavior(:clear);\nCairoMakie.activate!(; type=\"svg\");\n\n\nThe sleepstudy data are one of the datasets available with the MixedModels package.\n\nsleepstudy = dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nFigure¬†1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nprint(xyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n))\n\"\"\";\n\n\n\n\n\nFigure¬†1: Average response time versus days of sleep deprivation by subject\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject‚Äôs data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic.\n\n\n2 Fitting an initial model\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\nMinimizing 57    Time: 0:00:00 ( 3.66 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51066\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not ‚Äúparameters‚Äù in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1)\n\n\n\n\n\nFigure¬†2: Prediction intervals on random effects for model m1\n\n\n\n\nFigure¬†2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope.\n\n\n3 A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\n\nm2 = let\n  form = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n&lt;1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n&lt;1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\nAgain, the default display doesn‚Äôt show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\n\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nreaction ~ 1 + days + zerocorr(1 + days | subj)\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  DataFrame(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ‚àò leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2√ó6 DataFrame\n\n\n\nRow\nmodel\npars\ngeomdof\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nm2\n5\n29.045\n1762.0\n1777.97\n1762.35\n\n\n2\nm1\n6\n28.6115\n1763.94\n1783.1\n1764.42\n\n\n\n\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a ‚Äúsmaller is better‚Äù scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of ‚Äúdegrees of freedom‚Äù, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We‚Äôre not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor.\n\n\n4 Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure¬†3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\nFigure¬†3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)\n\n\n5 Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1; hide_progress=true)\nallpars = DataFrame(m1bstp.allpars)\n\n30000√ó5 DataFrame29975 rows omitted\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n260.712\n\n\n2\n1\nŒ≤\nmissing\ndays\n9.84975\n\n\n3\n1\nœÉ\nsubj\n(Intercept)\n15.3314\n\n\n4\n1\nœÉ\nsubj\ndays\n6.40292\n\n\n5\n1\nœÅ\nsubj\n(Intercept), days\n-0.0259483\n\n\n6\n1\nœÉ\nresidual\nmissing\n23.4092\n\n\n7\n2\nŒ≤\nmissing\n(Intercept)\n262.253\n\n\n8\n2\nŒ≤\nmissing\ndays\n12.3008\n\n\n9\n2\nœÉ\nsubj\n(Intercept)\n16.3183\n\n\n10\n2\nœÉ\nsubj\ndays\n5.54687\n\n\n11\n2\nœÅ\nsubj\n(Intercept), days\n0.552609\n\n\n12\n2\nœÉ\nresidual\nmissing\n25.7047\n\n\n13\n3\nŒ≤\nmissing\n(Intercept)\n253.149\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n29989\n4999\nŒ≤\nmissing\n(Intercept)\n251.077\n\n\n29990\n4999\nŒ≤\nmissing\ndays\n10.8061\n\n\n29991\n4999\nœÉ\nsubj\n(Intercept)\n31.6349\n\n\n29992\n4999\nœÉ\nsubj\ndays\n5.53411\n\n\n29993\n4999\nœÅ\nsubj\n(Intercept), days\n0.171683\n\n\n29994\n4999\nœÉ\nresidual\nmissing\n22.4941\n\n\n29995\n5000\nŒ≤\nmissing\n(Intercept)\n249.945\n\n\n29996\n5000\nŒ≤\nmissing\ndays\n9.25346\n\n\n29997\n5000\nœÉ\nsubj\n(Intercept)\n38.7082\n\n\n29998\n5000\nœÉ\nsubj\ndays\n4.68739\n\n\n29999\n5000\nœÅ\nsubj\n(Intercept), days\n-0.251217\n\n\n30000\n5000\nœÉ\nresidual\nmissing\n26.0088\n\n\n\n\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure¬†4, shows the normal distribution, ‚Äúbell-curve‚Äù, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"days\").value,\n  )\n  f1\nend\n\n\n\n\n\nFigure¬†4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-Œ± shortestcovint to be the shortest interval that contains a proportion 1-Œ± (defaults to 95%) of the bootstrap estimates of the parameter.\n\nDataFrame(shortestcovint(m1bstp))\n\n6√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n239.64\n265.228\n\n\n2\nŒ≤\nmissing\ndays\n7.42347\n13.1607\n\n\n3\nœÉ\nsubj\n(Intercept)\n10.1722\n33.0876\n\n\n4\nœÉ\nsubj\ndays\n2.9948\n7.66117\n\n\n5\nœÅ\nsubj\n(Intercept), days\n-0.40135\n1.0\n\n\n6\nœÉ\nresidual\nmissing\n22.701\n28.5016\n\n\n\n\n\n\nThe intervals look reasonable except that the upper bound on the interval for œÅ, the correlation coefficient, is 1.0 . It turns out that the estimates of œÅ have a great deal of variability.\nEven more alarming, some of these œÅ values are undefined (denoted NaN) because the way œÅ is calculated can create a division by zero.\n\ndescribe(@select(@subset(allpars, :type == \"œÅ\"), :value))\n\n1√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nFloat64\nNothing\nFloat64\nInt64\nDataType\n\n\n\n\n1\nvalue\nNaN\nNaN\n\nNaN\n0\nFloat64\n\n\n\n\n\n\nBecause there are several values on the boundary (œÅ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure¬†5.\n\n\nCode\nhist(\n  @subset(allpars, :type == \"œÅ\", isfinite(:value)).value;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\nFigure¬†5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure¬†6, show reasonable symmetry.\n\n\nCode\nbegin\n  œÉs = @subset(allpars, :type == \"œÉ\")\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual œÉ\"),\n    @subset(œÉs, :group == \"residual\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"days\").value,\n  )\n  f2\nend\n\n\n\n\n\nFigure¬†6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, Œ≤‚ÇÅ and Œ≤‚ÇÇ, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure¬†7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2√ó2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  vals = disallowmissing(\n    Array(\n      select(\n        unstack(DataFrame(m1bstp.Œ≤), :iter, :coefname, :Œ≤),\n        Not(:iter),\n      ),\n    ),\n  )\n  scatter(\n    vals;\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde(vals))\n  current_figure()\nend\n\n\n\n\n\nFigure¬†7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours\n\n\n\n\n\n\n6 References\n\n\nBalkin, T., Thome, D., Sing, H., Thomas, M., Redmond, D., Wesensten, N., Williams, J., Hall, S., & Belenky, G. (2000). Effects of sleep schedules on commercial motor vehicle driver performance (DOT-MC-00-133). Federal Motor Carrier Safety Administration. https://doi.org/10.21949/1503015.\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: A sleep dose-response study. Journal of Sleep Research, 12(1), 1‚Äì12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible high-performance data visualization for julia. Journal of Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nSarkar, D. (2008). Lattice: Mutivariate data visualization with r. Springer-Verlag GmbH. https://www.ebook.de/de/product/11429038/deepayan_sarkar_lattice.html\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "selection.html",
    "href": "selection.html",
    "title": "Raw score density",
    "section": "",
    "text": "Code\nusing Arrow\nusing CairoMakie\nusing DataFrames\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\n\n\n\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n\n\n\n\n\nlet\n  fdensity = Figure(; resolution=(1000, 500))\n  axs = Axis(fdensity[1, 1])\n  tdf = filter(:Test =&gt; ==(test), df)\n  colors = Makie.cgrad(:PuOr_4, 2; categorical=true, alpha=0.6)\n  if by_sex\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"female\"), tdf).score;\n      color=colors[1],\n      label=\"Girls\",\n    )\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"male\"), tdf).score;\n      color=colors[2],\n      label=\"Boys\",\n    )\n    axislegend(axs; position=:lt)\n  else\n    density!(axs, tdf.score)\n  end\n  fdensity\nend\n\n\n\n\n Back to top"
  },
  {
    "objectID": "largescaledesigned.html",
    "href": "largescaledesigned.html",
    "title": "A large-scale designed experiment",
    "section": "",
    "text": "Load the packages to be used.\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Effects\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\nusing SMLP2023\nusing SMLP2023: dataset\nusing StandardizedPredictors\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\nThe English Lexicon Project (Balota et al., 2007) was a large-scale multicenter study to examine properties of English words. It incorporated both a lexical decision task and a word recognition task. Different groups of subjects participated in the different tasks."
  },
  {
    "objectID": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "href": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "title": "A large-scale designed experiment",
    "section": "1.1 Trial-level data from the LDT",
    "text": "1.1 Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = dataset(:ELP_ldt_trial)\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  Int16\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"     =&gt; \"Trial-level data from Lexical Discrimination Task in the Engl‚Ä¶\n  \"reference\" =&gt; \"Balota et al. (2007), The English Lexicon Project, Behavior R‚Ä¶\n  \"source\"    =&gt; \"https://osf.io/n63s2\"\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day.\nNotice the metadata with a citation and a URL for the OSF project.\nWe convert to a DataFrame and add a Boolean column s2 which is true for trials in the second session.\n\nldttrial = @transform!(DataFrame(ldttrial), :s2 = :seq &gt; 2000)\ndescribe(ldttrial)\n\n6√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool"
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialexplore",
    "href": "largescaledesigned.html#sec-ldtinitialexplore",
    "title": "A large-scale designed experiment",
    "section": "1.2 Initial data exploration",
    "text": "1.2 Initial data exploration\nFrom the basic summary of ldttrial we can see that there are some questionable response times ‚Äî negative values and values over 32 seconds.\nBecause of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\n1.2.1 Summaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‚Äòi‚Äô in the names of these summaries (and an ‚Äòs‚Äô in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),               # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(\n    :wrdlen = Int8(length(:item)),\n    :ipropacc = :iacc / :ni\n  )\nend\n\n80962√ó7 DataFrame80937 rows omitted\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\n\n\n\n\n1\na\n35\n0\n26\n743.0\n1\n0.742857\n\n\n2\ne\n35\n0\n19\n824.0\n1\n0.542857\n\n\n3\naah\n34\n0\n21\n770.5\n3\n0.617647\n\n\n4\naal\n34\n0\n32\n702.5\n3\n0.941176\n\n\n5\nAaron\n33\n0\n31\n625.0\n5\n0.939394\n\n\n6\nAarod\n33\n0\n23\n810.0\n5\n0.69697\n\n\n7\naback\n34\n0\n15\n710.0\n5\n0.441176\n\n\n8\nahack\n34\n0\n34\n662.0\n5\n1.0\n\n\n9\nabacus\n34\n0\n17\n671.5\n6\n0.5\n\n\n10\nalacus\n34\n0\n29\n640.0\n6\n0.852941\n\n\n11\nabandon\n34\n0\n32\n641.0\n7\n0.941176\n\n\n12\nacandon\n34\n0\n33\n725.5\n7\n0.970588\n\n\n13\nabandoned\n34\n0\n31\n667.5\n9\n0.911765\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n80951\nzoology\n33\n0\n32\n623.0\n7\n0.969697\n\n\n80952\npoology\n33\n0\n32\n757.0\n7\n0.969697\n\n\n80953\nzoom\n35\n0\n34\n548.0\n4\n0.971429\n\n\n80954\nzool\n35\n0\n30\n633.0\n4\n0.857143\n\n\n80955\nzooming\n33\n0\n29\n617.0\n7\n0.878788\n\n\n80956\nsooming\n33\n0\n30\n721.0\n7\n0.909091\n\n\n80957\nzooms\n33\n0\n30\n598.0\n5\n0.909091\n\n\n80958\ncooms\n33\n0\n31\n660.0\n5\n0.939394\n\n\n80959\nzucchini\n34\n0\n29\n781.5\n8\n0.852941\n\n\n80960\nhucchini\n34\n0\n32\n727.5\n8\n0.941176\n\n\n80961\nZurich\n34\n0\n21\n731.5\n6\n0.617647\n\n\n80962\nZurach\n34\n0\n26\n811.0\n6\n0.764706\n\n\n\n\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nni\n33.9166\n30\n34.0\n37\n0\nInt64\n\n\n3\nimiss\n0.0169215\n0\n0.0\n2\n0\nInt64\n\n\n4\niacc\n29.0194\n0\n31.0\n37\n0\nInt64\n\n\n5\nimedianrt\n753.069\n458.0\n737.5\n1691.0\n0\nFloat64\n\n\n6\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n7\nipropacc\n0.855616\n0.0\n0.911765\n1.0\n0\nFloat64\n\n\n8\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n\n\n\n\nThis table shows that some of the items were never identified correctly. These are\n\nfilter(:iacc =&gt; iszero, byitem)\n\n9√ó8 DataFrame\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\nisword\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\nBool\n\n\n\n\n1\nbaobab\n34\n0\n0\n616.5\n6\n0.0\ntrue\n\n\n2\nhaulage\n34\n0\n0\n708.5\n7\n0.0\ntrue\n\n\n3\nleitmotif\n35\n0\n0\n688.0\n9\n0.0\ntrue\n\n\n4\nmiasmal\n35\n0\n0\n774.0\n7\n0.0\ntrue\n\n\n5\npeahen\n34\n0\n0\n684.0\n6\n0.0\ntrue\n\n\n6\nplosive\n34\n0\n0\n663.0\n7\n0.0\ntrue\n\n\n7\nplugugly\n33\n0\n0\n709.0\n8\n0.0\ntrue\n\n\n8\nposhest\n34\n0\n0\n740.0\n7\n0.0\ntrue\n\n\n9\nservo\n33\n0\n0\n697.0\n5\n0.0\ntrue\n\n\n\n\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a ‚Äúleft join‚Äù. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nUnion{Missing, Int8}\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nUnion{Missing, Bool}\n\n\n\n\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nBool\n\n\n\n\n\n\n\n\n\n\n\n\nNamed argument ‚Äúerror‚Äù\n\n\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\n\nA barchart of the word length counts, Figure¬†1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\nlet\n  wlen = 1:21\n  draw(\n    data((; wrdlen=wlen, count=counts(byitem.wrdlen, wlen))) *\n    mapping(:wrdlen =&gt; \"Length of word\", :count) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\nFigure¬†1: Histogram of word lengths in the items used in the lexical decision task.\n\n\n\n\nTo examine trends in accuracy by word length we create a plot of the response versus word length using just a scatterplot smoother. It would not be meaningful to plot the raw data because that would just provide horizontal lines at \\(\\pm 1\\). Instead we add the smoother to show the trend and omit the raw data points.\nThe resulting plot, Figure¬†2, shows the accuracy of identifying words is more-or-less constant at around 84%, but accuracy decreases with increasing word length for the nonwords.\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc))) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :acc =&gt; \"Accuracy\";\n    color=:isword,\n  ) * smooth(; span=0.75, degree=2, npoints=200);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†2: Smoothed curves of accuracy versus word length in the lexical decision task.\n\n\n\n\nFigure¬†2 may be a bit misleading because the largest discrepancies in proportion of accurate identifications of words and nonwords occur for the longest words, of which there are few. Over 95% of the words are between 4 and 13 characters in length\n\ncount(x -&gt; 4 ‚â§ x ‚â§ 13, byitem.wrdlen) / nrow(byitem)\n\n0.9654899829549666\n\n\nIf we restrict the smoother curves to this range, as in Figure¬†3,\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc), 4 ‚â§ :wrdlen ‚â§ 13)) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :acc =&gt; \"Accuracy\";\n    color=:isword,\n  ) * smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†3: Smoothed curves of accuracy versus word length in the range 4 to 13 characters in the lexical decision task.\n\n\n\n\nthe differences are less dramatic.\nAnother way to visualize these results is by plotting the proportion accurate versus word-length separately for words and non-words with the area of each marker proportional to the number of observations for that combinations (Figure¬†4).\n\n\nCode\nlet\n  itemsummry = combine(\n    groupby(byitem, [:wrdlen, :isword]),\n    :ni =&gt; sum,\n    :imiss =&gt; sum,\n    :iacc =&gt; sum,\n  )\n  @transform!(\n    itemsummry,\n    :iacc_mean = :iacc_sum / (:ni_sum - :imiss_sum)\n  )\n  @transform!(itemsummry, :msz = sqrt((:ni_sum - :imiss_sum) / 800))\n  draw(\n    data(itemsummry) * mapping(\n      :wrdlen =&gt; \"Word length\",\n      :iacc_mean =&gt; \"Proportion accurate\";\n      color=:isword,\n      markersize=:msz,\n    );\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\nFigure¬†4: Proportion of accurate trials in the LDT versus word length separately for words and non-words. The area of the marker is proportional to the number of observations represented.\n\n\n\n\nThe pattern in the range of word lengths with non-negligible counts (there are points in the plot down to word lengths of 1 and up to word lengths of 21 but these points are very small) is that the accuracy for words is nearly constant at about 84% and the accuracy fof nonwords is slightly higher until lengths of 13, at which point it falls off a bit.\n\n\n1.2.2 Summaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),               # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n814√ó6 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\n\n\n\nInt16\nInt64\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n3374\n0\n3158\n554.0\n0.935981\n\n\n2\n2\n3372\n1\n3031\n960.0\n0.898873\n\n\n3\n3\n3372\n3\n3006\n813.0\n0.891459\n\n\n4\n4\n3374\n1\n3062\n619.0\n0.907528\n\n\n5\n5\n3374\n0\n2574\n677.0\n0.762893\n\n\n6\n6\n3374\n0\n2927\n855.0\n0.867516\n\n\n7\n7\n3374\n4\n2877\n918.5\n0.852697\n\n\n8\n8\n3372\n1\n2731\n1310.0\n0.809905\n\n\n9\n9\n3374\n13\n2669\n657.0\n0.791049\n\n\n10\n10\n3374\n0\n2722\n757.0\n0.806758\n\n\n11\n11\n3374\n0\n2894\n632.0\n0.857736\n\n\n12\n12\n3374\n4\n2979\n692.0\n0.882928\n\n\n13\n13\n3374\n2\n2980\n1114.0\n0.883225\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n803\n805\n3374\n5\n2881\n534.0\n0.853883\n\n\n804\n806\n3374\n1\n3097\n841.5\n0.917902\n\n\n805\n807\n3374\n3\n2994\n704.0\n0.887374\n\n\n806\n808\n3374\n2\n2751\n630.5\n0.815353\n\n\n807\n809\n3372\n4\n2603\n627.0\n0.771945\n\n\n808\n810\n3374\n1\n3242\n603.5\n0.960877\n\n\n809\n811\n3374\n2\n2861\n827.0\n0.847955\n\n\n810\n812\n3372\n6\n3012\n471.0\n0.893238\n\n\n811\n813\n3372\n4\n2932\n823.0\n0.869514\n\n\n812\n814\n3374\n1\n3070\n773.0\n0.909899\n\n\n813\n815\n3374\n1\n3024\n602.0\n0.896266\n\n\n814\n816\n3374\n0\n2950\n733.0\n0.874333\n\n\n\n\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n6√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nns\n3373.41\n3370\n3374.0\n3374\n0\nInt64\n\n\n3\nsmiss\n1.68305\n0\n1.0\n22\n0\nInt64\n\n\n4\nsacc\n2886.33\n1727\n2928.0\n3286\n0\nInt64\n\n\n5\nsmedianrt\n760.992\n205.0\n735.0\n1804.0\n0\nFloat64\n\n\n6\nspropacc\n0.855613\n0.511855\n0.868031\n0.973918\n0\nFloat64\n\n\n\n\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure¬†5, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc =&gt; \"Proportion accurate\",\n    :smedianrt =&gt; \"Median response time (ms)\",\n  ) *\n  (visual(Scatter) + smooth())\n)\n\n\n\n\n\nFigure¬†5: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(&lt;(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(@subset(bysubj, :spropacc &gt; 0.9).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant‚Äôs mean response.\nAs described in Section¬†1.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ‚â§ :rt ‚â§ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.802\n1\n410.0\n816\n0\nInt16\n\n\n2\nseq\n1684.56\n1\n1684.0\n3374\n0\nInt16\n\n\n3\nacc\n0.859884\nfalse\n1.0\ntrue\n0\nBool\n\n\n4\nrt\n838.712\n200\n733.0\n3000\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.40663\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99244\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.500126\nfalse\n1.0\ntrue\n0\nBool\n\n\n9\nspropacc\n0.857169\n0.511855\n0.869295\n0.973918\n0\nFloat64\n\n\n\n\n\n\n\n\n1.2.3 Choice of response scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure¬†6, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt =&gt; \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†6: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\nThe density of the response speed, in responses per second, is shown in Figure¬†7.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :rt =&gt; (x -&gt; 1000 / x) =&gt; \"Response speed (s‚Åª¬π) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†7: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\nFigure¬†6 and Figure¬†7 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(\n  r -&gt; !ismissing(r.acc) && 200 &lt; r.rt &lt; 333,\n  eachrow(ldttrial),\n) / count(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\n\n\n\n\n\n\nUsing inline transformations vs defining new columns\n\n\n\n\n\nIf you examine the code for (fit-elpldtspeeddens?), you will see that the conversion from rt to speed is done inline rather than creating and storing a new variable in the DataFrame.\nI prefer to keep the DataFrame simple with the integer variables (e.g.¬†:rt) if possible.\nI recommend using the StandardizedPredictors.jl capabilities to center numeric variables or convert to zscores.\n\n\n\n\n\n1.2.4 Transformation of response and the form of the model\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure¬†8 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; \"Response time (ms)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\nFigure¬†8: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\nand Figure¬†9 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s‚Åª¬π)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\nFigure¬†9: Scatterplot smooths of response speed versus word length in the LDT.\n\n\n\n\nFor the most part the smoother lines in Figure¬†9 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study.\nFigure¬†10 shows a ‚Äúviolin plot‚Äù - the empirical density of the response speed by word length separately for words and nonwords. The lines on the plot are fit by linear regression.\n\n\nCode\nlet\n  plt = data(@subset(pruned, :wrdlen &gt; 3, :wrdlen &lt; 14))\n  plt *= mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s‚Åª¬π)\",\n    color=:isword,\n    side=:isword,\n  )\n  plt *= (visual(Violin) + linear(; interval=:confidence))\n  draw(plt, axis=(; limits=(nothing, (0.0, 2.8))))\nend\n\n\n\n\n\nFigure¬†10: Empirical density of response speed versus word length by word/non-word status, with lines fit by linear regression to each group."
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialmodel",
    "href": "largescaledesigned.html#sec-ldtinitialmodel",
    "title": "A large-scale designed experiment",
    "section": "1.3 Models with scalar random effects",
    "text": "1.3 Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\n1.3.1 Establish the contrasts\nBecause there are a large number of items in the data set it is important to assign a Grouping() contrast to item (and, less importantly, to subj). For the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts = Dict(\n  :subj =&gt; Grouping(),\n  :item =&gt; Grouping(),\n  :isword =&gt; EffectsCoding(; base=false),\n  :wrdlen =&gt; Center(8),\n)\n\nDict{Symbol, Any} with 4 entries:\n  :item   =&gt; Grouping()\n  :wrdlen =&gt; Center(8)\n  :isword =&gt; EffectsCoding(false, nothing)\n  :subj   =&gt; Grouping()\n\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  fit(MixedModel, form, pruned; contrasts)\nend\n\nMinimizing 53    Time: 0:00:07 ( 0.14  s/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nThe predicted response speed by word length and word/nonword status can be summarized as\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm01)\n\n10√ó6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.46555\n0.00903111\n1.45652\n1.47458\n\n\n2\n6\nfalse\n1.38947\n0.00898124\n1.38049\n1.39845\n\n\n3\n8\nfalse\n1.31338\n0.00896459\n1.30442\n1.32235\n\n\n4\n10\nfalse\n1.2373\n0.00898134\n1.22832\n1.24628\n\n\n5\n12\nfalse\n1.16121\n0.00903129\n1.15218\n1.17025\n\n\n6\n4\ntrue\n1.6351\n0.0090311\n1.62607\n1.64413\n\n\n7\n6\ntrue\n1.5367\n0.00898124\n1.52772\n1.54569\n\n\n8\n8\ntrue\n1.43831\n0.00896459\n1.42934\n1.44727\n\n\n9\n10\ntrue\n1.33991\n0.00898133\n1.33092\n1.34889\n\n\n10\n12\ntrue\n1.24151\n0.00903128\n1.23248\n1.25054\n\n\n\n\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  dat = @subset(pruned, :spropacc &gt; 0.8)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 53    Time: 0:00:04 (89.75 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3611\n0.0088\n153.98\n&lt;1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.72\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n&lt;1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm02)\n\n10√ó6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.45036\n0.00892467\n1.44144\n1.45929\n\n\n2\n6\nfalse\n1.37297\n0.00887101\n1.3641\n1.38184\n\n\n3\n8\nfalse\n1.29557\n0.00885309\n1.28672\n1.30443\n\n\n4\n10\nfalse\n1.21818\n0.00887111\n1.20931\n1.22705\n\n\n5\n12\nfalse\n1.14078\n0.00892487\n1.13186\n1.14971\n\n\n6\n4\ntrue\n1.62735\n0.00892466\n1.61842\n1.63627\n\n\n7\n6\ntrue\n1.52702\n0.00887101\n1.51815\n1.53589\n\n\n8\n8\ntrue\n1.4267\n0.00885308\n1.41784\n1.43555\n\n\n9\n10\ntrue\n1.32637\n0.0088711\n1.3175\n1.33524\n\n\n10\n12\ntrue\n1.22605\n0.00892484\n1.21712\n1.23497\n\n\n\n\n\n\nThe differences in the fixed-effects parameter estimates between a model fit to the full data set and one fit to the data from accurate responders only, are small.\nHowever, the random effects for the item, while highly correlated, are not perfectly correlated.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndisallowmissing!(\n  leftjoin!(\n    byitem,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:item]), [:item, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:item]), [:item, :elm02]);\n      on=:item,\n    ),\n    on=:item,\n  ),\n)\ndisallowmissing!(\n  leftjoin!(\n    bysubj,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:subj]), [:subj, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:subj]), [:subj, :elm02]);\n      on=:subj,\n    ),\n    on=:subj,\n  ); error=false,\n)\ndraw(\n  data(byitem) * mapping(\n    :elm01 =&gt; \"Conditional means of item random effects for model elm01\",\n    :elm02 =&gt; \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2);\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\nFigure¬†11: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure¬†11.\n\n\nFigure¬†11 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(byitem, :elm01, :elm02)))\n\n2√ó2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nsaveoptsum(\"./fits/elm01.json\", elm01);\n\n\nelm01a = restoreoptsum!(\n  let\n    form = @formula(\n      1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n    )\n    MixedModel(form, pruned; contrasts)\n  end,\n  \"./fits/elm01.json\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(dataset(\"ELP_ldt_item\"))\ndescribe(elpldtitem)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nOrtho_N\n1.53309\n0\n1.0\n25\n0\nInt8\n\n\n3\nBG_Sum\n13938.4\n11\n13026.0\n59803\n177\nUnion{Missing, Int32}\n\n\n4\nBG_Mean\n1921.25\n5.5\n1907.0\n6910.0\n177\nUnion{Missing, Float32}\n\n\n5\nBG_Freq_By_Pos\n2043.08\n0\n1928.0\n6985\n4\nUnion{Missing, Int16}\n\n\n6\nitemno\n40481.5\n1\n40481.5\n80962\n0\nInt32\n\n\n7\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n8\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n9\npairno\n20241.0\n1\n20241.0\n40481\n0\nInt32\n\n\n\n\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(dataset(\"ELP_ldt_subj\"))\ndescribe(elpldtsubj)\n\n20√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nAny\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nuniv\n\nKansas\n\nWayne State\n0\nString\n\n\n3\nsex\n\nf\n\nm\n8\nUnion{Missing, String}\n\n\n4\nDOB\n\n1938-06-07\n\n1984-11-14\n0\nDate\n\n\n5\nMEQ\n44.4932\n19.0\n44.0\n75.0\n8\nUnion{Missing, Float32}\n\n\n6\nvision\n5.51169\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n7\nhearing\n5.86101\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n8\neducatn\n8.89681\n1\n12.0\n28\n0\nInt8\n\n\n9\nncorrct\n29.8505\n5\n30.0\n40\n18\nUnion{Missing, Int8}\n\n\n10\nrawscor\n31.9925\n13\n32.0\n40\n18\nUnion{Missing, Int8}\n\n\n11\nvocabAge\n17.8123\n10.3\n17.8\n21.0\n19\nUnion{Missing, Float32}\n\n\n12\nshipTime\n3.0861\n0\n3.0\n9\n1\nUnion{Missing, Int8}\n\n\n13\nreadTime\n2.50215\n0.0\n2.0\n15.0\n1\nUnion{Missing, Float32}\n\n\n14\npreshlth\n5.48708\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n15\npasthlth\n4.92989\n0\n5.0\n7\n1\nUnion{Missing, Int8}\n\n\n16\nS1start\n\n2001-03-16T13:49:27\n2001-10-16T11:38:28.500\n2003-07-29T18:48:44\n0\nDateTime\n\n\n17\nS2start\n\n2001-03-19T10:00:35\n2001-10-19T14:24:19.500\n2003-07-30T13:07:45\n0\nDateTime\n\n\n18\nMEQstrt\n\n2001-03-22T18:32:00\n2001-10-23T11:26:13\n2003-07-30T14:30:49\n7\nUnion{Missing, DateTime}\n\n\n19\nfilename\n\n101DATA.LDT\n\nData998.LDT\n0\nString\n\n\n20\nfrstLang\n\nEnglish\n\nother\n8\nUnion{Missing, String}\n\n\n\n\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure¬†12,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n\n\nFigure¬†12: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\nshows definite distinctions between subjects because the widths of the prediction intervals are small compared to the range of the conditional modes. Also, there is at least one outlier with a conditional mode over 1.0.\nFigure¬†13 is the corresponding caterpillar plot for model elm02 fit to the data with inaccurate responders eliminated.\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm02, :subj),\n)\n\n\n\n\n\nFigure¬†13: Conditional means and 95% prediction intervals for subject random effects in elm02."
  },
  {
    "objectID": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "href": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "title": "A large-scale designed experiment",
    "section": "1.4 Random effects from the simple model related to covariates",
    "text": "1.4 Random effects from the simple model related to covariates\nThe random effects ‚Äúestimates‚Äù (technically they are ‚Äúconditional means‚Äù) from the simple model elm01 provide a measure of how much the item or subject differs from the population. (We use elm01 because the main difference between elm01 and elm02 are that some subjects were dropped before fitting elm02.)\nFor the item its length and word/non-word status have already been incorporated in the model. At this point the subjects are just being treated as a homogeneous population.\nThe random effects conditional means have been extracted and incorporated in the byitem and bysubj tables. Now add selected demographic and item-specific measures.\n\nitemextended = leftjoin(\n  byitem,\n  select(elpldtitem, 1:5);\n  on = :item,\n)\nsubjextended = leftjoin(\n  bysubj,\n  select(elpldtsubj, 1:3, :vocabAge);\n  on=:subj,\n)\n\n814√ó11 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\nelm01\nelm02\nuniv\nsex\nvocabAge\n\n\n\nInt16\nInt64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64?\nString?\nString?\nFloat32?\n\n\n\n\n1\n1\n3374\n0\n3158\n554.0\n0.935981\n0.411459\n0.426624\nMorehead\nm\n19.8\n\n\n2\n2\n3372\n1\n3031\n960.0\n0.898873\n-0.30907\n-0.293732\nMorehead\nf\n17.8\n\n\n3\n3\n3372\n3\n3006\n813.0\n0.891459\n-0.153078\n-0.139436\nMorehead\nf\n18.2\n\n\n4\n4\n3374\n1\n3062\n619.0\n0.907528\n0.213047\n0.22754\nMorehead\nf\n18.6\n\n\n5\n5\n3374\n0\n2574\n677.0\n0.762893\n0.0850349\nmissing\nMorehead\nf\n16.2\n\n\n6\n6\n3374\n0\n2927\n855.0\n0.867516\n-0.207356\n-0.192651\nMorehead\nf\n17.8\n\n\n7\n7\n3374\n4\n2877\n918.5\n0.852697\n-0.182201\n-0.166357\nMorehead\nf\n17.4\n\n\n8\n8\n3372\n1\n2731\n1310.0\n0.809905\n-0.541434\n-0.526828\nMorehead\nm\n16.2\n\n\n9\n9\n3374\n13\n2669\n657.0\n0.791049\n0.154926\nmissing\nMorehead\nf\n16.6\n\n\n10\n10\n3374\n0\n2722\n757.0\n0.806758\n-0.0541104\n-0.0403266\nMorehead\nf\n17.0\n\n\n11\n11\n3374\n0\n2894\n632.0\n0.857736\n0.217734\n0.231618\nMorehead\nf\n17.4\n\n\n12\n12\n3374\n4\n2979\n692.0\n0.882928\n0.062351\n0.0770981\nMorehead\nm\n18.2\n\n\n13\n13\n3374\n2\n2980\n1114.0\n0.883225\n-0.409761\n-0.3956\nMorehead\nf\n18.2\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n803\n805\n3374\n5\n2881\n534.0\n0.853883\n0.480461\n0.495683\nWash. Univ\nm\n19.0\n\n\n804\n806\n3374\n1\n3097\n841.5\n0.917902\n-0.1888\n-0.173376\nWash. Univ\nm\n19.8\n\n\n805\n807\n3374\n3\n2994\n704.0\n0.887374\n0.01919\n0.0338241\nWash. Univ\nm\n17.4\n\n\n806\n808\n3374\n2\n2751\n630.5\n0.815353\n0.199416\n0.214299\nWash. Univ\nf\n18.6\n\n\n807\n809\n3372\n4\n2603\n627.0\n0.771945\n0.2277\nmissing\nWash. Univ\nm\n15.1\n\n\n808\n810\n3374\n1\n3242\n603.5\n0.960877\n0.252522\n0.266822\nWash. Univ\nm\n19.8\n\n\n809\n811\n3374\n2\n2861\n827.0\n0.847955\n-0.158097\n-0.143568\nWash. Univ\nf\n16.2\n\n\n810\n812\n3372\n6\n3012\n471.0\n0.893238\n0.748427\n0.76354\nWash. Univ\nf\n19.8\n\n\n811\n813\n3372\n4\n2932\n823.0\n0.869514\n-0.167166\n-0.153846\nWash. Univ\nm\n17.4\n\n\n812\n814\n3374\n1\n3070\n773.0\n0.909899\n-0.0753662\n-0.0606956\nWash. Univ\nf\n18.6\n\n\n813\n815\n3374\n1\n3024\n602.0\n0.896266\n0.249134\n0.2643\nWash. Univ\nf\n18.6\n\n\n814\n816\n3374\n0\n2950\n733.0\n0.874333\n-0.0364596\n-0.0222916\nWash. Univ\nf\n17.8\n\n\n\n\n\n\nAs shown in Figure¬†14, there does not seem to be a strong relationship between vocabulary age and speed of response by subject.\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :vocabAge, :sex))) *\n  mapping(\n    :vocabAge =&gt; \"Vocabulary age (yr) of subject\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:sex,\n  ) * visual(Scatter; alpha=0.6)\n)\n\n\n\n\n\nFigure¬†14: Random effect for subject in model elm01 versus vocabulary age\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :univ))) *\n  mapping(\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\nFigure¬†15: Estimated density of random effects for subject in model elm01 by university\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm02, :univ))) *\n  mapping(\n    :elm02 =&gt; \"Random effect in model elm02 (accurate responders only)\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\nFigure¬†16: Estimated density of random effects for subject in model elm02, fit to accurate responders only, by university\n\n\n\n\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data(dropmissing(select(itemextended, :elm01, :BG_Mean, :isword))) *\n  mapping(\n    :BG_Mean =&gt; \"Mean bigram frequency\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2)\n)\n\n\n\n\n\nFigure¬†17: Random effect in model elm01 versus mean bigram frequency, by word/nonword status"
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2010) (see also script mmt_kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visiable in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2010). To keep matters somewhat simpler and comparable we ignore them in this script.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2010), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2010), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions. Therefore, the code in this script is largely the same as the one in kwdyz.jl.\nThere will be another vignette modelling the additional experimental manipulations of target size and orientation of cue rectangle. This analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); they were also used in a paper of GAMMs (Baayen et al., 2017). Data and R scripts are also available in R-package RePsychLing. Here we provide some of the corresponding analyses with MixedModels.jl and a much wider variety of visualizations of LMM results."
  },
  {
    "objectID": "kkl15.html#residual-over-fitted-plot",
    "href": "kkl15.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.1 Residual-over-fitted plot",
    "text": "5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nscatter(fitted(m1), residuals(m1); alpha=0.3)\n\n\n\n\n\nFigure¬†4: Residuals versus fitted values for model m1\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f =&gt; \"Fitted values from m1\", :r =&gt; \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†5: Heatmap of residuals versus fitted values for model m1"
  },
  {
    "objectID": "kkl15.html#q-q-plot",
    "href": "kkl15.html#q-q-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.2 Q-Q plot",
    "text": "5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\n\n\nCode\nqqnorm(m1; qqline=:none)\n\nFigure¬†6: ?(caption)\n\n\n\n\n\nCode\nqqnorm(m1_rt; qqline=:none)\n\nFigure¬†7: ?(caption)"
  },
  {
    "objectID": "kkl15.html#observed-and-theoretical-normal-distribution",
    "href": "kkl15.html#observed-and-theoretical-normal-distribution",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.3 Observed and theoretical normal distribution",
    "text": "5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is narrower than expected from a normal distribution. We can see this in this plot. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = (;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=repeat([\"residual\", \"normal\"]; inner=n),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure¬†8: Kernel density plot of the standardized residuals for model m1 versus a standard normal"
  },
  {
    "objectID": "kkl15.html#caterpillar-plot",
    "href": "kkl15.html#caterpillar-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "6.1 Caterpillar plot",
    "text": "6.1 Caterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n\n\nFigure¬†9: Prediction intervals of the subject random effects in model m1\n\n\n\n\nWhen we order the conditional modes for GM, that is (Intercept), the outlier subject S113 becomes visible; the associated experimental effects are not unusual.\n\n\nCode\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=1)\n\n\n\n\n\nFigure¬†10: Prediction intervals of the subject random effects in model m1 ordered by mean response\n\n\n\n\nThe catepillar plot also reveals that credibilty intervals are much shorter for subjects‚Äô Grand Means, shown in (Intercept), than the subjects‚Äô experimental effects, because the latter are based on difference scores not means. Moreover, credibility intervals are shorter for the first spatial effect sod than the other two effects, because the spatial effect involves the valid condition which yielded three times as many trials than the other three conditions. Consequently, the spatial effect is more reliable. Unfortunately, due to differences in scaling of the x-axis of the panels this effect must be inferred. One option to reveal this difference is to reparameterize the LMM such model parameters estimate the conditional modes for the levels of condition rather than the contrast-based effects. This is accomplished by replacing the 1 in the random effect term with 0, as shown next.\n\nm1L = let\n  form = @formula rt ~ 1 + CTR + (0 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4148\n48.05\n&lt;1e-99\n\n\n\nCTR: sod\n23.0720\n2.6414\n8.73\n&lt;1e-17\n60.1752\n\n\nCTR: dos\n13.0855\n1.4584\n8.97\n&lt;1e-18\n62.4743\n\n\nCTR: dod\n2.6860\n2.0608\n1.30\n0.1924\n71.5386\n\n\nCTR: val\n\n\n\n\n47.2895\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1L)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\nCTR: val\n2236.2946\n47.2895\n\n\n\n\n\n\nCTR: sod\n3621.0574\n60.1752\n+0.94\n\n\n\n\n\nCTR: dos\n3903.0376\n62.4743\n+0.93\n+0.99\n\n\n\n\nCTR: dod\n5117.7734\n71.5386\n+0.89\n+0.98\n+0.98\n\n\nResidual\n\n4254.2486\n65.2246\n\n\n\n\n\n\n\n\nThe caterpillar plot for levels shows the effect of the number of trials on credibility intervals; they are obviously much shorter for the valid condition. Note that this effect is not visible in a repeated-measure ANOVA with four condition means per subject as input.\n\n\nCode\n@chain m1L begin\n  ranefinfo\n  only\n  caterpillar!(Figure(; resolution=(800, 1000)), _; orderby=1)\nend\n\n\n\n\n\nFigure¬†11: Prediction intervals of the subject random effects in model m1L"
  },
  {
    "objectID": "kkl15.html#shrinkage-plot",
    "href": "kkl15.html#shrinkage-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "6.2 Shrinkage plot",
    "text": "6.2 Shrinkage plot\n\n6.2.1 Log-transformed reaction times (LMM m1)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1)\n\n\n\n\n\nFigure¬†12: Shrinkage plots of the subject random effects in model m1L\n\n\n\n\nThree of the CPs are imploded, but not the theoretically critical ones. These implosions did not occur (or were not as visible) for raw reaction times.\n\n\n6.2.2 Raw reaction times (LMM m1_rt)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1_rt)\n\n\n\n\n\nFigure¬†13: Shrinkage plots of the subject random effects in model m1_rt\n\n\n\n\nThe implosion is for three CP visualizations is not observed for raw reaction times. Interesting."
  },
  {
    "objectID": "kkl15.html#generate-a-bootstrap-sample",
    "href": "kkl15.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "7.1 Generate a bootstrap sample",
    "text": "7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\nsamp = parametricbootstrap(MersenneTwister(1234321), 2500, m1;\n                           optsum_overrides=(; ftol_rel=1e-8));\n\n\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n10√ó5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n5.65297\n\n\n2\n1\nŒ≤\nmissing\nCTR: sod\n0.0776158\n\n\n3\n1\nŒ≤\nmissing\nCTR: dos\n0.0391215\n\n\n4\n1\nŒ≤\nmissing\nCTR: dod\n-0.000438953\n\n\n5\n1\nœÉ\nSubj\n(Intercept)\n0.173459\n\n\n6\n1\nœÉ\nSubj\nCTR: sod\n0.0733919\n\n\n7\n1\nœÅ\nSubj\n(Intercept), CTR: sod\n0.384278\n\n\n8\n1\nœÉ\nSubj\nCTR: dos\n0.0122473\n\n\n9\n1\nœÅ\nSubj\n(Intercept), CTR: dos\n0.283016\n\n\n10\n1\nœÅ\nSubj\nCTR: sod, CTR: dos\n-0.0863335\n\n\n\n\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters\n\n37500"
  },
  {
    "objectID": "kkl15.html#shortest-coverage-interval",
    "href": "kkl15.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "7.2 Shortest coverage interval",
    "text": "7.2 Shortest coverage interval\n\nDataFrame(shortestcovint(samp))\n\n15√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n5.65481\n5.73072\n\n\n2\nŒ≤\nmissing\nCTR: sod\n0.0589656\n0.0895305\n\n\n3\nŒ≤\nmissing\nCTR: dos\n0.0333891\n0.0485955\n\n\n4\nŒ≤\nmissing\nCTR: dod\n-0.00924826\n0.0123093\n\n\n5\nœÉ\nSubj\n(Intercept)\n0.155353\n0.212131\n\n\n6\nœÉ\nSubj\nCTR: sod\n0.0568623\n0.0805194\n\n\n7\nœÅ\nSubj\n(Intercept), CTR: sod\n0.402269\n0.724905\n\n\n8\nœÉ\nSubj\nCTR: dos\n0.000515284\n0.016901\n\n\n9\nœÅ\nSubj\n(Intercept), CTR: dos\n-0.999995\n0.922292\n\n\n10\nœÅ\nSubj\nCTR: sod, CTR: dos\n-0.871473\n1.0\n\n\n11\nœÉ\nSubj\nCTR: dod\n0.0277575\n0.048869\n\n\n12\nœÅ\nSubj\n(Intercept), CTR: dod\n0.381958\n0.809421\n\n\n13\nœÅ\nSubj\nCTR: sod, CTR: dod\n0.456997\n0.910073\n\n\n14\nœÅ\nSubj\nCTR: dos, CTR: dod\n-0.835528\n0.85201\n\n\n15\nœÉ\nresidual\nmissing\n0.195883\n0.198241\n\n\n\n\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\nFigure¬†14: Ridge plot of fixed-effects bootstrap samples from model m1L"
  },
  {
    "objectID": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "7.3 Comparative density plots of bootstrapped parameter estimates\n\n7.3.1 Residual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"residual\")) *\n  mapping(:value =&gt; \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\nFigure¬†15: Kernel density estimate from bootstrap samples of the residual standard deviation for model m1L\n\n\n\n\n\n\n7.3.2 Fixed effects and associated variance components (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms and the associate variance component from .15 to .21. To keep the plot range small we do not include their densities here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" =&gt; \"GM\",\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n  \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n  \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n  \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n])\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value =&gt; \"Experimental effect size [ms]\";\n    color=:names =&gt; rn =&gt; \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†16: Kernel density estimate from bootstrap samples of the fixed effects for model m1L\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\" && :names ‚â† \"(Intercept)\") ) *\n  mapping(\n    :value =&gt; \"Standard deviations [ms]\";\n    color=:names =&gt; rn =&gt; \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†17: Kernel density estimate from bootstrap samples of the standard deviations for model m1L (excluding Grand Mean)\n\n\n\n\nThe VC are all very nicely defined.\n\n\n7.3.3 Correlation parameters (CPs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÅ\")) *\n  mapping(\n    :value =&gt; \"Correlation\";\n    color=:names =&gt; rn =&gt; \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†18: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl.\nThe three remaining CPs are not well defined for log-transformed reaction times; they only fit noise and should be removed. It is also possible that fitting the complex experimental design (including target size and rectangle orientation) will lead to more acceptable estimates. The corresponding plot based on LMM m1_rt for raw reaction times still shows them with very wide distributions, but acceptable."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "",
    "text": "This site provides materials for the Advanced frequentist methods stream of the Summer School on Statistical Methods to be held at the University of Potsdam, 11-15 September, 2023."
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.1 git",
    "text": "1.1 git\nWe will assume that you have git installed and are able to clone a repository from github. If not, Happy Git with R is a good place to learn about git for data science.\nThis website is built using quarto, described below, from the repository. Clone this repository with, e.g.\ngit clone https://github.com/RePsychLing/SMLP2023"
  },
  {
    "objectID": "index.html#julia-programming-language",
    "href": "index.html#julia-programming-language",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.2 Julia Programming Language",
    "text": "1.2 Julia Programming Language\nWe will use Julia v1.9 in the summer school. We recommend using Juliaup to install and manage Julia versions. Juliaup makes it trivial to upgrade to new Julia releases or even use old ones. Alternatively, you can download the version appropriate for your setup from here: Julia Programming Language"
  },
  {
    "objectID": "index.html#visual-studio-code-vs-code",
    "href": "index.html#visual-studio-code-vs-code",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.3 Visual Studio Code (VS Code)",
    "text": "1.3 Visual Studio Code (VS Code)\nWe will use VS Code IDE, that is Julia : VS Code ~ R : RStudio. You can download the version appropriate for your setup from here: VS Code"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nThe web site and other documents for this course are rendered using a knitr-like system called Quarto. You can download the version appropriate for your setup from here: quarto"
  },
  {
    "objectID": "fggk21.html",
    "href": "fggk21.html",
    "title": "Basics with Emotikon Project",
    "section": "",
    "text": "This script uses a subset of data reported in F√ºhner et al. (2021). To circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the article.\nThe script is structured in four main sections:"
  },
  {
    "objectID": "fggk21.html#read-data",
    "href": "fggk21.html#read-data",
    "title": "Basics with Emotikon Project",
    "section": "3.1 Read data",
    "text": "3.1 Read data\n\ndf = DataFrame(dataset(:fggk21))\ntransform!(df,\n    :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1,\n    :Sex =&gt; categorical =&gt; :Sex,\n    :Test =&gt; categorical =&gt; :Test,\n  )\nlevels!(df.Sex, [\"male\", \"female\"])\nrecode!(df.Sex, \"male\" =&gt; \"Boys\", \"female\" =&gt; \"Girls\")\nlevels!(df.Test, [\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\nrecode!(\n  df.Test,\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n)\ndescribe(df)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nBoys\n\nGirls\n0\nCategoricalValue{String, UInt32}\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nEndurance\n\nPowerUP\n0\nCategoricalValue{String, UInt32}\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n\n\n\n\n\n3.1.1 Transformations\nWe center age at 8.5 years and compute z-scores for each Test. With these variables the data frame df contains all variables used for the final model in the original publication.\n\nselect!(groupby(df, :Test),  Not(:score), :score =&gt; zscore =&gt; :zScore)\n\n525126√ó8 DataFrame525101 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat‚Ä¶\nString\nString\nString\nCat‚Ä¶\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.7913\n\n\n2\nPowerUP\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0622317\n\n\n3\nPowerLOW\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0336567\n\n\n4\nCoordination\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.46874\n\n\n5\nEndurance\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n0.331058\n\n\n6\nSpeed\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n1.15471\n\n\n7\nPowerUP\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.498354\n\n\n8\nPowerLOW\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.498822\n\n\n9\nCoordination\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.9773\n\n\n10\nEndurance\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.574056\n\n\n11\nSpeed\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.0551481\n\n\n12\nPowerUP\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.218061\n\n\n13\nPowerLOW\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n-0.757248\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nCoordination\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-1.43175\n\n\n525116\nEndurance\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-0.944681\n\n\n525117\nSpeed\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.31086\n\n\n525118\nPowerUP\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.0779146\n\n\n525119\nPowerLOW\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-0.137027\n\n\n525120\nCoordination\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-1.8077\n\n\n525121\nEndurance\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.513306\n\n\n525122\nSpeed\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0551481\n\n\n525123\nPowerUP\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0779146\n\n\n525124\nPowerLOW\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-1.32578\n\n\n525125\nCoordination\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.473217\n\n\n525126\nEndurance\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-0.0941883"
  },
  {
    "objectID": "fggk21.html#extract-a-stratified-subsample",
    "href": "fggk21.html#extract-a-stratified-subsample",
    "title": "Basics with Emotikon Project",
    "section": "3.2 Extract a stratified subsample",
    "text": "3.2 Extract a stratified subsample\nFor the prupose of the tutorial, we extract a random sample of 1000 boys and 1000 girls. Child, School, and Cohort are grouping variables. Traditionally, they are called random factors because the units (levels) of the factor are assumed to be a random sample from the population of their units (levels).\nCohort has only nine ‚Äúgroups‚Äù and could have been included as a set of polynomical fixed-effect contrasts rather than a random factor. This choice warrants a short excursion: The secular trends are very different for different tests and require the inclusion of interaction terms with Test contrasts (see Figure 4 in (F√ºhner et al., 2021). The authors opted to absorb these effects in cohort-related variance components for the Test contrasts and plan to address the details of secular changes in a separate analysis.\nFor complex designs, when they are in the theoretical focus of an article, factors and covariates should be specified as part of the fixed effects. If they are not in the theoretical focus, but serve as statistical control variables, they could be put in the RES - if supported by the data.\nStratified sampling: We generate a Child table with information about children. MersenneTwister(42) specifies 42 as the seed for the random number generator to ensure reproducibility of the stratification. For a different pattern of results choose, for example, 84. We randomly sample 1000 boys and 1000 girls from this table; they are stored in samp. Then, we extract the corresponding subset of these children‚Äôs test scores from df and store them dat.\n\nChild = unique(select(df, :Cohort, :School, :Child, :Sex, :age))\nsample = let\n  rng = MersenneTwister(42)\n  combine(\n    groupby(Child, :Sex), x -&gt; x[rand(rng, 1:nrow(x), 1000), :]\n  )\nend\ninsamp(x) = x ‚àà sample.Child\ndat = @subset(df, insamp(:Child))\n\n9663√ó8 DataFrame9638 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat‚Ä¶\nString\nString\nString\nCat‚Ä¶\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n0.0551481\n\n\n2\nPowerUP\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.342524\n\n\n3\nPowerLOW\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n0.74162\n\n\n4\nCoordination\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.209186\n\n\n5\nEndurance\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.127938\n\n\n6\nSpeed\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.422921\n\n\n7\nPowerUP\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.762963\n\n\n8\nPowerLOW\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.188712\n\n\n9\nCoordination\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n0.81382\n\n\n10\nEndurance\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n0.452557\n\n\n11\nSpeed\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.859704\n\n\n12\nPowerUP\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.218061\n\n\n13\nPowerLOW\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.74162\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n9652\nCoordination\n2018\nS111508\nC117805\nGirls\n9.10609\n0.606092\n-0.95589\n\n\n9653\nEndurance\n2018\nS111508\nC117805\nGirls\n9.10609\n0.606092\n-1.67367\n\n\n9654\nSpeed\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.859704\n\n\n9655\nPowerUP\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.6385\n\n\n9656\nPowerLOW\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.483194\n\n\n9657\nCoordination\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.708474\n\n\n9658\nEndurance\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n-0.337186\n\n\n9659\nSpeed\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.578748\n\n\n9660\nPowerUP\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.498354\n\n\n9661\nPowerLOW\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n-0.550508\n\n\n9662\nCoordination\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.538978\n\n\n9663\nEndurance\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.938553\n\n\n\n\n\n\nDue to missing scores for some tests we have about 2% less than 10,000 observtions."
  },
  {
    "objectID": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "href": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "title": "Basics with Emotikon Project",
    "section": "3.3 No evidence for age x Sex x Test interaction",
    "text": "3.3 No evidence for age x Sex x Test interaction\nThe main results are captured in the figure constructed in this section. We build it both for the full data and the stratified subset.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nBoys\nSpeed\n8.0\n-0.0265138\n1223\n\n\n2\nBoys\nPowerUP\n8.0\n0.026973\n1227\n\n\n3\nBoys\nPowerLOW\n8.0\n0.121609\n1227\n\n\n4\nBoys\nCoordination\n8.0\n-0.0571726\n1186\n\n\n5\nBoys\nEndurance\n8.0\n0.292695\n1210\n\n\n6\nGirls\nSpeed\n8.0\n-0.35164\n1411\n\n\n7\nGirls\nPowerUP\n8.0\n-0.610355\n1417\n\n\n8\nGirls\nPowerLOW\n8.0\n-0.279872\n1418\n\n\n9\nGirls\nCoordination\n8.0\n-0.268221\n1381\n\n\n10\nGirls\nEndurance\n8.0\n-0.245573\n1387\n\n\n11\nBoys\nSpeed\n8.1\n0.0608397\n3042\n\n\n12\nBoys\nPowerUP\n8.1\n0.0955413\n3069\n\n\n13\nBoys\nPowerLOW\n8.1\n0.123099\n3069\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nBoys\nCoordination\n9.0\n0.254973\n4049\n\n\n110\nBoys\nEndurance\n9.0\n0.258082\n4034\n\n\n111\nGirls\nSpeed\n9.1\n-0.0286172\n1154\n\n\n112\nGirls\nPowerUP\n9.1\n-0.0752301\n1186\n\n\n113\nGirls\nPowerLOW\n9.1\n-0.094587\n1174\n\n\n114\nGirls\nCoordination\n9.1\n0.00276252\n1162\n\n\n115\nGirls\nEndurance\n9.1\n-0.235591\n1150\n\n\n116\nBoys\nSpeed\n9.1\n0.325745\n1303\n\n\n117\nBoys\nPowerUP\n9.1\n0.616416\n1320\n\n\n118\nBoys\nPowerLOW\n9.1\n0.267577\n1310\n\n\n119\nBoys\nCoordination\n9.1\n0.254342\n1297\n\n\n120\nBoys\nEndurance\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n3.3.1 Figure(s) of interaction\nThe core results of the article are reported in Figure 2 of F√ºhner et al. (2021). In summary:\n\nMain effects of age and Sex: There are developmental gains in the ninth year of life; boys outperform girls. There is no main effect of Test because of z-scoring.\nInteractions of Test and age: Tests differ in how much children improve during the year (i.e., the magnitude of developmental gain), that is slopes depend on Test.\nInteractions of Test and Sex: The sex difference is test dependent, that is the difference between the slopes depends on Test.\nThe most distinctive result is the absence of evidence for an age x Sex x Test interaction, that is the slopes for boys and girls are statistically parallel for each of the five tests.\n\n\n\nCode\nlet\n  design1 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines1 = design1 * linear()\n  means1 = design1 * visual(Scatter; markersize=5)\n  draw(data(df2) * means1 + data(df) * lines1;)\nend\n\n\n\n\n\nFigure¬†1: Age trends by sex for each Test for the full data set\n\n\n\n\nFigure¬†1 shows performance differences for the full set of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\nWhat do the results look like for the stratified subsample? Here the parallelism is much less clear. In the final LMM we test whether the two regression lines in each of the five panels are statistically parallel for this subset of data. That is, we test the interaction of Sex and age as nested within the levels of Test. Most people want to know the signficance of these five Sex x age interactions.\nThe theoretical focus of the article, however, is on comparisons between tests displayed next to each other. We ask whether the degree of parallelism is statistically the same for Endurance and Coordination (H1), Coordination and Speed (H2), Speed and PowerLOW (H3), and PowerLow and PowerUP (H4). Hypotheses H1 to H4 require Sequential Difference contrasts c1 to c4 for Test; they are tested as fixed effects for`H1 x age x Sex, H2 x age x Sex, H3 x age x Sex, and H4 x age x Sex.\n\n\nCode\ndat2 = combine(\n  groupby(\n    select(dat, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nGirls\nSpeed\n8.0\n-0.323114\n28\n\n\n2\nGirls\nPowerUP\n8.0\n-0.590476\n26\n\n\n3\nGirls\nPowerLOW\n8.0\n0.0677992\n27\n\n\n4\nGirls\nCoordination\n8.0\n0.0273318\n25\n\n\n5\nGirls\nEndurance\n8.0\n-0.17337\n26\n\n\n6\nBoys\nSpeed\n8.0\n0.394634\n19\n\n\n7\nBoys\nPowerUP\n8.0\n0.328703\n19\n\n\n8\nBoys\nPowerLOW\n8.0\n0.105077\n19\n\n\n9\nBoys\nCoordination\n8.0\n-0.170018\n19\n\n\n10\nBoys\nEndurance\n8.0\n0.407084\n19\n\n\n11\nGirls\nSpeed\n8.1\n-0.205934\n54\n\n\n12\nGirls\nPowerUP\n8.1\n-0.653961\n54\n\n\n13\nGirls\nPowerLOW\n8.1\n-0.157127\n54\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nGirls\nCoordination\n9.0\n-0.000188842\n68\n\n\n110\nGirls\nEndurance\n9.0\n-0.234448\n68\n\n\n111\nGirls\nSpeed\n9.1\n-0.285047\n25\n\n\n112\nGirls\nPowerUP\n9.1\n-0.112684\n25\n\n\n113\nGirls\nPowerLOW\n9.1\n-0.117645\n24\n\n\n114\nGirls\nCoordination\n9.1\n-0.127844\n25\n\n\n115\nGirls\nEndurance\n9.1\n-0.587496\n24\n\n\n116\nBoys\nSpeed\n9.1\n0.379808\n17\n\n\n117\nBoys\nPowerUP\n9.1\n0.523085\n17\n\n\n118\nBoys\nPowerLOW\n9.1\n0.294696\n17\n\n\n119\nBoys\nCoordination\n9.1\n0.209309\n16\n\n\n120\nBoys\nEndurance\n9.1\n0.266512\n16\n\n\n\n\n\n\n\n\nCode\nlet\n  design2 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines2 = design2 * linear()\n  means2 = design2 * visual(Scatter; markersize=5)\n  draw(data(dat2) * means2 + data(dat) * lines2;)\nend\n\n\n\n\n\nFigure¬†2: Age trends by sex for each Test for the stratified sample\n\n\n\n\nFigure¬†2 Performance differences for subset of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\n\n\n3.3.2 Regression on age by Sex for each Test\nAnother set of relevant statistics are the slopes for the regression of performance on age for boys and girls in each of the five tests. The lines in Figures 1 and 2, however, are computed directly from the raw data with the linear() command.\n\ncombine(\n  groupby(df, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10√ó3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nTuple‚Ä¶\n\n\n\n\n1\nBoys\nEndurance\n(0.00256718, 0.0291899)\n\n\n2\nBoys\nCoordination\n(-2.47279, 0.302819)\n\n\n3\nBoys\nSpeed\n(-2.12689, 0.267153)\n\n\n4\nBoys\nPowerLOW\n(-1.4307, 0.189659)\n\n\n5\nBoys\nPowerUP\n(-4.35864, 0.549005)\n\n\n6\nGirls\nEndurance\n(-0.692022, 0.0523217)\n\n\n7\nGirls\nCoordination\n(-2.50524, 0.279119)\n\n\n8\nGirls\nSpeed\n(-2.34431, 0.255687)\n\n\n9\nGirls\nPowerLOW\n(-1.87241, 0.196917)\n\n\n10\nGirls\nPowerUP\n(-4.82271, 0.524799)\n\n\n\n\n\n\n\ncombine(\n  groupby(dat, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10√ó3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nTuple‚Ä¶\n\n\n\n\n1\nBoys\nEndurance\n(0.39203, -0.0150694)\n\n\n2\nBoys\nCoordination\n(-3.33518, 0.401051)\n\n\n3\nBoys\nSpeed\n(-1.75685, 0.228662)\n\n\n4\nBoys\nPowerLOW\n(-1.06646, 0.151546)\n\n\n5\nBoys\nPowerUP\n(-4.15536, 0.5245)\n\n\n6\nGirls\nEndurance\n(0.941712, -0.141158)\n\n\n7\nGirls\nCoordination\n(-0.681898, 0.0714891)\n\n\n8\nGirls\nSpeed\n(-0.786382, 0.0725931)\n\n\n9\nGirls\nPowerLOW\n(-0.208472, 0.00150731)\n\n\n10\nGirls\nPowerUP\n(-5.23593, 0.570806)"
  },
  {
    "objectID": "fggk21.html#seqdiffcoding-of-test",
    "href": "fggk21.html#seqdiffcoding-of-test",
    "title": "Basics with Emotikon Project",
    "section": "3.4 SeqDiffCoding of Test",
    "text": "3.4 SeqDiffCoding of Test\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nH1: Star_r - Run (2-1)\nH2: S20_r - Star_r (3-2)\nH3: SLJ - S20_r (4-3)\nH4: BPT - SLJ (5-4)\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe.\nNote that random factors Child, School, and Cohort are declared as Grouping variables. Technically, this specification is required for variables with a very large number of levels (e.g., 100K+ children). We recommend the explicit specification for all random factors as a general coding style.\nThe first command recodes names indicating the physical fitness components used in the above figures and tables back to the shorter actual test names. This reduces clutter in LMM outputs.\n\nrecode!(\n  dat.Test,\n  \"Endurance\" =&gt; \"Run\",\n  \"Coordination\" =&gt; \"Star_r\",\n  \"Speed\" =&gt; \"S20_r\",\n  \"PowerLOW\" =&gt; \"SLJ\",\n  \"PowerUP\" =&gt; \"BMT\",\n)\ncontrasts = merge(\n  Dict(nm =&gt; SeqDiffCoding() for nm in (:Test, :Sex)),\n  Dict(nm =&gt; Grouping() for nm in (:Child, :School, :Cohort)),\n);\n\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitations on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\nVarious options for contrast coding are the topic of the MixedModelsTutorial_contrasts_emotikon.jl and MixedModelsTutorial_contrasts_kwdyz.jl notebooks."
  },
  {
    "objectID": "fggk21.html#lmm-m_ovi",
    "href": "fggk21.html#lmm-m_ovi",
    "title": "Basics with Emotikon Project",
    "section": "4.1 LMM m_ovi",
    "text": "4.1 LMM m_ovi\nIn its random-effect structure (RES) we only vary intercepts (i.e., Grand Means) for School (LMM m_ovi), that is we allow that the schools differ in the average fitness of its children, average over the five tests.\nIt is well known that such a simple RES is likely to be anti-conservative with respect to fixed-effect test statistics.\n\nm_ovi = let\n  f = @formula zScore ~ 1 + Test * Sex * a1 + (1 | School)\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 19    Time: 0:00:00 (11.84 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0174\n0.0198\n-0.88\n0.3787\n0.3417\n\n\nTest: Star_r\n-0.0026\n0.0303\n-0.09\n0.9304\n\n\n\nTest: S20_r\n0.0097\n0.0302\n0.32\n0.7469\n\n\n\nTest: SLJ\n0.0033\n0.0300\n0.11\n0.9119\n\n\n\nTest: BMT\n-0.0539\n0.0299\n-1.80\n0.0711\n\n\n\nSex: Girls\n-0.4372\n0.0205\n-21.35\n&lt;1e-99\n\n\n\na1\n0.1761\n0.0354\n4.97\n&lt;1e-06\n\n\n\nTest: Star_r & Sex: Girls\n0.3802\n0.0605\n6.28\n&lt;1e-09\n\n\n\nTest: S20_r & Sex: Girls\n-0.2038\n0.0604\n-3.38\n0.0007\n\n\n\nTest: SLJ & Sex: Girls\n-0.0669\n0.0600\n-1.11\n0.2651\n\n\n\nTest: BMT & Sex: Girls\n-0.2730\n0.0598\n-4.57\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3146\n0.1038\n3.03\n0.0024\n\n\n\nTest: S20_r & a1\n-0.0767\n0.1034\n-0.74\n0.4584\n\n\n\nTest: SLJ & a1\n-0.0745\n0.1027\n-0.73\n0.4683\n\n\n\nTest: BMT & a1\n0.4724\n0.1025\n4.61\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1044\n0.0709\n-1.47\n0.1405\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1912\n0.2076\n-0.92\n0.3570\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1742\n0.2068\n0.84\n0.3997\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0084\n0.2054\n0.04\n0.9672\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1923\n0.2050\n0.94\n0.3482\n\n\n\nResidual\n0.9152\n\n\n\n\n\n\n\n\n\nIs the model singular (overparameterized, degenerate)? In other words: Is the model not supported by the data?\n\nissingular(m_ovi)\n\nfalse\n\n\nModels varying only in intercepts are almost always supported by the data."
  },
  {
    "objectID": "fggk21.html#lmm-m_zcp",
    "href": "fggk21.html#lmm-m_zcp",
    "title": "Basics with Emotikon Project",
    "section": "4.2 LMM m_zcp",
    "text": "4.2 LMM m_zcp\nIn this LMM we allow that schools differ not only in GM, but also in the size of the four contrasts defined for Test, in the difference between boys and girls (Sex) and the developmental gain children achieve within the third grade (age).\nWe assume that there is covariance associated with these CPs beyond residual noise, that is we assume that there is no detectable evidence in the data that the CPs are different from zero.\n\nm_zcp = let\n  f = @formula(\n    zScore ~\n      1 + Test * Sex * a1 + zerocorr(1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 148   Time: 0:00:00 ( 0.86 ms/it)\n  objective:  25923.460469750276\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0247\n0.0198\n-1.24\n0.2136\n0.3257\n\n\nTest: Star_r\n0.0029\n0.0311\n0.09\n0.9254\n0.2208\n\n\nTest: S20_r\n0.0102\n0.0286\n0.36\n0.7209\n0.0503\n\n\nTest: SLJ\n0.0049\n0.0283\n0.17\n0.8634\n0.0000\n\n\nTest: BMT\n-0.0460\n0.0309\n-1.49\n0.1368\n0.2286\n\n\nSex: Girls\n-0.4399\n0.0322\n-13.64\n&lt;1e-41\n0.4503\n\n\na1\n0.1840\n0.0573\n3.21\n0.0013\n0.7780\n\n\nTest: Star_r & Sex: Girls\n0.3784\n0.0577\n6.55\n&lt;1e-10\n\n\n\nTest: S20_r & Sex: Girls\n-0.2019\n0.0570\n-3.54\n0.0004\n\n\n\nTest: SLJ & Sex: Girls\n-0.0666\n0.0566\n-1.18\n0.2392\n\n\n\nTest: BMT & Sex: Girls\n-0.2780\n0.0570\n-4.87\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3101\n0.0992\n3.13\n0.0018\n\n\n\nTest: S20_r & a1\n-0.0728\n0.0976\n-0.75\n0.4558\n\n\n\nTest: SLJ & a1\n-0.0784\n0.0968\n-0.81\n0.4180\n\n\n\nTest: BMT & a1\n0.4741\n0.0979\n4.84\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1453\n0.0791\n-1.84\n0.0662\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1570\n0.1982\n-0.79\n0.4284\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1799\n0.1952\n0.92\n0.3566\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0061\n0.1936\n0.03\n0.9749\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1657\n0.1958\n0.85\n0.3974\n\n\n\nResidual\n0.8623\n\n\n\n\n\n\n\n\n\nDepending on sampling, this model estimating variance components for School may or may not be supported by the data.\n\nissingular(m_zcp)\n\ntrue"
  },
  {
    "objectID": "fggk21.html#lmm-m_cpx",
    "href": "fggk21.html#lmm-m_cpx",
    "title": "Basics with Emotikon Project",
    "section": "4.3 LMM m_cpx",
    "text": "4.3 LMM m_cpx\nIn the complex LMM investigated in this sequence we give up the assumption of zero-correlation between VCs.\n\nm_cpx = let\n  f = @formula(\n    zScore ~ 1 + Test * Sex * a1 + (1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 2857      Time: 0:00:02 ( 0.76 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0268\n0.0198\n-1.35\n0.1757\n0.3264\n\n\nTest: Star_r\n0.0056\n0.0310\n0.18\n0.8574\n0.2203\n\n\nTest: S20_r\n0.0104\n0.0301\n0.35\n0.7294\n0.1755\n\n\nTest: SLJ\n0.0024\n0.0294\n0.08\n0.9355\n0.1476\n\n\nTest: BMT\n-0.0401\n0.0305\n-1.32\n0.1881\n0.2242\n\n\nSex: Girls\n-0.4356\n0.0320\n-13.61\n&lt;1e-41\n0.4482\n\n\na1\n0.1877\n0.0566\n3.31\n0.0009\n0.7686\n\n\nTest: Star_r & Sex: Girls\n0.3759\n0.0575\n6.53\n&lt;1e-10\n\n\n\nTest: S20_r & Sex: Girls\n-0.1969\n0.0572\n-3.44\n0.0006\n\n\n\nTest: SLJ & Sex: Girls\n-0.0673\n0.0567\n-1.19\n0.2349\n\n\n\nTest: BMT & Sex: Girls\n-0.2717\n0.0566\n-4.80\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3112\n0.0988\n3.15\n0.0016\n\n\n\nTest: S20_r & a1\n-0.0707\n0.0981\n-0.72\n0.4710\n\n\n\nTest: SLJ & a1\n-0.0642\n0.0971\n-0.66\n0.5085\n\n\n\nTest: BMT & a1\n0.4678\n0.0971\n4.82\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1405\n0.0783\n-1.79\n0.0728\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1632\n0.1975\n-0.83\n0.4085\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1862\n0.1961\n0.95\n0.3424\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0011\n0.1941\n0.01\n0.9954\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1639\n0.1942\n0.84\n0.3987\n\n\n\nResidual\n0.8598\n\n\n\n\n\n\n\n\n\nWe also need to see the VCs and CPs of the random-effect structure (RES).\n\nVarCorr(m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.106506\n0.326352\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.048510\n0.220251\n+0.10\n\n\n\n\n\n\n\n\nTest: S20_r\n0.030805\n0.175513\n-0.19\n-0.22\n\n\n\n\n\n\n\nTest: SLJ\n0.021774\n0.147561\n-0.10\n+0.07\n-0.83\n\n\n\n\n\n\nTest: BMT\n0.050266\n0.224200\n-0.75\n+0.43\n-0.13\n+0.16\n\n\n\n\n\nSex: Girls\n0.200891\n0.448209\n-0.04\n+0.25\n+0.20\n-0.26\n-0.10\n\n\n\n\na1\n0.590683\n0.768559\n+0.02\n-0.13\n+0.03\n-0.49\n+0.17\n-0.06\n\n\nResidual\n\n0.739305\n0.859829\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\ntrue\n\n\nThe complex model may or may not be supported by the data."
  },
  {
    "objectID": "fggk21.html#model-comparisons",
    "href": "fggk21.html#model-comparisons",
    "title": "Basics with Emotikon Project",
    "section": "4.4 Model comparisons",
    "text": "4.4 Model comparisons\nThe checks of model singularity indicate that the three models are supported by the data. Does model complexification also increase the goodness of fit or are we only fitting noise?\n\n4.4.1 LRT and goodness-of-fit statistics\nAs the thee models are strictly hierarchically nested, we compare them with a likelihood-ratio tests (LRT) and AIC and BIC goodness-of-fit statistics derived from them.\n\nMixedModels.likelihoodratiotest(m_ovi, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 | School)\n22\n26273\n\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + zerocorr(1 + Test + Sex + a1 | School)\n28\n25923\n349\n6\n&lt;1e-71\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 + Test + Sex + a1 | School)\n49\n25864\n59\n21\n&lt;1e-04\n\n\n\n\n\n\n\nCode\ngof_summary = let\n  nms = [:m_ovi, :m_zcp, :m_cpx]\n  mods = eval.(nms)\n  DataFrame(;\n    name=nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nm_ovi\n22\n26272.9\n26316.9\n26317.0\n26474.8\n\n\n2\nm_zcp\n28\n25923.5\n25979.5\n25979.6\n26180.4\n\n\n3\nm_cpx\n49\n25864.4\n25962.4\n25962.9\n26314.0\n\n\n\n\n\n\nThese statistics will depend on sampling. In general, smaller deviance, AIC, and BIC indicate an improvement in goodness of fit. Usually, œá¬≤ should be larger than the associated degrees of freedom; for AIC and BIC the decrease should amount to more than 5, according to some literature. Severity of meeting these criteria increases from deviance to AIC to BIC. Therefore, it is not always the case that the criteria are unanimous in their verdict. Basicly, the more confirmatory the analysis, the more one may go with deviance and AIC; for exploratory analyses the BIC is probably a better guide. There are grey zones here.\n\n\n4.4.2 Comparing fixed effects of m_ovi, m_zcp, and m_cpx\nWe check whether enriching the RES changed the significance of fixed effects in the final model.\n\n\nCode\nm_ovi_fe = DataFrame(coeftable(m_ovi));\nm_zcp_fe = DataFrame(coeftable(m_zcp));\nm_cpx_fe = DataFrame(coeftable(m_cpx));\nm_all = hcat(\n  m_ovi_fe[:, [1, 2, 4]],\n  leftjoin(\n    m_zcp_fe[:, [1, 2, 4]],\n    m_cpx_fe[:, [1, 2, 4]];\n    on=:Name,\n    makeunique=true,\n  );\n  makeunique=true,\n)\nrename!(\n  m_all,\n  \"Coef.\" =&gt; \"b_ovi\",\n  \"Coef._2\" =&gt; \"b_zcp\",\n  \"Coef._1\" =&gt; \"b_cpx\",\n  \"z\" =&gt; \"z_ovi\",\n  \"z_2\" =&gt; \"z_zcp\",\n  \"z_1\" =&gt; \"z_cpx\",\n)\nm_all2 =\n  round.(\n    m_all[:, [:b_ovi, :b_zcp, :b_cpx, :z_ovi, :z_zcp, :z_cpx]],\n    digits=2,\n  )\nm_all3 = hcat(m_all.Name, m_all2)\n\n\n20√ó7 DataFrame\n\n\n\nRow\nx1\nb_ovi\nb_zcp\nb_cpx\nz_ovi\nz_zcp\nz_cpx\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n(Intercept)\n-0.02\n-0.02\n-0.03\n-0.88\n-1.24\n-1.35\n\n\n2\nTest: Star_r\n-0.0\n0.0\n0.01\n-0.09\n0.09\n0.18\n\n\n3\nTest: S20_r\n0.01\n0.01\n0.01\n0.32\n0.36\n0.35\n\n\n4\nTest: SLJ\n0.0\n0.0\n0.0\n0.11\n0.17\n0.08\n\n\n5\nTest: BMT\n-0.05\n-0.05\n-0.04\n-1.8\n-1.49\n-1.32\n\n\n6\nSex: Girls\n-0.44\n-0.44\n-0.44\n-21.35\n-13.64\n-13.61\n\n\n7\na1\n0.18\n0.18\n0.19\n4.97\n3.21\n3.31\n\n\n8\nTest: Star_r & Sex: Girls\n0.38\n0.38\n0.38\n6.28\n6.55\n6.53\n\n\n9\nTest: S20_r & Sex: Girls\n-0.2\n-0.2\n-0.2\n-3.38\n-3.54\n-3.44\n\n\n10\nTest: SLJ & Sex: Girls\n-0.07\n-0.07\n-0.07\n-1.11\n-1.18\n-1.19\n\n\n11\nTest: BMT & Sex: Girls\n-0.27\n-0.28\n-0.27\n-4.57\n-4.87\n-4.8\n\n\n12\nTest: Star_r & a1\n0.31\n0.31\n0.31\n3.03\n3.13\n3.15\n\n\n13\nTest: S20_r & a1\n-0.08\n-0.07\n-0.07\n-0.74\n-0.75\n-0.72\n\n\n14\nTest: SLJ & a1\n-0.07\n-0.08\n-0.06\n-0.73\n-0.81\n-0.66\n\n\n15\nTest: BMT & a1\n0.47\n0.47\n0.47\n4.61\n4.84\n4.82\n\n\n16\nSex: Girls & a1\n-0.1\n-0.15\n-0.14\n-1.47\n-1.84\n-1.79\n\n\n17\nTest: Star_r & Sex: Girls & a1\n-0.19\n-0.16\n-0.16\n-0.92\n-0.79\n-0.83\n\n\n18\nTest: S20_r & Sex: Girls & a1\n0.17\n0.18\n0.19\n0.84\n0.92\n0.95\n\n\n19\nTest: SLJ & Sex: Girls & a1\n0.01\n0.01\n0.0\n0.04\n0.03\n0.01\n\n\n20\nTest: BMT & Sex: Girls & a1\n0.19\n0.17\n0.16\n0.94\n0.85\n0.84\n\n\n\n\n\n\nThe three models usually do not differ in fixed-effect estimates. For main effects of age and Sex, z-values decrease strongly with the complexity of the model (i.e., standard errors are larger). For other coefficients, the changes are not very large and not consistent.\nIn general, dropping significant variance components and/or correlation parameters may lead to anti-conservative estimates of fixed effects (e.g., Schielzeth & Forstmeier, 2008). Basically, some of the variance allocated to age and Sex in LMM m_ovi could also be due to differences between schools. This ambiguity increased the uncertainty of the respective fixed effects in the other two LMMs."
  },
  {
    "objectID": "fggk21.html#fitting-an-overparameterized-lmm",
    "href": "fggk21.html#fitting-an-overparameterized-lmm",
    "title": "Basics with Emotikon Project",
    "section": "4.5 Fitting an overparameterized LMM",
    "text": "4.5 Fitting an overparameterized LMM\nThe complex LMM was not overparameterized with respect to School, because there are over 400 schools in the data. When the number of units (levels) of a grouping factor is small relative to the number of parameters we are trying to estimate, we often end up with an overparameterized / degenerate random-effect structure.\nAs an illustration, we fit a full CP matrix for the Cohort. As there are only nine cohorts in the data, we may be asking too much to estimate 5*6/2 = 15 VC/CP parameters.\n\nm_cpxCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 + Test | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0009\n0.0161\n-0.06\n0.9548\n0.0378\n\n\nTest: Star_r\n-0.0073\n0.0380\n-0.19\n0.8482\n0.0614\n\n\nTest: S20_r\n0.0056\n0.0383\n0.15\n0.8845\n0.0636\n\n\nTest: SLJ\n0.0101\n0.0451\n0.22\n0.8226\n0.0958\n\n\nTest: BMT\n-0.0556\n0.0335\n-1.66\n0.0968\n0.0330\n\n\na1\n0.2055\n0.0349\n5.90\n&lt;1e-08\n\n\n\nSex: Girls\n-0.4238\n0.0201\n-21.07\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.2849\n0.1101\n2.59\n0.0097\n\n\n\nTest: S20_r & a1\n-0.1172\n0.1096\n-1.07\n0.2851\n\n\n\nTest: SLJ & a1\n-0.0270\n0.1094\n-0.25\n0.8049\n\n\n\nTest: BMT & a1\n0.4555\n0.1084\n4.20\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3700\n0.0640\n5.78\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2116\n0.0638\n-3.32\n0.0009\n\n\n\nTest: SLJ & Sex: Girls\n-0.0552\n0.0634\n-0.87\n0.3844\n\n\n\nTest: BMT & Sex: Girls\n-0.2718\n0.0632\n-4.30\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1368\n0.0690\n-1.98\n0.0473\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2099\n0.2194\n-0.96\n0.3387\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1609\n0.2186\n0.74\n0.4615\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0225\n0.2172\n0.10\n0.9174\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1901\n0.2166\n0.88\n0.3801\n\n\n\nResidual\n0.9671\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpxCohort)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nCohort\n(Intercept)\n0.0014272\n0.0377788\n\n\n\n\n\n\n\nTest: Star_r\n0.0037684\n0.0613873\n-0.90\n\n\n\n\n\n\nTest: S20_r\n0.0040406\n0.0635659\n-1.00\n+0.87\n\n\n\n\n\nTest: SLJ\n0.0091867\n0.0958474\n+0.98\n-0.97\n-0.97\n\n\n\n\nTest: BMT\n0.0010891\n0.0330020\n-1.00\n+0.91\n+0.99\n-0.99\n\n\nResidual\n\n0.9353139\n0.9671163\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpxCohort)\n\ntrue\n\n\nThe model is overparameterized with several CPs estimated between |.98| and |1.00|. How about the zero-correlation parameter (zcp) version of this LMM?\n\nm_zcpCohort = let\n  f = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0022\n0.0152\n-0.15\n0.8837\n0.0341\n\n\nTest: Star_r\n-0.0042\n0.0339\n-0.12\n0.9023\n0.0331\n\n\nTest: S20_r\n0.0088\n0.0319\n0.27\n0.7837\n0.0000\n\n\nTest: SLJ\n0.0045\n0.0317\n0.14\n0.8876\n0.0000\n\n\nTest: BMT\n-0.0536\n0.0316\n-1.69\n0.0903\n0.0000\n\n\na1\n0.1999\n0.0351\n5.70\n&lt;1e-07\n\n\n\nSex: Girls\n-0.4245\n0.0201\n-21.08\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.3078\n0.1101\n2.80\n0.0052\n\n\n\nTest: S20_r & a1\n-0.0849\n0.1093\n-0.78\n0.4377\n\n\n\nTest: SLJ & a1\n-0.0748\n0.1086\n-0.69\n0.4911\n\n\n\nTest: BMT & a1\n0.4717\n0.1084\n4.35\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3729\n0.0640\n5.82\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2079\n0.0638\n-3.26\n0.0011\n\n\n\nTest: SLJ & Sex: Girls\n-0.0611\n0.0634\n-0.96\n0.3354\n\n\n\nTest: BMT & Sex: Girls\n-0.2697\n0.0632\n-4.27\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1372\n0.0691\n-1.99\n0.0470\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2041\n0.2195\n-0.93\n0.3525\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1733\n0.2187\n0.79\n0.4280\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0051\n0.2172\n0.02\n0.9811\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1962\n0.2168\n0.90\n0.3655\n\n\n\nResidual\n0.9680\n\n\n\n\n\n\n\n\n\n\nissingular(m_zcpCohort)\n\ntrue\n\n\nThis zcpLMM is also singular. Three of the five VCs are estimated as zero. This raises the possibility that LMM m_oviCohort might fit as well as LMM m_zcpCohort.\n\nm_oviCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0022\n0.0152\n-0.15\n0.8835\n0.0340\n\n\nTest: Star_r\n-0.0032\n0.0320\n-0.10\n0.9196\n\n\n\nTest: S20_r\n0.0087\n0.0319\n0.27\n0.7847\n\n\n\nTest: SLJ\n0.0045\n0.0317\n0.14\n0.8869\n\n\n\nTest: BMT\n-0.0536\n0.0316\n-1.69\n0.0903\n\n\n\na1\n0.1999\n0.0351\n5.69\n&lt;1e-07\n\n\n\nSex: Girls\n-0.4245\n0.0201\n-21.08\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.3135\n0.1097\n2.86\n0.0043\n\n\n\nTest: S20_r & a1\n-0.0848\n0.1093\n-0.78\n0.4378\n\n\n\nTest: SLJ & a1\n-0.0748\n0.1086\n-0.69\n0.4910\n\n\n\nTest: BMT & a1\n0.4718\n0.1084\n4.35\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3736\n0.0640\n5.84\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2079\n0.0638\n-3.26\n0.0011\n\n\n\nTest: SLJ & Sex: Girls\n-0.0611\n0.0634\n-0.96\n0.3356\n\n\n\nTest: BMT & Sex: Girls\n-0.2697\n0.0632\n-4.27\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1371\n0.0691\n-1.99\n0.0471\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2022\n0.2195\n-0.92\n0.3568\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1733\n0.2187\n0.79\n0.4281\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0051\n0.2172\n0.02\n0.9811\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1961\n0.2168\n0.90\n0.3657\n\n\n\nResidual\n0.9681\n\n\n\n\n\n\n\n\n\n\nissingular(m_oviCohort)\n\nfalse\n\n\nThis solves the problem with singularity, but does LMM m_zcpCohort fit noise relative to the LMM m_oviCohort?\n\nMixedModels.likelihoodratiotest(m_oviCohort, m_zcpCohort)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 | Cohort)\n22\n26803\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Cohort)\n26\n26803\n0\n4\n0.9968\n\n\n\n\n\n\ngof_summary2 = let\n  mods = [m_oviCohort, m_zcpCohort, m_cpxCohort]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n3√ó5 DataFrame\n\n\n\nRow\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n22\n26802.9\n26846.9\n26847.0\n27004.7\n\n\n2\n26\n26802.7\n26854.7\n26854.8\n27041.3\n\n\n3\n36\n26790.5\n26862.5\n26862.7\n27120.8\n\n\n\n\n\n\nIndeed, adding VCs is fitting noise. Again, the goodness of fit statistics unanimously favor the selection of the LMM m_oviCohort.\nNot shown here, but the Cohort-related VCs for the Test contrasts could be estimated reliably for the full data. Thus, the small number of cohorts does not necessarily prevent the determination of reliable differences between tests across cohorts. What if we include VCs and CPs related to random factors Child and School?"
  },
  {
    "objectID": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "href": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "title": "Basics with Emotikon Project",
    "section": "4.6 Fitting the published LMM m1 to the reduced data",
    "text": "4.6 Fitting the published LMM m1 to the reduced data\n\n\n\n\n\n\nWarning\n\n\n\nThe following LMMs m1, m2, etc. take a bit longer (e.g., close to 6 minutes in the Pluto notebook, close to 3 minutes in the REPL on a MacBook Pro).\n\n\nLMM m1 reported in F√ºhner et al. (2021) included random factors for School, Child, and Cohort. The RES for School was specified like in LMM m_cpx. The RES for Child included VCs and CPs for Test, but not for linear developmental gain in the ninth year of life a1 or Sex; they are between-Child effects.\nThe RES for Cohort included only VCs, no CPs for Test. The parsimony was due to the small number of nine levels for this grouping factor.\nHere we fit this LMM m1 for the reduced data. For a different subset of similar size on MacBook Pro [13 | 15 | 16] this took [303 | 250 | 244 ] s; for LMM m1a (i.e., dropping 1 school-relate VC for Sex), times are [212 | 165 | 160] s. The corresponding lme4 times for LMM m1 are [397 | 348 | 195].\nFinally, times for fitting the full set of data ‚Äìnot in this script‚Äì, for LMM m1are [60 | 62 | 85] minutes (!); for LMM m1a the times were [46 | 48 | 34] minutes. It was not possible to fit the full set of data with lme4; after about 13 to 18 minutes the program stopped with: Error in eval_f(x, ...) : Downdated VtV is not positive definite.\n\nm1 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (1 + Test + a1 + Sex | School) +\n      (1 + Test | Child) +\n      zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1474      Time: 0:00:22 (15.20 ms/it)\n  objective:  24651.01449586962\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0133\n0.0192\n-0.69\n0.4874\n0.5899\n0.2138\n0.0131\n\n\nTest: Star_r\n0.0089\n0.0375\n0.24\n0.8132\n0.7640\n0.3330\n0.0637\n\n\nTest: S20_r\n0.0092\n0.0292\n0.31\n0.7535\n0.6582\n0.3280\n0.0020\n\n\nTest: SLJ\n0.0034\n0.0299\n0.11\n0.9107\n0.5700\n0.3210\n0.0336\n\n\nTest: BMT\n-0.0383\n0.0296\n-1.29\n0.1954\n0.7425\n0.3128\n0.0000\n\n\na1\n0.1956\n0.0545\n3.59\n0.0003\n\n0.2826\n\n\n\nSex: Girls\n-0.4287\n0.0312\n-13.75\n&lt;1e-42\n\n0.1438\n\n\n\nTest: Star_r & a1\n0.2897\n0.0887\n3.26\n0.0011\n\n\n\n\n\nTest: S20_r & a1\n-0.0781\n0.0817\n-0.96\n0.3394\n\n\n\n\n\nTest: SLJ & a1\n-0.0633\n0.0769\n-0.82\n0.4106\n\n\n\n\n\nTest: BMT & a1\n0.4713\n0.0852\n5.53\n&lt;1e-07\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3756\n0.0511\n7.35\n&lt;1e-12\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.1867\n0.0475\n-3.93\n&lt;1e-04\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0695\n0.0445\n-1.56\n0.1183\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2812\n0.0495\n-5.68\n&lt;1e-07\n\n\n\n\n\na1 & Sex: Girls\n-0.1267\n0.1048\n-1.21\n0.2269\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.1382\n0.1756\n-0.79\n0.4311\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1754\n0.1633\n1.07\n0.2828\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0155\n0.1530\n-0.10\n0.9193\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1548\n0.1702\n0.91\n0.3630\n\n\n\n\n\nResidual\n0.5184\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.34801743\n0.58993002\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.58363674\n0.76396121\n+0.14\n\n\n\n\n\n\n\n\nTest: S20_r\n0.43325315\n0.65821968\n+0.00\n-0.52\n\n\n\n\n\n\n\nTest: SLJ\n0.32492588\n0.57002270\n+0.05\n-0.04\n-0.37\n\n\n\n\n\n\nTest: BMT\n0.55135686\n0.74253408\n-0.33\n+0.13\n-0.17\n-0.24\n\n\n\n\nSchool\n(Intercept)\n0.04569159\n0.21375591\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.11087441\n0.33297810\n-0.06\n\n\n\n\n\n\n\n\nTest: S20_r\n0.10760948\n0.32803884\n-0.08\n-0.39\n\n\n\n\n\n\n\nTest: SLJ\n0.10302512\n0.32097527\n-0.19\n+0.21\n-0.80\n\n\n\n\n\n\nTest: BMT\n0.09785675\n0.31282064\n-0.33\n-0.02\n+0.13\n-0.38\n\n\n\n\n\na1\n0.07988894\n0.28264632\n+0.62\n-0.20\n+0.11\n-0.61\n+0.45\n\n\n\n\nSex: Girls\n0.02068432\n0.14382046\n-0.45\n+0.45\n+0.31\n-0.27\n-0.15\n-0.37\n\n\nCohort\n(Intercept)\n0.00017292\n0.01314996\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.00405923\n0.06371208\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.00000413\n0.00203252\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.00113003\n0.03361595\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.00000000\n0.00000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.26873924\n0.51840065\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nDepending on the random number for stratified samplign, LMM m1 may or may not be supported by the data.\nWe also fit an alternative parameterization, estimating VCs and CPs for Test scores rather than Test effects by replacing the 1 + ... in the RE terms with 0 + ....\n\nm2 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (0 + Test + a1 + Sex | School) +\n      (0 + Test | Child) +\n      zerocorr(0 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1592      Time: 0:00:27 (17.42 ms/it)\n  objective:  24646.87204583089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0136\n0.0195\n-0.70\n0.4846\n\n\n\n\n\nTest: Star_r\n0.0093\n0.0365\n0.25\n0.7988\n0.7983\n0.3103\n0.0208\n\n\nTest: S20_r\n0.0060\n0.0354\n0.17\n0.8653\n0.7689\n0.3304\n0.0563\n\n\nTest: SLJ\n0.0052\n0.0339\n0.15\n0.8786\n0.7818\n0.2476\n0.0157\n\n\nTest: BMT\n-0.0388\n0.0300\n-1.29\n0.1966\n0.6983\n0.1967\n0.0000\n\n\na1\n0.1934\n0.0545\n3.55\n0.0004\n\n0.2832\n\n\n\nSex: Girls\n-0.4290\n0.0312\n-13.75\n&lt;1e-42\n\n0.1434\n\n\n\nTest: Star_r & a1\n0.2932\n0.0887\n3.31\n0.0009\n\n\n\n\n\nTest: S20_r & a1\n-0.1007\n0.0826\n-1.22\n0.2227\n\n\n\n\n\nTest: SLJ & a1\n-0.0456\n0.0773\n-0.59\n0.5550\n\n\n\n\n\nTest: BMT & a1\n0.4685\n0.0853\n5.50\n&lt;1e-07\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3759\n0.0511\n7.36\n&lt;1e-12\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.1892\n0.0475\n-3.98\n&lt;1e-04\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0680\n0.0445\n-1.53\n0.1262\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2815\n0.0495\n-5.68\n&lt;1e-07\n\n\n\n\n\na1 & Sex: Girls\n-0.1262\n0.1049\n-1.20\n0.2289\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.1371\n0.1756\n-0.78\n0.4348\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1720\n0.1634\n1.05\n0.2926\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0125\n0.1529\n-0.08\n0.9348\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1542\n0.1702\n0.91\n0.3648\n\n\n\n\n\nTest: Run\n\n\n\n\n0.7513\n0.3701\n0.0548\n\n\nResidual\n0.5101\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m2)\n\ntrue\n\n\nDepending on the random number generator seed, the model may or may not be supported in the alternative parameterization of scores. The fixed-effects profile is not affected (see 2.8 below).\n\n\n\n\n\n\nCaution\n\n\n\nRK: The order of RE terms is critical. In formula f2 the zerocorr() term must be placed last as shown. If it is placed first, School-related and Child-related CPs are estimated/reported (?) as zero. This was not the case for formula m1. Thus, it appears to be related to the 0-intercepts in School and Child terms. Need a reprex.\n\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5644446\n0.7512953\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6372678\n0.7982906\n+0.50\n\n\n\n\n\n\n\n\nTest: S20_r\n0.5912539\n0.7689304\n+0.56\n+0.64\n\n\n\n\n\n\n\nTest: SLJ\n0.6111377\n0.7817530\n+0.55\n+0.60\n+0.72\n\n\n\n\n\n\nTest: BMT\n0.4876120\n0.6982922\n+0.19\n+0.40\n+0.37\n+0.49\n\n\n\n\nSchool\nTest: Run\n0.1369534\n0.3700721\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0962706\n0.3102750\n+0.53\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1091782\n0.3304212\n+0.47\n+0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.0613205\n0.2476297\n+0.47\n+0.76\n+0.41\n\n\n\n\n\n\nTest: BMT\n0.0387012\n0.1967263\n+0.15\n+0.38\n+0.18\n+0.02\n\n\n\n\n\na1\n0.0801896\n0.2831776\n+0.58\n+0.47\n+0.56\n-0.05\n+0.65\n\n\n\n\nSex: Girls\n0.0205758\n0.1434427\n-0.63\n-0.26\n+0.06\n-0.28\n-0.58\n-0.37\n\n\nCohort\nTest: Run\n0.0030051\n0.0548189\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0004342\n0.0208386\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0031741\n0.0563393\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0002468\n0.0157111\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2601583\n0.5100571"
  },
  {
    "objectID": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "href": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "title": "Basics with Emotikon Project",
    "section": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)",
    "text": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)\nThe √¨ssingular() command is sort of a shortcut for a quick inspection of the principle components (PCs) of the variance-covariance matrix of the RES. With the MixedModels.PCA() command, we also obtain information about the amount of cumulative variance accounted for as we add PCs.\nThe output also provides PC loadings which may facilitate interpretation of the CP matrices (if estimated). This topic will be picked uo in a separate vignette. See also F√ºhner et al. (2021) for an application."
  },
  {
    "objectID": "fggk21.html#effects-in-res",
    "href": "fggk21.html#effects-in-res",
    "title": "Basics with Emotikon Project",
    "section": "4.8 Effects in RES",
    "text": "4.8 Effects in RES\nFor every random factor, MixedModels.PCA() extracts as many PCs as there are VCs. Therefore, the cumulation of variance across PCs within a random factor will always add up to 100% ‚Äì at the latest with the last VC, but, in the case of overparameterized LMMs, the ceiling will be reached earlier. The final PCs are usually quite small.\nPCs are extracted in the order of the amount of unique variance they account for. The first PC accounts for the largest and the final PC for the least amount of variance. The number the PCs with percent variance above a certain threshold indicates the number of weighted composites needed and reflects the dimensionality of the orthogonal space within which (almost) all the variance can be accounted for. The weights for forming composite scores are the listed loadings. For ease of interpretation it is often useful to change the sign of some composite scores.\nThe PCA for LMM m1 shows that each of the five PCs for Child accounts for a non-zero percent of unique variance.\nFor School fewer than seven PCs have unique variance. The exact number depends on sampling. The overparameterization of School might be resolved when the CPs for Sex are dropped from the LMM.\nCohort was estimated with CPs forced to zero. Therefore, the VCs were forced to be orthogonal; they already represent the PCA solution. However, depending on sampling, not all PCs may be identified for this random factor either.\nImportantly, again depending on sampling, a non-singular fit does not imply that unique variance is associated with all PCs (i.e., not for last PC for School). Embrace uncertainty!\n\nMixedModels.PCA(m1)\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .\n Test: Star_r   0.14   1.0     .      .      .\n Test: S20_r    0.0   -0.52   1.0     .      .\n Test: SLJ      0.05  -0.04  -0.37   1.0     .\n Test: BMT     -0.33   0.13  -0.17  -0.24   1.0\n\nNormalized cumulative variances:\n[0.3284, 0.6158, 0.8283, 0.9385, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n (Intercept)   -0.07   0.55   0.58  -0.59  -0.02\n Test: Star_r  -0.6   -0.09   0.45   0.41   0.5\n Test: S20_r    0.71   0.02   0.15   0.05   0.69\n Test: SLJ     -0.32   0.44  -0.66  -0.21   0.47\n Test: BMT     -0.15  -0.7   -0.01  -0.65   0.24, School = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .      .      .\n Test: Star_r  -0.06   1.0     .      .      .      .      .\n Test: S20_r   -0.08  -0.39   1.0     .      .      .      .\n Test: SLJ     -0.19   0.21  -0.8    1.0     .      .      .\n Test: BMT     -0.33  -0.02   0.13  -0.38   1.0     .      .\n a1             0.62  -0.2    0.11  -0.61   0.45   1.0     .\n Sex: Girls    -0.45   0.45   0.31  -0.27  -0.15  -0.37   1.0\n\nNormalized cumulative variances:\n[0.3564, 0.6346, 0.8055, 0.9685, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n (Intercept)   -0.25  -0.49  -0.4   -0.38   0.18   0.54   0.25\n Test: Star_r   0.31   0.12   0.2   -0.75   0.47  -0.25  -0.08\n Test: S20_r   -0.4    0.44  -0.32   0.19   0.52  -0.26   0.41\n Test: SLJ      0.55  -0.31   0.1    0.2    0.03  -0.16   0.73\n Test: BMT     -0.29   0.14   0.79   0.03   0.16   0.43   0.25\n a1            -0.52  -0.27   0.17  -0.31  -0.41  -0.55   0.24\n Sex: Girls     0.14   0.6   -0.18  -0.35  -0.53   0.25   0.35, Cohort = \nPrincipal components based on correlation matrix\n (Intercept)   1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  0.0\n\nNormalized cumulative variances:\n[0.25, 0.5, 0.75, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3   PC4     PC5\n (Intercept)   1.0   0.0   0.0   0.0     0.0\n Test: Star_r  0.0   1.0   0.0   0.0     0.0\n Test: S20_r   0.0   0.0   0.0   1.0     0.0\n Test: SLJ     0.0   0.0   1.0   0.0     0.0\n Test: BMT     0.0   0.0   0.0   0.0   NaN)\n\n\n\n4.8.1 Scores in RES\nNow lets looks at the PCA results for the alternative parameterization of LMM m2. It is important to note that the reparameterization to base estimates of VCs and CPs on scores rather than effects applies only to the Test factor (i.e., the first factor in the formula); VCs for Sex and age refer to the associated effects.\nDepending on sampling, the difference between LMM m1 and LMM m2 may show that overparameterization according to PCs may depend on the specification chosen for the other the random-effect structure.\n\n\n\n\n\n\nNote\n\n\n\nFor the complete data, all PCs had unique variance associated with them.\n\n\n\nMixedModels.PCA(m2)\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run     1.0    .     .     .     .\n Test: Star_r  0.5   1.0    .     .     .\n Test: S20_r   0.56  0.64  1.0    .     .\n Test: SLJ     0.55  0.6   0.72  1.0    .\n Test: BMT     0.19  0.4   0.37  0.49  1.0\n\nNormalized cumulative variances:\n[0.6101, 0.7756, 0.8667, 0.948, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.42   0.53   0.63   0.37   0.08\n Test: Star_r  -0.47   0.03  -0.65   0.58  -0.16\n Test: S20_r   -0.49   0.15  -0.24  -0.49   0.66\n Test: SLJ     -0.5   -0.05   0.09  -0.5   -0.7\n Test: BMT     -0.34  -0.83   0.33   0.2    0.2, School = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .      .      .\n Test: Star_r   0.53   1.0     .      .      .      .      .\n Test: S20_r    0.47   0.48   1.0     .      .      .      .\n Test: SLJ      0.47   0.76   0.41   1.0     .      .      .\n Test: BMT      0.15   0.38   0.18   0.02   1.0     .      .\n a1             0.58   0.47   0.56  -0.05   0.65   1.0     .\n Sex: Girls    -0.63  -0.26   0.06  -0.28  -0.58  -0.37   1.0\n\nNormalized cumulative variances:\n[0.4822, 0.694, 0.8509, 0.9554, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n Test: Run     -0.44   0.08   0.15  -0.64  -0.16   0.56   0.17\n Test: Star_r  -0.44   0.28   0.02   0.41  -0.56   0.06  -0.49\n Test: S20_r   -0.34   0.27  -0.58  -0.11   0.6   -0.0   -0.32\n Test: SLJ     -0.32   0.56   0.33   0.25   0.22  -0.24   0.55\n Test: BMT     -0.32  -0.53  -0.02   0.55   0.25   0.46   0.21\n a1            -0.41  -0.35  -0.42  -0.14  -0.33  -0.53   0.35\n Sex: Girls     0.34   0.35  -0.59   0.18  -0.29   0.37   0.4, Cohort = \nPrincipal components based on correlation matrix\n Test: Run     1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  0.0\n\nNormalized cumulative variances:\n[0.25, 0.5, 0.75, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3   PC4     PC5\n Test: Run     1.0   0.0   0.0   0.0     0.0\n Test: Star_r  0.0   1.0   0.0   0.0     0.0\n Test: S20_r   0.0   0.0   1.0   0.0     0.0\n Test: SLJ     0.0   0.0   0.0   1.0     0.0\n Test: BMT     0.0   0.0   0.0   0.0   NaN)"
  },
  {
    "objectID": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "href": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "title": "Basics with Emotikon Project",
    "section": "4.9 Summary of results for stratified subset of data",
    "text": "4.9 Summary of results for stratified subset of data\nReturning to the theoretical focus of the article, the significant main effects of age and Sex, the interactions between age and c1 and c4 contrasts and the interactions between Sex and three test contrasts (c1, c2, c4) are replicated. Obviously, the subset of data is much noisier than the full set."
  },
  {
    "objectID": "fggk21.html#overall-summary-statistics",
    "href": "fggk21.html#overall-summary-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.1 Overall summary statistics",
    "text": "6.1 Overall summary statistics\n+ julia&gt; m1.optsum         # MixedModels.OptSummary:  gets all info\n+ julia&gt; loglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood\n                             of the model\n+ julia&gt; deviance(m1)      # StatsBase.deviance: negative twice the log-likelihood\n                             relative to saturated model\n+ julia&gt; objective(m1)     # MixedModels.objective: saturated model not clear:\n                             negative twice the log-likelihood\n+ julia&gt; nobs(m1)          # n of observations; they are not independent\n+ julia&gt; dof(m1)           # n of degrees of freedom is number of model parameters\n+ julia&gt; aic(m1)           # objective(m1) + 2*dof(m1)\n+ julia&gt; bic(m1)           # objective(m1) + dof(m1)*log(nobs(m1))\n\nm1.optsum            # MixedModels.OptSummary:  gets all info\n\n\n\n\n\n\n\n\nInitialization\n\n\n\nInitial parameter vector\n[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nInitial objective value\n25840.65375540981\n\n\nOptimizer settings\n\n\n\nOptimizer (from NLopt)\nLN_BOBYQA\n\n\nLower bounds\n[0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nftol_rel\n1.0e-12\n\n\nftol_abs\n1.0e-8\n\n\nxtol_rel\n0.0\n\n\nxtol_abs\n[1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10]\n\n\ninitial_step\n[0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]\n\n\nmaxfeval\n-1\n\n\nmaxtime\n-1.0\n\n\nResult\n\n\n\nFunction evaluations\n1473\n\n\nFinal parameter vector\n[1.138, 0.2048, 0.0008, 0.0502, -0.4783, 1.4594, -0.6667, -0.0489, 0.2519, 1.0806, -0.5036, -0.1341, 0.975, -0.4254, 1.2492, 0.4123, -0.0382, -0.0482, -0.1148, -0.2013, 0.3377, -0.1245, 0.6412, -0.2524, 0.1239, -0.0249, -0.0896, 0.1169, 0.5783, -0.5004, 0.0587, 0.0528, 0.1349, 0.3231, -0.4144, -0.3978, -0.026, 0.3845, 0.1184, -0.1701, 0.0125, -0.0043, 0.0014, 0.0254, 0.1229, 0.0039, 0.0648, 0.0]\n\n\nFinal objective value\n24651.0145\n\n\nReturn code\nFTOL_REACHED\n\n\n\n\n\n\nloglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood of the model\n\n-12325.50724793481\n\n\n\ndeviance(m1)      # StatsBase.deviance: negative twice the log-likelihood relative to saturated mode`\n\n24651.01449586962\n\n\n\nobjective(m1)    # MixedModels.objective: saturated model not clear: negative twice the log-likelihood\n\n24651.01449586962\n\n\n\nnobs(m1) # n of observations; they are not independent\n\n9663\n\n\n\nn_, p_, q_, k_ = size(m1)\n\n(9663, 20, 13076, 3)\n\n\n\ndof(m1)  # n of degrees of freedom is number of model parameters\n\n69\n\n\n\ngeom_df = sum(leverage(m1)) # trace of hat / rank of model matrix / geom dof\n\n5672.4491657979415\n\n\n\nresid_df = nobs(m1) - geom_df  # eff. residual degrees of freedom\n\n3990.5508342020585\n\n\n\naic(m1)  # objective(m1) + 2*dof(m1)\n\n24789.01449586962\n\n\n\nbic(m1)  # objective(m1) + dof(m1)*log(nobs(m1))\n\n25284.16259709157"
  },
  {
    "objectID": "fggk21.html#fixed-effect-statistics",
    "href": "fggk21.html#fixed-effect-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.2 Fixed-effect statistics",
    "text": "6.2 Fixed-effect statistics\n+ julia&gt; coeftable(m1)     # StatsBase.coeftable: fixed-effects statiscs;\n                             default level=0.95\n+ julia&gt; Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n+ julia&gt; coef(m1)          # StatsBase.coef - parts of the table\n+ julia&gt; fixef(m1)         # MixedModels.fixef: not the same as coef()\n                             for rank-deficient case\n+ julia&gt; m1.beta           # alternative extractor\n+ julia&gt; fixefnames(m1)    # works also for coefnames(m1)\n+ julia&gt; vcov(m1)          # StatsBase.vcov: var-cov matrix of fixed-effects coef.\n+ julia&gt; stderror(m1)      # StatsBase.stderror: SE for fixed-effects coefficients\n+ julia&gt; propertynames(m1) # names of available extractors\n\ncoeftable(m1) # StatsBase.coeftable: fixed-effects statiscs; default level=0.95\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\n\n\n\n\n(Intercept)\n-0.0133264\n0.0191908\n-0.69\n0.4874\n\n\nTest: Star_r\n0.00885446\n0.0374649\n0.24\n0.8132\n\n\nTest: S20_r\n0.00915936\n0.0291725\n0.31\n0.7535\n\n\nTest: SLJ\n0.00335426\n0.0299082\n0.11\n0.9107\n\n\nTest: BMT\n-0.0382894\n0.0295702\n-1.29\n0.1954\n\n\na1\n0.19562\n0.0544531\n3.59\n0.0003\n\n\nSex: Girls\n-0.428736\n0.0311862\n-13.75\n&lt;1e-42\n\n\nTest: Star_r & a1\n0.289674\n0.0887445\n3.26\n0.0011\n\n\nTest: S20_r & a1\n-0.0780511\n0.0816905\n-0.96\n0.3394\n\n\nTest: SLJ & a1\n-0.063263\n0.0768824\n-0.82\n0.4106\n\n\nTest: BMT & a1\n0.471289\n0.0851509\n5.53\n&lt;1e-07\n\n\nTest: Star_r & Sex: Girls\n0.375595\n0.0510834\n7.35\n&lt;1e-12\n\n\nTest: S20_r & Sex: Girls\n-0.186708\n0.0475115\n-3.93\n&lt;1e-04\n\n\nTest: SLJ & Sex: Girls\n-0.0695301\n0.0445173\n-1.56\n0.1183\n\n\nTest: BMT & Sex: Girls\n-0.281236\n0.0495213\n-5.68\n&lt;1e-07\n\n\na1 & Sex: Girls\n-0.126678\n0.104825\n-1.21\n0.2269\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.138238\n0.175581\n-0.79\n0.4311\n\n\nTest: S20_r & a1 & Sex: Girls\n0.175422\n0.163343\n1.07\n0.2828\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0154911\n0.152984\n-0.10\n0.9193\n\n\nTest: BMT & a1 & Sex: Girls\n0.154795\n0.170159\n0.91\n0.3630\n\n\n\n\n\n\n#Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n\n\ncoef(m1)              # StatsBase.coef; parts of the table\n\n20-element Vector{Float64}:\n -0.013326418174543895\n  0.008854455767991822\n  0.009159355804501768\n  0.0033542600278063522\n -0.0382894498810129\n  0.19562037117168546\n -0.4287359064726347\n  0.2896737283700021\n -0.07805107265519066\n -0.06326301586054563\n  0.47128946887343964\n  0.37559463121941\n -0.18670827473612425\n -0.06953014822894996\n -0.2812361916524132\n -0.1266782570629409\n -0.13823832011459883\n  0.1754219065821277\n -0.015491064668230801\n  0.1547945432078642\n\n\n\nfixef(m1)    # MixedModels.fixef: not the same as coef() for rank-deficient case\n\n20-element Vector{Float64}:\n -0.013326418174543895\n  0.008854455767991822\n  0.009159355804501768\n  0.0033542600278063522\n -0.0382894498810129\n  0.19562037117168546\n -0.4287359064726347\n  0.2896737283700021\n -0.07805107265519066\n -0.06326301586054563\n  0.47128946887343964\n  0.37559463121941\n -0.18670827473612425\n -0.06953014822894996\n -0.2812361916524132\n -0.1266782570629409\n -0.13823832011459883\n  0.1754219065821277\n -0.015491064668230801\n  0.1547945432078642\n\n\n\nm1.Œ≤                  # alternative extractor\n\n20-element Vector{Float64}:\n -0.013326418174543895\n  0.008854455767991822\n  0.009159355804501768\n  0.0033542600278063522\n -0.0382894498810129\n  0.19562037117168546\n -0.4287359064726347\n  0.2896737283700021\n -0.07805107265519066\n -0.06326301586054563\n  0.47128946887343964\n  0.37559463121941\n -0.18670827473612425\n -0.06953014822894996\n -0.2812361916524132\n -0.1266782570629409\n -0.13823832011459883\n  0.1754219065821277\n -0.015491064668230801\n  0.1547945432078642\n\n\n\nfixefnames(m1)        # works also for coefnames(m1)\n\n20-element Vector{String}:\n \"(Intercept)\"\n \"Test: Star_r\"\n \"Test: S20_r\"\n \"Test: SLJ\"\n \"Test: BMT\"\n \"a1\"\n \"Sex: Girls\"\n \"Test: Star_r & a1\"\n \"Test: S20_r & a1\"\n \"Test: SLJ & a1\"\n \"Test: BMT & a1\"\n \"Test: Star_r & Sex: Girls\"\n \"Test: S20_r & Sex: Girls\"\n \"Test: SLJ & Sex: Girls\"\n \"Test: BMT & Sex: Girls\"\n \"a1 & Sex: Girls\"\n \"Test: Star_r & a1 & Sex: Girls\"\n \"Test: S20_r & a1 & Sex: Girls\"\n \"Test: SLJ & a1 & Sex: Girls\"\n \"Test: BMT & a1 & Sex: Girls\"\n\n\n\nvcov(m1)   # StatsBase.vcov: var-cov matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  0.000368288   2.44834e-5   -1.71765e-5   ‚Ä¶   3.89507e-6    2.86399e-6\n  2.44834e-5    0.00140362   -0.000421413     -9.25565e-7   -1.05427e-7\n -1.71765e-5   -0.000421413   0.000851032      2.05425e-5   -6.89378e-7\n -2.68157e-5    5.43469e-5   -0.000468208     -2.4403e-5     1.36724e-6\n -0.000143255   3.25955e-5   -8.0956e-6        1.40951e-6   -1.19346e-5\n -4.45442e-5   -7.39539e-5    2.71186e-5   ‚Ä¶  -6.03726e-5   -5.07707e-5\n -4.4393e-5     6.34994e-5    4.35402e-5      -7.3791e-5     0.000282454\n -2.72933e-5   -0.000442556   0.000212958      8.7071e-6     2.62752e-6\n  3.39758e-6    0.000213337  -0.000395714     -0.00010435    8.17628e-6\n -2.00038e-5    6.88797e-8    0.000175398      0.000158855  -6.10943e-5\n  7.30797e-5   -2.67185e-5    2.85748e-5   ‚Ä¶  -5.78095e-5    8.26953e-5\n  6.91142e-6    8.70957e-7   -2.05397e-6       8.63739e-6   -0.000108162\n  5.11841e-6   -1.95496e-6    7.66359e-6       0.000673899   0.000117963\n -5.24346e-6   -1.56019e-6   -4.65511e-6      -0.00136393    0.000576546\n -1.12612e-6    6.33588e-7   -9.0277e-7        0.000577209  -0.00171459\n -4.55233e-6   -1.20844e-5    3.61454e-7   ‚Ä¶   0.000151279  -0.0039188\n -1.28132e-5   -2.83547e-5    2.85268e-5      -0.000138513   0.00179136\n  4.97663e-7    2.85838e-5   -4.69381e-5      -0.0115833    -0.00195403\n  3.89507e-6   -9.25565e-7    2.05425e-5       0.0234041    -0.00998137\n  2.86399e-6   -1.05427e-7   -6.89378e-7      -0.00998137    0.028954\n\n\n\nvcov(m1; corr=true) # StatsBase.vcov: correlation matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  1.0           0.0340528    -0.0306808    ‚Ä¶   0.00132671    0.000877047\n  0.0340528     1.0          -0.385576        -0.000161486  -1.65376e-5\n -0.0306808    -0.385576      1.0              0.00460293   -0.000138877\n -0.0467202     0.0485019    -0.53663         -0.00533341    0.000268658\n -0.252442      0.0294224    -0.00938471       0.000311577  -0.00237191\n -0.042626     -0.0362505     0.0170715    ‚Ä¶  -0.00724721   -0.00547944\n -0.0741752     0.0543479     0.0478581       -0.0154666     0.0532268\n -0.0160258    -0.133107      0.0822583        0.000641336   0.000174001\n  0.00216723    0.069706     -0.166049        -0.00834977    0.000588207\n -0.0135579     2.39133e-5    0.0782035        0.013506     -0.00467003\n  0.0447212    -0.00837526    0.0115033    ‚Ä¶  -0.00443776    0.00570739\n  0.00705007    0.000455085  -0.0013783        0.00110524   -0.0124435\n  0.00561361   -0.00109828    0.00552918       0.0927148     0.0145913\n -0.00613755   -0.000935458  -0.00358449      -0.20027       0.0761115\n -0.00118495    0.0003415    -0.000624902      0.0761894    -0.203476\n -0.00226294   -0.00307704    0.000118199  ‚Ä¶   0.00943335   -0.219701\n -0.00380265   -0.00431046    0.00556933      -0.00515665    0.0599587\n  0.00015876    0.00467083   -0.00985034      -0.463536     -0.0703032\n  0.00132671   -0.000161486   0.00460293       1.0          -0.383433\n  0.000877047  -1.65376e-5   -0.000138877     -0.383433      1.0\n\n\n\nstderror(m1)       # StatsBase.stderror: SE for fixed-effects coefficients\n\n20-element Vector{Float64}:\n 0.019190838609013373\n 0.037464904374354346\n 0.029172457183929874\n 0.029908246837066204\n 0.029570247247537502\n 0.05445314275453583\n 0.03118620045393219\n 0.08874454325857666\n 0.08169050540658983\n 0.07688237996024007\n 0.08515085655609324\n 0.05108338328710162\n 0.047511503910903\n 0.0445173183865237\n 0.049521334150376524\n 0.10482540178954916\n 0.17558064233009524\n 0.16334323024815908\n 0.15298414940873858\n 0.17015877736759646\n\n\n\npropertynames(m1)  # names of available extractors\n\n(:formula, :reterms, :Xymat, :feterm, :sqrtwts, :parmap, :dims, :A, :L, :optsum, :Œ∏, :theta, :Œ≤, :beta, :Œ≤s, :betas, :Œª, :lambda, :stderror, :œÉ, :sigma, :œÉs, :sigmas, :œÉœÅs, :sigmarhos, :b, :u, :lowerbd, :X, :y, :corr, :vcov, :PCA, :rePCA, :objective, :pvalues)"
  },
  {
    "objectID": "fggk21.html#covariance-parameter-estimates",
    "href": "fggk21.html#covariance-parameter-estimates",
    "title": "Basics with Emotikon Project",
    "section": "6.3 Covariance parameter estimates",
    "text": "6.3 Covariance parameter estimates\nThese commands inform us about the model parameters associated with the RES.\n+ julia&gt; issingular(m1)        # Test singularity for param. vector m1.theta\n+ julia&gt; VarCorr(m1)           # MixedModels.VarCorr: est. of RES\n+ julia&gt; propertynames(m1)\n+ julia&gt; m1.œÉ                  # residual; or: m1.sigma\n+ julia&gt; m1.œÉs                 # VCs; m1.sigmas\n+ julia&gt; m1.Œ∏                  # Parameter vector for RES (w/o residual); m1.theta\n+ julia&gt; MixedModels.sdest(m1) #  prsqrt(MixedModels.varest(m1))\n+ julia&gt; BlockDescription(m1)  #  Description of blocks of A and L in an LMM\n\nissingular(m1) # Test if model is singular for paramter vector m1.theta (default)\n\ntrue\n\n\n\nissingular(m2)\n\ntrue\n\n\n\nVarCorr(m1) # MixedModels.VarCorr: estimates of random-effect structure (RES)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.34801743\n0.58993002\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.58363674\n0.76396121\n+0.14\n\n\n\n\n\n\n\n\nTest: S20_r\n0.43325315\n0.65821968\n+0.00\n-0.52\n\n\n\n\n\n\n\nTest: SLJ\n0.32492588\n0.57002270\n+0.05\n-0.04\n-0.37\n\n\n\n\n\n\nTest: BMT\n0.55135686\n0.74253408\n-0.33\n+0.13\n-0.17\n-0.24\n\n\n\n\nSchool\n(Intercept)\n0.04569159\n0.21375591\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.11087441\n0.33297810\n-0.06\n\n\n\n\n\n\n\n\nTest: S20_r\n0.10760948\n0.32803884\n-0.08\n-0.39\n\n\n\n\n\n\n\nTest: SLJ\n0.10302512\n0.32097527\n-0.19\n+0.21\n-0.80\n\n\n\n\n\n\nTest: BMT\n0.09785675\n0.31282064\n-0.33\n-0.02\n+0.13\n-0.38\n\n\n\n\n\na1\n0.07988894\n0.28264632\n+0.62\n-0.20\n+0.11\n-0.61\n+0.45\n\n\n\n\nSex: Girls\n0.02068432\n0.14382046\n-0.45\n+0.45\n+0.31\n-0.27\n-0.15\n-0.37\n\n\nCohort\n(Intercept)\n0.00017292\n0.01314996\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.00405923\n0.06371208\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.00000413\n0.00203252\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.00113003\n0.03361595\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.00000000\n0.00000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.26873924\n0.51840065\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5644446\n0.7512953\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6372678\n0.7982906\n+0.50\n\n\n\n\n\n\n\n\nTest: S20_r\n0.5912539\n0.7689304\n+0.56\n+0.64\n\n\n\n\n\n\n\nTest: SLJ\n0.6111377\n0.7817530\n+0.55\n+0.60\n+0.72\n\n\n\n\n\n\nTest: BMT\n0.4876120\n0.6982922\n+0.19\n+0.40\n+0.37\n+0.49\n\n\n\n\nSchool\nTest: Run\n0.1369534\n0.3700721\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0962706\n0.3102750\n+0.53\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1091782\n0.3304212\n+0.47\n+0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.0613205\n0.2476297\n+0.47\n+0.76\n+0.41\n\n\n\n\n\n\nTest: BMT\n0.0387012\n0.1967263\n+0.15\n+0.38\n+0.18\n+0.02\n\n\n\n\n\na1\n0.0801896\n0.2831776\n+0.58\n+0.47\n+0.56\n-0.05\n+0.65\n\n\n\n\nSex: Girls\n0.0205758\n0.1434427\n-0.63\n-0.26\n+0.06\n-0.28\n-0.58\n-0.37\n\n\nCohort\nTest: Run\n0.0030051\n0.0548189\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0004342\n0.0208386\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0031741\n0.0563393\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0002468\n0.0157111\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2601583\n0.5100571\n\n\n\n\n\n\n\n\n\n\n\n\nm1.œÉs      # VCs; m1.sigmas\n\n(Child = (var\"(Intercept)\" = 0.5899300186248844, var\"Test: Star_r\" = 0.7639612133604953, var\"Test: S20_r\" = 0.658219681909202, var\"Test: SLJ\" = 0.5700227005978211, var\"Test: BMT\" = 0.7425340800812925), School = (var\"(Intercept)\" = 0.2137559056537366, var\"Test: Star_r\" = 0.3329780987761621, var\"Test: S20_r\" = 0.32803884473380984, var\"Test: SLJ\" = 0.32097526864411674, var\"Test: BMT\" = 0.3128206391932548, a1 = 0.28264632412497126, var\"Sex: Girls\" = 0.1438204597464808), Cohort = (var\"(Intercept)\" = 0.01314995621094276, var\"Test: Star_r\" = 0.06371208452229353, var\"Test: S20_r\" = 0.0020325210658427655, var\"Test: SLJ\" = 0.033615946290033144, var\"Test: BMT\" = 0.0))\n\n\n\nm1.Œ∏       # Parameter vector for RES (w/o residual); m1.theta\n\n48-element Vector{Float64}:\n  1.1379808578187787\n  0.20482472350757436\n  0.0007568290993343182\n  0.05015862559035548\n -0.4783254792649123\n  1.4593852782675234\n -0.6667351473232765\n -0.048914305923583076\n  0.25189121887326976\n  1.080570675334367\n -0.5035583662207958\n -0.13405062065481124\n  0.9749848348297896\n  ‚ãÆ\n -0.025980202240427883\n  0.3845066623098876\n  0.11840626932751377\n -0.17005358340233095\n  0.012474158221657745\n -0.004267430792190249\n  0.0013652913404655612\n  0.025366395973694892\n  0.12290124304422674\n  0.003920753297880955\n  0.06484549385144353\n  0.0\n\n\n\nBlockDescription(m1) #  Description of blocks of A and L in a LinearMixedModel\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9930\nBlkDiag\n\n\n\n\n\n3101\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense\n\n\n\n\n\n\nm2.Œ∏\n\n48-element Vector{Float64}:\n  1.4729630067266388\n  0.7839590288563076\n  0.8442929134851123\n  0.8452773250238744\n  0.2646810066793581\n  1.354602147671133\n  0.6180607446857718\n  0.5707171082255509\n  0.4816688045536064\n  1.0852834447526913\n  0.547179574617927\n  0.21671488326108312\n  1.0047299268364405\n  ‚ãÆ\n -0.01805876323213624\n  0.31935406501511093\n  0.14056642217397697\n -0.18722520941287563\n  0.0019409804824588209\n -0.000854702979588228\n  0.0\n  0.1074759722955535\n  0.0408553982374923\n  0.11045685078823214\n  0.03080253323285869\n  0.0\n\n\n\nBlockDescription(m2)\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9930\nBlkDiag\n\n\n\n\n\n3101\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense"
  },
  {
    "objectID": "fggk21.html#model-predictions",
    "href": "fggk21.html#model-predictions",
    "title": "Basics with Emotikon Project",
    "section": "6.4 Model ‚Äúpredictions‚Äù",
    "text": "6.4 Model ‚Äúpredictions‚Äù\nThese commands inform us about extracion of conditional modes/means and (co-)variances, that using the model parameters to improve the predictions for units (levels) of the grouping (random) factors. We need this information, e.g., for partial-effect response profiles (e.g., facet plot) or effect profiles (e.g., caterpillar plot), or visualizing the borrowing-strength effect for correlation parameters (e.g., shrinkage plots). We are using the fit of LMM m2.\njulia&gt; condVar(m2)\nSome plotting functions are currently available from the MixedModelsMakie package or via custom functions.\n+ julia&gt; caterpillar!(m2)\n+ julia&gt; shrinkage!(m2)\n\n6.4.1 Conditional covariances\n\ncondVar(m1)\n\n3-element Vector{Array{Float64, 3}}:\n [0.05646046092775441 0.008741087080492059 ‚Ä¶ 0.0031541310120115004 -0.021366492030079107; 0.008741087080492059 0.2898070743228609 ‚Ä¶ -0.007395273870692747 0.02373863963471936; ‚Ä¶ ; 0.0031541310120115004 -0.007395273870692747 ‚Ä¶ 0.20511454245689775 -0.08320864426004519; -0.021366492030079107 0.02373863963471936 ‚Ä¶ -0.08320864426004519 0.2621984696751095;;; 0.06282981191597331 0.010873340067507205 ‚Ä¶ 0.003911304320141656 -0.028546511341588685; 0.010873340067507205 0.29888738083442257 ‚Ä¶ -0.005906595669994303 0.024370676194331312; ‚Ä¶ ; 0.003911304320141656 -0.005906595669994303 ‚Ä¶ 0.20895499475382556 -0.08406553259629664; -0.028546511341588685 0.024370676194331312 ‚Ä¶ -0.08406553259629664 0.27184758398121617;;; 0.05998897485888433 0.01007026807161604 ‚Ä¶ 0.0032319545291463526 -0.025312045521883212; 0.01007026807161604 0.2940394670641541 ‚Ä¶ -0.006817553888102131 0.023795082138195707; ‚Ä¶ ; 0.0032319545291463526 -0.006817553888102131 ‚Ä¶ 0.2078092502139327 -0.08344591293042244; -0.025312045521883212 0.023795082138195707 ‚Ä¶ -0.08344591293042244 0.26738308073259914;;; ‚Ä¶ ;;; 0.0911642999173916 0.01076609564325965 ‚Ä¶ -0.005637609554982028 -0.023662455633272725; 0.01076609564325965 0.29665063424015253 ‚Ä¶ -0.007332917065768334 0.025074183712715036; ‚Ä¶ ; -0.005637609554982028 -0.007332917065768334 ‚Ä¶ 0.20700327578177205 -0.08137850941302857; -0.023662455633272725 0.025074183712715036 ‚Ä¶ -0.08137850941302857 0.2673185912381177;;; 0.09758767655129515 0.008244537154902042 ‚Ä¶ -0.0010354836383137042 -0.022662590984633135; 0.008244537154902042 0.29316657183773565 ‚Ä¶ -0.007558444051222894 0.02545529557164294; ‚Ä¶ ; -0.0010354836383137042 -0.007558444051222894 ‚Ä¶ 0.20636024095767308 -0.08204002021725448; -0.022662590984633135 0.02545529557164294 ‚Ä¶ -0.08204002021725448 0.2643140589320666;;; 0.08427902904823857 0.008137131089769597 ‚Ä¶ -5.448343803071856e-5 -0.01908570180944885; 0.008137131089769597 0.2903404218730588 ‚Ä¶ -0.0077630785771130115 0.024937151640092022; ‚Ä¶ ; -5.448343803071856e-5 -0.0077630785771130115 ‚Ä¶ 0.20514228601518525 -0.08223816422128372; -0.01908570180944885 0.024937151640092022 ‚Ä¶ -0.08223816422128372 0.2613879265256324]\n [0.037382661325217725 -0.0018051470792949033 ‚Ä¶ 0.029940911136657724 -0.01244345223719613; -0.0018051470792949033 0.09051215264208363 ‚Ä¶ -0.012244610563499873 0.017439875521624456; ‚Ä¶ ; 0.029940911136657724 -0.012244610563499873 ‚Ä¶ 0.06558800966167774 -0.012746513247199394; -0.01244345223719613 0.017439875521624456 ‚Ä¶ -0.012746513247199394 0.018352311028304123;;; 0.03217464671591924 0.00019909974801327407 ‚Ä¶ 0.02627029742637268 -0.010660132285994132; 0.00019909974801327407 0.07639620696704796 ‚Ä¶ -0.007615472878529819 0.014245884411538212; ‚Ä¶ ; 0.02627029742637268 -0.007615472878529819 ‚Ä¶ 0.057429424799467686 -0.010752601438413252; -0.010660132285994132 0.014245884411538212 ‚Ä¶ -0.010752601438413252 0.015916025259580583;;; 0.02931773029899377 -0.00318202484369497 ‚Ä¶ 0.021529371207791897 -0.010468735783714597; -0.00318202484369497 0.0877854372242869 ‚Ä¶ -0.013135551350890045 0.018155944442391866; ‚Ä¶ ; 0.021529371207791897 -0.013135551350890045 ‚Ä¶ 0.05185205623812096 -0.01035444832586628; -0.010468735783714597 0.018155944442391866 ‚Ä¶ -0.01035444832586628 0.016797747820827533;;; ‚Ä¶ ;;; 0.026752946917134604 0.0028930880769873213 ‚Ä¶ 0.01903100884665739 -0.006858818931875681; 0.0028930880769873213 0.07613947167907514 ‚Ä¶ -0.005487860383599791 0.012942351267027008; ‚Ä¶ ; 0.01903100884665739 -0.005487860383599791 ‚Ä¶ 0.05093403107631236 -0.006102052856935498; -0.006858818931875681 0.012942351267027008 ‚Ä¶ -0.006102052856935498 0.013722354630431572;;; 0.027218597898895663 0.0035105914941084113 ‚Ä¶ 0.02031450027310185 -0.006162818466481287; 0.0035105914941084113 0.07605736439573893 ‚Ä¶ -0.004958713622938641 0.012639471135981264; ‚Ä¶ ; 0.02031450027310185 -0.004958713622938641 ‚Ä¶ 0.05307335906565115 -0.006292276029352733; -0.006162818466481287 0.012639471135981264 ‚Ä¶ -0.006292276029352733 0.013744887928746738;;; 0.043329326310117225 -0.003077486069647413 ‚Ä¶ 0.03639152682395296 -0.013665784308341528; -0.003077486069647413 0.09944879336028199 ‚Ä¶ -0.014990671654062395 0.01920040229893295; ‚Ä¶ ; 0.03639152682395296 -0.014990671654062395 ‚Ä¶ 0.07512799464195363 -0.014346672368784442; -0.013665784308341528 0.01920040229893295 ‚Ä¶ -0.014346672368784442 0.019529046292703082]\n [0.0001624893302946822 6.809276760802604e-6 ‚Ä¶ -4.6917226461083115e-6 0.0; 6.809276760802604e-6 0.002350815403225895 ‚Ä¶ -0.00015352131411632052 0.0; ‚Ä¶ ; -4.6917226461083115e-6 -0.00015352131411632052 ‚Ä¶ 0.000851482182428563 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00015913802081720684 8.636268905937163e-6 ‚Ä¶ -5.971228600979366e-6 0.0; 8.636268905937163e-6 0.0020370776713334775 ‚Ä¶ -0.00016803507630411106 0.0; ‚Ä¶ ; -5.971228600979366e-6 -0.00016803507630411106 ‚Ä¶ 0.0007841967975071386 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00016107332729719428 7.517372519491449e-6 ‚Ä¶ -5.10400306824662e-6 0.0; 7.517372519491449e-6 0.0022348773520284035 ‚Ä¶ -0.00015922539038549057 0.0; ‚Ä¶ ; -5.10400306824662e-6 -0.00015922539038549057 ‚Ä¶ 0.0008237973652566612 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00016046986147184554 8.294439488860557e-6 ‚Ä¶ -5.42301575559805e-6 0.0; 8.294439488860557e-6 0.0021568108017217787 ‚Ä¶ -0.0001637977537184879 0.0; ‚Ä¶ ; -5.42301575559805e-6 -0.0001637977537184879 ‚Ä¶ 0.0008119432272028718 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0001580856198253438 8.548764514673167e-6 ‚Ä¶ -6.842906693187147e-6 0.0; 8.548764514673167e-6 0.0019544407667465287 ‚Ä¶ -0.00016998440973049989 0.0; ‚Ä¶ ; -6.842906693187147e-6 -0.00016998440973049989 ‚Ä¶ 0.000763281290146378 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00015732346828528415 7.309910180796245e-6 ‚Ä¶ -7.278918997664716e-6 0.0; 7.309910180796245e-6 0.0019259339350955339 ‚Ä¶ -0.00016887030366421503 0.0; ‚Ä¶ ; -7.278918997664716e-6 -0.00016887030366421503 ‚Ä¶ 0.0007463694625123178 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0001593872289661442 7.892011292068771e-6 ‚Ä¶ -6.353962391643299e-6 0.0; 7.892011292068771e-6 0.002069851138743824 ‚Ä¶ -0.00016756307945140484 0.0; ‚Ä¶ ; -6.353962391643299e-6 -0.00016756307945140484 ‚Ä¶ 0.0007884351701505988 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00015708408471662314 8.323399337044809e-6 ‚Ä¶ -7.845522431501192e-6 0.0; 8.323399337044809e-6 0.0018808801004042105 ‚Ä¶ -0.00017392113057051914 0.0; ‚Ä¶ ; -7.845522431501192e-6 -0.00017392113057051914 ‚Ä¶ 0.0007429208051915409 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.00015529524243854123 9.604794249145802e-6 ‚Ä¶ -7.639791823552067e-6 0.0; 9.604794249145802e-6 0.0017804671632761467 ‚Ä¶ -0.0001735958331640202 0.0; ‚Ä¶ ; -7.639791823552067e-6 -0.0001735958331640202 ‚Ä¶ 0.0007169963867619168 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0]\n\n\n\ncondVar(m2)\n\n3-element Vector{Array{Float64, 3}}:\n [0.17090827908394837 0.027262026576516135 ‚Ä¶ 0.03380346024298022 -0.00725898477697015; 0.027262026576516135 0.17362860841886468 ‚Ä¶ 0.03725760536477991 0.018606919834712306; ‚Ä¶ ; 0.03380346024298022 0.03725760536477991 ‚Ä¶ 0.16027831963262387 0.028396673886695065; -0.00725898477697015 0.018606919834712306 ‚Ä¶ 0.028396673886695065 0.16044266373295085;;; 0.18217266813532992 0.03403215638194173 ‚Ä¶ 0.04110909046910593 -0.009203498970715065; 0.03403215638194173 0.1854807112555363 ‚Ä¶ 0.0484535420768501 0.021145389994612112; ‚Ä¶ ; 0.04110909046910593 0.0484535420768501 ‚Ä¶ 0.17270612527308093 0.03080503422943898; -0.009203498970715065 0.021145389994612112 ‚Ä¶ 0.03080503422943898 0.1628999798226587;;; 0.17598207538028804 0.030839707406116214 ‚Ä¶ 0.037829209453142054 -0.008027447928738635; 0.030839707406116214 0.18017131892730673 ‚Ä¶ 0.04329994145914574 0.019886266952541386; ‚Ä¶ ; 0.037829209453142054 0.04329994145914574 ‚Ä¶ 0.16700723694482034 0.029647017101376034; -0.008027447928738635 0.019886266952541386 ‚Ä¶ 0.029647017101376034 0.16162269221670272;;; ‚Ä¶ ;;; 0.2173727394927963 0.07077417442793549 ‚Ä¶ 0.0671331561164205 0.021622197935503698; 0.07077417442793549 0.22137583323754262 ‚Ä¶ 0.07253677420131632 0.05084345335238386; ‚Ä¶ ; 0.0671331561164205 0.07253677420131632 ‚Ä¶ 0.18357724232380448 0.05024789366648706; 0.021622197935503698 0.05084345335238386 ‚Ä¶ 0.05024789366648706 0.18615473888535655;;; 0.22427545347433642 0.07760416492591914 ‚Ä¶ 0.0766739573301095 0.03241003430210565; 0.07760416492591914 0.22447630154317874 ‚Ä¶ 0.07866285848857124 0.05862485650015413; ‚Ä¶ ; 0.0766739573301095 0.07866285848857124 ‚Ä¶ 0.19229940149864155 0.06063971394271496; 0.03241003430210565 0.05862485650015413 ‚Ä¶ 0.06063971394271496 0.1950709514513498;;; 0.20604841432017237 0.06133280233274787 ‚Ä¶ 0.06198644244541778 0.02228070633700272; 0.06133280233274787 0.2071819423793147 ‚Ä¶ 0.0637708267715584 0.04774655852993967; ‚Ä¶ ; 0.06198644244541778 0.0637708267715584 ‚Ä¶ 0.1791596515000308 0.051377608691885325; 0.02228070633700272 0.04774655852993967 ‚Ä¶ 0.051377608691885325 0.18669597320366366]\n [0.10762625120838518 0.04838303958232419 ‚Ä¶ 0.0477193367718641 -0.027746475219333506; 0.04838303958232419 0.07948773455156048 ‚Ä¶ 0.0348982189813234 -0.010275451351059854; ‚Ä¶ ; 0.0477193367718641 0.0348982189813234 ‚Ä¶ 0.06571253141716403 -0.012819964751108165; -0.027746475219333506 -0.010275451351059854 ‚Ä¶ -0.012819964751108165 0.018249001379576288;;; 0.08786705405494342 0.04038394833689375 ‚Ä¶ 0.04010641390582609 -0.02268094493330316; 0.04038394833689375 0.0691386963788005 ‚Ä¶ 0.03203056278906753 -0.00839840230704004; ‚Ä¶ ; 0.04010641390582609 0.03203056278906753 ‚Ä¶ 0.057460951832629205 -0.010821249376082568; -0.02268094493330316 -0.00839840230704004 ‚Ä¶ -0.010821249376082568 0.015830523970505453;;; 0.09631029751643853 0.03587539258354575 ‚Ä¶ 0.03838831009690908 -0.025396292957756198; 0.03587539258354575 0.06308408495911258 ‚Ä¶ 0.024737958411540202 -0.007211734104591098; ‚Ä¶ ; 0.03838831009690908 0.024737958411540202 ‚Ä¶ 0.05195118580798586 -0.010453523302885338; -0.025396292957756198 -0.007211734104591098 ‚Ä¶ -0.010453523302885338 0.01671012877070668;;; ‚Ä¶ ;;; 0.07438147990531531 0.0305305111571838 ‚Ä¶ 0.029412174280976297 -0.017012235372204258; 0.0305305111571838 0.06268187406983632 ‚Ä¶ 0.023453579910918183 -0.00403058941307241; ‚Ä¶ ; 0.029412174280976297 0.023453579910918183 ‚Ä¶ 0.05096069628401587 -0.00618427832636701; -0.017012235372204258 -0.00403058941307241 ‚Ä¶ -0.00618427832636701 0.013655181344400849;;; 0.07353882538543838 0.030541819753753648 ‚Ä¶ 0.030140659927341572 -0.016051860394370857; 0.030541819753753648 0.06347376869476529 ‚Ä¶ 0.024725729033149037 -0.0033653120951671655; ‚Ä¶ ; 0.030140659927341572 0.024725729033149037 ‚Ä¶ 0.053085730286024674 -0.006368644184479695; -0.016051860394370857 -0.0033653120951671655 ‚Ä¶ -0.006368644184479695 0.013679938216201639;;; 0.12465965193831827 0.0579439268525617 ‚Ä¶ 0.0576096293475985 -0.030973421607732164; 0.0579439268525617 0.09049603789220044 ‚Ä¶ 0.04197587901957836 -0.01174320550801033; ‚Ä¶ ; 0.0576096293475985 0.04197587901957836 ‚Ä¶ 0.07532213698238206 -0.01442195543124506; -0.030973421607732164 -0.01174320550801033 ‚Ä¶ -0.01442195543124506 0.019422773635780873]\n [0.0018733240498050652 3.096151407390133e-5 ‚Ä¶ 2.2244806033743188e-5 0.0; 3.096151407390133e-5 0.0003979172766731916 ‚Ä¶ 5.69132236754644e-6 0.0; ‚Ä¶ ; 2.2244806033743188e-5 5.69132236754644e-6 ‚Ä¶ 0.00023306275288016156 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0016446428664714044 3.72910599876562e-5 ‚Ä¶ 2.723027443151125e-5 0.0; 3.72910599876562e-5 0.00038664282322931987 ‚Ä¶ 7.791883693382405e-6 0.0; ‚Ä¶ ; 2.723027443151125e-5 7.791883693382405e-6 ‚Ä¶ 0.00022861952824465714 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0017877970891102882 3.243158361859379e-5 ‚Ä¶ 2.4229209032921504e-5 0.0; 3.243158361859379e-5 0.00039383229376694387 ‚Ä¶ 6.503673530124919e-6 0.0; ‚Ä¶ ; 2.4229209032921504e-5 6.503673530124919e-6 ‚Ä¶ 0.000231255131594314 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0017309507895529432 3.4360740268888014e-5 ‚Ä¶ 2.5245564123808525e-5 0.0; 3.4360740268888014e-5 0.000391740323920398 ‚Ä¶ 6.72051620707648e-6 0.0; ‚Ä¶ ; 2.5245564123808525e-5 6.72051620707648e-6 ‚Ä¶ 0.00023059331670321413 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.001586490132064953 3.888041156707243e-5 ‚Ä¶ 2.8017507733504467e-5 0.0; 3.888041156707243e-5 0.00038357584814544546 ‚Ä¶ 8.147743352690285e-6 0.0; ‚Ä¶ ; 2.8017507733504467e-5 8.147743352690285e-6 ‚Ä¶ 0.00022719515986812782 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0015684894428273305 4.000129888505287e-5 ‚Ä¶ 2.8685004036434755e-5 0.0; 4.000129888505287e-5 0.00038013479938511346 ‚Ä¶ 8.963434176674353e-6 0.0; ‚Ä¶ ; 2.8685004036434755e-5 8.963434176674353e-6 ‚Ä¶ 0.00022596457956954328 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0016701131270366056 3.692631322356877e-5 ‚Ä¶ 2.624349422033717e-5 0.0; 3.692631322356877e-5 0.00038705399687490586 ‚Ä¶ 7.544287495385443e-6 0.0; ‚Ä¶ ; 2.624349422033717e-5 7.544287495385443e-6 ‚Ä¶ 0.00022886710469376584 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.0015333233660692336 4.0894638884906606e-5 ‚Ä¶ 2.924110404372773e-5 0.0; 4.0894638884906606e-5 0.00037999301464735014 ‚Ä¶ 8.864917680249245e-6 0.0; ‚Ä¶ ; 2.924110404372773e-5 8.864917680249245e-6 ‚Ä¶ 0.00022568435707695788 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0;;; 0.001453393379725358 4.285087133359302e-5 ‚Ä¶ 3.085019200677401e-5 0.0; 4.285087133359302e-5 0.0003739504094135209 ‚Ä¶ 1.0040622295976805e-5 0.0; ‚Ä¶ ; 3.085019200677401e-5 1.0040622295976805e-5 ‚Ä¶ 0.00022374726281913847 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0]\n\n\nThey are hard to look at. Let‚Äôs take pictures.\n\n\n6.4.2 Caterpillar plots\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 400)), ranefinfo(m1, :Cohort)\n)\n\n\n\n\n\nFigure¬†3: Prediction intervals of the random effects for Cohort in model m1\n\n\n\n\n\n\n6.4.3 Shrinkage plots\nThese are just teasers. We will pick this up in a separate tutorial. Enjoy!\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m1, :Cohort)\n\n\n\n\n\nFigure¬†4: Shrinkage plot of the random effects for Cohort in model m1\n\n\n\n\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m2, :Cohort)\n\n\n\n\n\nFigure¬†5: Shrinkage plot of the random effects for Cohort in model m2\n\n\n\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nSchielzeth, H., & Forstmeier, W. (2008). Conclusions beyond support: Overconfident estimates in mixed models. Behavioral Ecology, 20(2), 416‚Äì420. https://doi.org/10.1093/beheco/arn145"
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "",
    "text": "Ths script uses a subset of data reported in F√ºhner, Golle, Granacher, & Kliegl (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. (F√ºhner et al., 2021)\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollement, that is in their ninth year of life in the third grade. To avoid delays associated with model fitting we work with a reduced data set and less complex models than those in the reference publication. The script requires only a few changes to specify the more complex models in the paper.\nThe script is structured in three main sections:"
  },
  {
    "objectID": "contrasts_fggk21.html#packages-and-functions",
    "href": "contrasts_fggk21.html#packages-and-functions",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.1 Packages and functions",
    "text": "1.1 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing AlgebraOfGraphics: linear\nusing Arrow\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing ProgressMeter\nusing SMLP2023: dataset\nusing Statistics\nusing StatsBase\n\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "href": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.2 Readme for dataset(\"fggk21\")",
    "text": "1.2 Readme for dataset(\"fggk21\")\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: ‚ÄúGirls‚Äù (n=55,086), ‚ÄúBoys‚Äù (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units"
  },
  {
    "objectID": "contrasts_fggk21.html#preprocessing",
    "href": "contrasts_fggk21.html#preprocessing",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.3 Preprocessing",
    "text": "1.3 Preprocessing\n\n1.3.1 Read data\n\ntbl = dataset(:fggk21)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n\n\n\n\n\n\n1.3.2 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\nbegin\n  dat = @chain df begin\n    @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n    @groupby(:Test, :Sex)\n    combine(x -&gt; x[sample(1:nrow(x), 500), :])\n  end\nend\n\n5000√ó7 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2016\nS101126\nC055331\n8.54483\n4.25532\n\n\n2\nS20_r\nBoys\n2015\nS101928\nC064538\n8.62149\n5.40541\n\n\n3\nS20_r\nBoys\n2019\nS100596\nC084546\n8.80219\n4.0\n\n\n4\nS20_r\nBoys\n2012\nS103706\nC003399\n7.99726\n4.16667\n\n\n5\nS20_r\nBoys\n2019\nS106124\nC066228\n8.63244\n4.16667\n\n\n6\nS20_r\nBoys\n2015\nS112665\nC075466\n8.71184\n4.87805\n\n\n7\nS20_r\nBoys\n2013\nS110346\nC021775\n8.24641\n4.7619\n\n\n8\nS20_r\nBoys\n2013\nS104863\nC068725\n8.65982\n3.92157\n\n\n9\nS20_r\nBoys\n2013\nS101552\nC086997\n8.82683\n4.87805\n\n\n10\nS20_r\nBoys\n2012\nS105454\nC089151\n8.83231\n5.40541\n\n\n11\nS20_r\nBoys\n2017\nS104954\nC044311\n8.44353\n4.25532\n\n\n12\nS20_r\nBoys\n2014\nS102167\nC073637\n8.70363\n4.87805\n\n\n13\nS20_r\nBoys\n2016\nS103925\nC085905\n8.80493\n4.25532\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n4989\nRun\nGirls\n2012\nS103330\nC007396\n8.08214\n1008.0\n\n\n4990\nRun\nGirls\n2012\nS106227\nC014395\n8.16701\n1053.0\n\n\n4991\nRun\nGirls\n2011\nS102416\nC041801\n8.41889\n1026.0\n\n\n4992\nRun\nGirls\n2013\nS112963\nC040439\n8.41342\n1035.0\n\n\n4993\nRun\nGirls\n2012\nS130606\nC003730\n7.99726\n756.0\n\n\n4994\nRun\nGirls\n2014\nS105983\nC009886\n8.1232\n956.0\n\n\n4995\nRun\nGirls\n2019\nS104838\nC037959\n8.38877\n1080.0\n\n\n4996\nRun\nGirls\n2013\nS102465\nC039707\n8.41342\n940.0\n\n\n4997\nRun\nGirls\n2017\nS110346\nC103601\n8.95825\n990.0\n\n\n4998\nRun\nGirls\n2015\nS104590\nC010614\n8.13142\n1143.0\n\n\n4999\nRun\nGirls\n2011\nS103123\nC069916\n8.6653\n1050.0\n\n\n5000\nRun\nGirls\n2018\nS103172\nC034090\n8.35866\n1017.0\n\n\n\n\n\n\n\n\n1.3.3 Transformations\n\nbegin\n  transform!(dat, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\n  select!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\nend\n\n5000√ó9 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\na1\nzScore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2016\nS101126\nC055331\n8.54483\n4.25532\n0.0448323\n-0.673696\n\n\n2\nS20_r\nBoys\n2015\nS101928\nC064538\n8.62149\n5.40541\n0.121492\n2.15564\n\n\n3\nS20_r\nBoys\n2019\nS100596\nC084546\n8.80219\n4.0\n0.30219\n-1.30181\n\n\n4\nS20_r\nBoys\n2012\nS103706\nC003399\n7.99726\n4.16667\n-0.502738\n-0.891791\n\n\n5\nS20_r\nBoys\n2019\nS106124\nC066228\n8.63244\n4.16667\n0.132444\n-0.891791\n\n\n6\nS20_r\nBoys\n2015\nS112665\nC075466\n8.71184\n4.87805\n0.211841\n0.858285\n\n\n7\nS20_r\nBoys\n2013\nS110346\nC021775\n8.24641\n4.7619\n-0.253593\n0.572558\n\n\n8\nS20_r\nBoys\n2013\nS104863\nC068725\n8.65982\n3.92157\n0.159822\n-1.49476\n\n\n9\nS20_r\nBoys\n2013\nS101552\nC086997\n8.82683\n4.87805\n0.326831\n0.858285\n\n\n10\nS20_r\nBoys\n2012\nS105454\nC089151\n8.83231\n5.40541\n0.332307\n2.15564\n\n\n11\nS20_r\nBoys\n2017\nS104954\nC044311\n8.44353\n4.25532\n-0.0564682\n-0.673696\n\n\n12\nS20_r\nBoys\n2014\nS102167\nC073637\n8.70363\n4.87805\n0.203628\n0.858285\n\n\n13\nS20_r\nBoys\n2016\nS103925\nC085905\n8.80493\n4.25532\n0.304928\n-0.673696\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n4989\nRun\nGirls\n2012\nS103330\nC007396\n8.08214\n1008.0\n-0.417864\n-0.0226422\n\n\n4990\nRun\nGirls\n2012\nS106227\nC014395\n8.16701\n1053.0\n-0.332991\n0.288948\n\n\n4991\nRun\nGirls\n2011\nS102416\nC041801\n8.41889\n1026.0\n-0.0811088\n0.101994\n\n\n4992\nRun\nGirls\n2013\nS112963\nC040439\n8.41342\n1035.0\n-0.0865845\n0.164312\n\n\n4993\nRun\nGirls\n2012\nS130606\nC003730\n7.99726\n756.0\n-0.502738\n-1.76755\n\n\n4994\nRun\nGirls\n2014\nS105983\nC009886\n8.1232\n956.0\n-0.376797\n-0.382702\n\n\n4995\nRun\nGirls\n2019\nS104838\nC037959\n8.38877\n1080.0\n-0.111225\n0.475902\n\n\n4996\nRun\nGirls\n2013\nS102465\nC039707\n8.41342\n940.0\n-0.0865845\n-0.493489\n\n\n4997\nRun\nGirls\n2017\nS110346\nC103601\n8.95825\n990.0\n0.458248\n-0.147278\n\n\n4998\nRun\nGirls\n2015\nS104590\nC010614\n8.13142\n1143.0\n-0.368583\n0.912127\n\n\n4999\nRun\nGirls\n2011\nS103123\nC069916\n8.6653\n1050.0\n0.165298\n0.268175\n\n\n5000\nRun\nGirls\n2018\nS103172\nC034090\n8.35866\n1017.0\n-0.141342\n0.0396758\n\n\n\n\n\n\n\nbegin\n  dat2 = combine(\n    groupby(dat, [:Test, :Sex]),\n    :score =&gt; mean,\n    :score =&gt; std,\n    :zScore =&gt; mean,\n    :zScore =&gt; std,\n  )\nend\n\n10√ó6 DataFrame\n\n\n\nRow\nTest\nSex\nscore_mean\nscore_std\nzScore_mean\nzScore_std\n\n\n\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n4.60109\n0.403418\n0.176935\n0.992451\n\n\n2\nBPT\nBoys\n3.9824\n0.714052\n0.327665\n0.995193\n\n\n3\nSLJ\nBoys\n128.066\n18.9068\n0.213395\n0.992772\n\n\n4\nStar_r\nBoys\n2.08526\n0.301998\n0.106804\n1.04534\n\n\n5\nRun\nBoys\n1039.76\n149.845\n0.197299\n1.03756\n\n\n6\nS20_r\nGirls\n4.45725\n0.397091\n-0.176935\n0.976887\n\n\n7\nBPT\nGirls\n3.5122\n0.640321\n-0.327665\n0.892433\n\n\n8\nSLJ\nGirls\n119.938\n18.3179\n-0.213395\n0.961847\n\n\n9\nStar_r\nGirls\n2.02355\n0.271995\n-0.106804\n0.941489\n\n\n10\nRun\nGirls\n982.776\n132.951\n-0.197299\n0.920579\n\n\n\n\n\n\n\n\n1.3.4 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566."
  },
  {
    "objectID": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "href": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.1 SeqDiffCoding: contr1",
    "text": "2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = merge(\n  Dict(nm =&gt; Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  ),\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :Child  =&gt; Grouping()\n  :School =&gt; Grouping()\n  :Test   =&gt; SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Cohort =&gt; Grouping()\n  :Sex    =&gt; EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nf_ovi_1 = @formula zScore ~ 1 + Test + (1 | Child);\n\n\nm_ovi_SeqDiff_1 = fit(MixedModel, f_ovi_1, dat; contrasts=contr1)\n\nMinimizing 12    Time: 0:00:00 (18.49 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0007\n0.0143\n-0.05\n0.9628\n0.7504\n\n\nTest: Star_r\n0.0084\n0.0441\n0.19\n0.8482\n\n\n\nTest: S20_r\n-0.0003\n0.0441\n-0.01\n0.9951\n\n\n\nTest: SLJ\n0.0049\n0.0442\n0.11\n0.9117\n\n\n\nTest: BPT\n-0.0105\n0.0441\n-0.24\n0.8111\n\n\n\nResidual\n0.6601\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = let\n  form = @formula zScore ~ 1 + Test * a1 + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0173\n0.0145\n-1.19\n0.2330\n0.7363\n\n\nTest: Star_r\n-0.0170\n0.0447\n-0.38\n0.7038\n\n\n\nTest: S20_r\n-0.0044\n0.0451\n-0.10\n0.9222\n\n\n\nTest: SLJ\n0.0235\n0.0453\n0.52\n0.6029\n\n\n\nTest: BPT\n-0.0299\n0.0447\n-0.67\n0.5032\n\n\n\na1\n0.2799\n0.0494\n5.67\n&lt;1e-07\n\n\n\nTest: Star_r & a1\n0.4328\n0.1513\n2.86\n0.0042\n\n\n\nTest: S20_r & a1\n-0.0572\n0.1527\n-0.37\n0.7078\n\n\n\nTest: SLJ & a1\n-0.2174\n0.1553\n-1.40\n0.1614\n\n\n\nTest: BPT & a1\n0.4043\n0.1540\n2.63\n0.0087\n\n\n\nResidual\n0.6680\n\n\n\n\n\n\n\n\n\nThe difference between older and younger childrend is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = let\n  form = @formula zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0162\n0.0141\n-1.14\n0.2530\n0.7065\n\n\nTest: Star_r\n-0.0192\n0.0437\n-0.44\n0.6598\n\n\n\nTest: S20_r\n-0.0019\n0.0441\n-0.04\n0.9656\n\n\n\nTest: SLJ\n0.0225\n0.0442\n0.51\n0.6114\n\n\n\nTest: BPT\n-0.0323\n0.0437\n-0.74\n0.4591\n\n\n\na1\n0.2487\n0.0483\n5.15\n&lt;1e-06\n\n\n\nSex: Boys\n0.2042\n0.0139\n14.72\n&lt;1e-48\n\n\n\nTest: Star_r & a1\n0.4826\n0.1481\n3.26\n0.0011\n\n\n\nTest: S20_r & a1\n-0.0973\n0.1495\n-0.65\n0.5151\n\n\n\nTest: SLJ & a1\n-0.2326\n0.1521\n-1.53\n0.1263\n\n\n\nTest: BPT & a1\n0.4721\n0.1507\n3.13\n0.0017\n\n\n\nTest: Star_r & Sex: Boys\n-0.1038\n0.0430\n-2.42\n0.0157\n\n\n\nTest: S20_r & Sex: Boys\n0.0732\n0.0430\n1.70\n0.0883\n\n\n\nTest: SLJ & Sex: Boys\n0.0374\n0.0431\n0.87\n0.3858\n\n\n\nTest: BPT & Sex: Boys\n0.1119\n0.0430\n2.60\n0.0093\n\n\n\nResidual\n0.6652\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role ‚Äì as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls‚Äô ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\nf_ovi = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Child)\nm_ovi_SeqDiff = fit(MixedModel, f_ovi, dat; contrasts=contr1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0146\n0.0142\n-1.03\n0.3014\n0.7071\n\n\nTest: Star_r\n-0.0214\n0.0437\n-0.49\n0.6240\n\n\n\nTest: S20_r\n-0.0046\n0.0441\n-0.10\n0.9164\n\n\n\nTest: SLJ\n0.0307\n0.0443\n0.69\n0.4893\n\n\n\nTest: BPT\n-0.0384\n0.0437\n-0.88\n0.3803\n\n\n\na1\n0.2498\n0.0482\n5.18\n&lt;1e-06\n\n\n\nSex: Boys\n0.2074\n0.0142\n14.65\n&lt;1e-47\n\n\n\nTest: Star_r & a1\n0.4820\n0.1480\n3.26\n0.0011\n\n\n\nTest: S20_r & a1\n-0.0950\n0.1495\n-0.64\n0.5251\n\n\n\nTest: SLJ & a1\n-0.2365\n0.1520\n-1.56\n0.1198\n\n\n\nTest: BPT & a1\n0.4716\n0.1506\n3.13\n0.0017\n\n\n\nTest: Star_r & Sex: Boys\n-0.1086\n0.0437\n-2.49\n0.0129\n\n\n\nTest: S20_r & Sex: Boys\n0.0644\n0.0441\n1.46\n0.1447\n\n\n\nTest: SLJ & Sex: Boys\n0.0585\n0.0443\n1.32\n0.1867\n\n\n\nTest: BPT & Sex: Boys\n0.0935\n0.0437\n2.14\n0.0325\n\n\n\na1 & Sex: Boys\n-0.0623\n0.0482\n-1.29\n0.1965\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.0842\n0.1480\n0.57\n0.5695\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.1266\n0.1495\n0.85\n0.3970\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.3431\n0.1520\n-2.26\n0.0240\n\n\n\nTest: BPT & a1 & Sex: Boys\n0.3316\n0.1506\n2.20\n0.0277\n\n\n\nResidual\n0.6633\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics."
  },
  {
    "objectID": "contrasts_fggk21.html#helmertcoding-contr2",
    "href": "contrasts_fggk21.html#helmertcoding-contr2",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.2 HelmertCoding: contr2",
    "text": "2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiosrespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performats also depends on endurance. HeC1 yields a measure of Coordination ‚Äúcorrected‚Äù for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate ‚Äúorthogonal‚Äù than ‚Äúnon-orthogonal‚Äù random-effect structures.\n\ncontr2 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = fit(MixedModel, f_ovi, dat; contrasts=contr2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0146\n0.0142\n-1.03\n0.3014\n0.7071\n\n\nTest: Star_r\n-0.0107\n0.0218\n-0.49\n0.6240\n\n\n\nTest: S20_r\n-0.0051\n0.0128\n-0.40\n0.6894\n\n\n\nTest: SLJ\n0.0051\n0.0090\n0.57\n0.5694\n\n\n\nTest: BPT\n-0.0046\n0.0069\n-0.67\n0.5046\n\n\n\na1\n0.2498\n0.0482\n5.18\n&lt;1e-06\n\n\n\nSex: Boys\n0.2074\n0.0142\n14.65\n&lt;1e-47\n\n\n\nTest: Star_r & a1\n0.2410\n0.0740\n3.26\n0.0011\n\n\n\nTest: S20_r & a1\n0.0487\n0.0433\n1.12\n0.2612\n\n\n\nTest: SLJ & a1\n-0.0348\n0.0309\n-1.13\n0.2606\n\n\n\nTest: BPT & a1\n0.0734\n0.0235\n3.12\n0.0018\n\n\n\nTest: Star_r & Sex: Boys\n-0.0543\n0.0218\n-2.49\n0.0129\n\n\n\nTest: S20_r & Sex: Boys\n0.0034\n0.0128\n0.26\n0.7929\n\n\n\nTest: SLJ & Sex: Boys\n0.0163\n0.0090\n1.82\n0.0691\n\n\n\nTest: BPT & Sex: Boys\n0.0285\n0.0069\n4.12\n&lt;1e-04\n\n\n\na1 & Sex: Boys\n-0.0623\n0.0482\n-1.29\n0.1965\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.0421\n0.0740\n0.57\n0.5695\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0562\n0.0433\n1.30\n0.1945\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.0577\n0.0309\n-1.86\n0.0622\n\n\n\nTest: BPT & a1 & Sex: Boys\n0.0317\n0.0235\n1.35\n0.1780\n\n\n\nResidual\n0.6633\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent."
  },
  {
    "objectID": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "href": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.3 HypothesisCoding: contr3",
    "text": "2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1 0 0 0\n       0 -1 +1 0 0\n       0 0 -1 +1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = fit(MixedModel, f_ovi, dat; contrasts=contr3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0146\n0.0142\n-1.03\n0.3014\n0.7071\n\n\nTest: BPT-other\n-0.0921\n0.1381\n-0.67\n0.5046\n\n\n\nTest: Star-End\n-0.0214\n0.0437\n-0.49\n0.6240\n\n\n\nTest: S20-Star\n-0.0046\n0.0441\n-0.10\n0.9164\n\n\n\nTest: SLJ-S20\n0.0307\n0.0443\n0.69\n0.4893\n\n\n\na1\n0.2498\n0.0482\n5.18\n&lt;1e-06\n\n\n\nSex: Boys\n0.2074\n0.0142\n14.65\n&lt;1e-47\n\n\n\nTest: BPT-other & a1\n1.4689\n0.4708\n3.12\n0.0018\n\n\n\nTest: Star-End & a1\n0.4820\n0.1480\n3.26\n0.0011\n\n\n\nTest: S20-Star & a1\n-0.0950\n0.1495\n-0.64\n0.5251\n\n\n\nTest: SLJ-S20 & a1\n-0.2365\n0.1520\n-1.56\n0.1198\n\n\n\nTest: BPT-other & Sex: Boys\n0.5696\n0.1381\n4.12\n&lt;1e-04\n\n\n\nTest: Star-End & Sex: Boys\n-0.1086\n0.0437\n-2.49\n0.0129\n\n\n\nTest: S20-Star & Sex: Boys\n0.0644\n0.0441\n1.46\n0.1447\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0585\n0.0443\n1.32\n0.1867\n\n\n\na1 & Sex: Boys\n-0.0623\n0.0482\n-1.29\n0.1965\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n0.6342\n0.4708\n1.35\n0.1780\n\n\n\nTest: Star-End & a1 & Sex: Boys\n0.0842\n0.1480\n0.57\n0.5695\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1266\n0.1495\n0.85\n0.3970\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3431\n0.1520\n-2.26\n0.0240\n\n\n\nResidual\n0.6633\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 +1 0 0 0\n      0 -1 +1 0 0\n      0 0 -1 +1 0\n      0 0 0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = fit(MixedModel, f_ovi, dat; contrasts=contr1b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0146\n0.0142\n-1.03\n0.3014\n0.7071\n\n\nTest: Star-Run\n-0.0214\n0.0437\n-0.49\n0.6240\n\n\n\nTest: S20-Star\n-0.0046\n0.0441\n-0.10\n0.9164\n\n\n\nTest: SLJ-S20\n0.0307\n0.0443\n0.69\n0.4893\n\n\n\nTest: BPT-SLJ\n-0.0384\n0.0437\n-0.88\n0.3803\n\n\n\na1\n0.2498\n0.0482\n5.18\n&lt;1e-06\n\n\n\nSex: Boys\n0.2074\n0.0142\n14.65\n&lt;1e-47\n\n\n\nTest: Star-Run & a1\n0.4820\n0.1480\n3.26\n0.0011\n\n\n\nTest: S20-Star & a1\n-0.0950\n0.1495\n-0.64\n0.5251\n\n\n\nTest: SLJ-S20 & a1\n-0.2365\n0.1520\n-1.56\n0.1198\n\n\n\nTest: BPT-SLJ & a1\n0.4716\n0.1506\n3.13\n0.0017\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1086\n0.0437\n-2.49\n0.0129\n\n\n\nTest: S20-Star & Sex: Boys\n0.0644\n0.0441\n1.46\n0.1447\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0585\n0.0443\n1.32\n0.1867\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0935\n0.0437\n2.14\n0.0325\n\n\n\na1 & Sex: Boys\n-0.0623\n0.0482\n-1.29\n0.1965\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0842\n0.1480\n0.57\n0.5695\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1266\n0.1495\n0.85\n0.3970\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3431\n0.1520\n-2.26\n0.0240\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.3316\n0.1506\n2.20\n0.0277\n\n\n\nResidual\n0.6633\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 147   Time: 0:00:00 ( 4.62 ms/it)\n  objective:  13830.250639396229\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0148\n0.0142\n-1.05\n0.2944\n0.7189\n\n\nTest: Star-Run\n-0.0195\n0.0445\n-0.44\n0.6611\n0.1035\n\n\nTest: S20-Star\n-0.0076\n0.0441\n-0.17\n0.8632\n0.6320\n\n\nTest: SLJ-S20\n0.0337\n0.0435\n0.77\n0.4393\n0.3200\n\n\nTest: BPT-SLJ\n-0.0388\n0.0430\n-0.90\n0.3674\n0.0000\n\n\na1\n0.2499\n0.0482\n5.18\n&lt;1e-06\n\n\n\nSex: Boys\n0.2075\n0.0142\n14.66\n&lt;1e-47\n\n\n\nTest: Star-Run & a1\n0.4792\n0.1510\n3.17\n0.0015\n\n\n\nTest: S20-Star & a1\n-0.0958\n0.1496\n-0.64\n0.5219\n\n\n\nTest: SLJ-S20 & a1\n-0.2303\n0.1495\n-1.54\n0.1234\n\n\n\nTest: BPT-SLJ & a1\n0.4668\n0.1488\n3.14\n0.0017\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1069\n0.0445\n-2.40\n0.0164\n\n\n\nTest: S20-Star & Sex: Boys\n0.0645\n0.0441\n1.46\n0.1441\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0579\n0.0435\n1.33\n0.1835\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0954\n0.0430\n2.22\n0.0267\n\n\n\na1 & Sex: Boys\n-0.0616\n0.0482\n-1.28\n0.2015\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0605\n0.1510\n0.40\n0.6887\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1386\n0.1496\n0.93\n0.3539\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3496\n0.1495\n-2.34\n0.0193\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.3519\n0.1488\n2.36\n0.0180\n\n\n\nResidual\n0.5490\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 3310      Time: 0:00:20 ( 6.30 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0149\n0.0141\n-1.06\n0.2894\n\n\n\nTest: Star-Run\n-0.0237\n0.0442\n-0.54\n0.5915\n\n\n\nTest: S20-Star\n-0.0073\n0.0448\n-0.16\n0.8702\n\n\n\nTest: SLJ-S20\n0.0322\n0.0444\n0.72\n0.4685\n\n\n\nTest: BPT-SLJ\n-0.0389\n0.0429\n-0.91\n0.3653\n\n\n\na1\n0.2465\n0.0481\n5.13\n&lt;1e-06\n\n\n\nSex: Boys\n0.2078\n0.0141\n14.73\n&lt;1e-48\n\n\n\nTest: Star-Run & a1\n0.4954\n0.1499\n3.31\n0.0009\n\n\n\nTest: S20-Star & a1\n-0.1040\n0.1518\n-0.69\n0.4931\n\n\n\nTest: SLJ-S20 & a1\n-0.2344\n0.1524\n-1.54\n0.1241\n\n\n\nTest: BPT-SLJ & a1\n0.4832\n0.1478\n3.27\n0.0011\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1063\n0.0442\n-2.40\n0.0162\n\n\n\nTest: S20-Star & Sex: Boys\n0.0613\n0.0448\n1.37\n0.1708\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0615\n0.0444\n1.38\n0.1662\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0970\n0.0429\n2.26\n0.0238\n\n\n\na1 & Sex: Boys\n-0.0610\n0.0481\n-1.27\n0.2043\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0579\n0.1499\n0.39\n0.6991\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1218\n0.1518\n0.80\n0.4221\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3295\n0.1524\n-2.16\n0.0306\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.3374\n0.1478\n2.28\n0.0224\n\n\n\nTest: BPT\n\n\n\n\n0.9308\n\n\nTest: SLJ\n\n\n\n\n0.9665\n\n\nTest: Star_r\n\n\n\n\n0.9837\n\n\nTest: Run\n\n\n\n\n0.9738\n\n\nTest: S20_r\n\n\n\n\n0.9791\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = let\n  f_cpx_0 = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, f_cpx_0, dat; contrasts=contr1b)\nend\n\nMinimizing 3310      Time: 0:00:21 ( 6.48 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0149\n0.0141\n-1.06\n0.2894\n\n\n\nTest: Star-Run\n-0.0237\n0.0442\n-0.54\n0.5915\n\n\n\nTest: S20-Star\n-0.0073\n0.0448\n-0.16\n0.8702\n\n\n\nTest: SLJ-S20\n0.0322\n0.0444\n0.72\n0.4685\n\n\n\nTest: BPT-SLJ\n-0.0389\n0.0429\n-0.91\n0.3653\n\n\n\na1\n0.2465\n0.0481\n5.13\n&lt;1e-06\n\n\n\nSex: Boys\n0.2078\n0.0141\n14.73\n&lt;1e-48\n\n\n\nTest: Star-Run & a1\n0.4954\n0.1499\n3.31\n0.0009\n\n\n\nTest: S20-Star & a1\n-0.1040\n0.1518\n-0.69\n0.4931\n\n\n\nTest: SLJ-S20 & a1\n-0.2344\n0.1524\n-1.54\n0.1241\n\n\n\nTest: BPT-SLJ & a1\n0.4832\n0.1478\n3.27\n0.0011\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1063\n0.0442\n-2.40\n0.0162\n\n\n\nTest: S20-Star & Sex: Boys\n0.0613\n0.0448\n1.37\n0.1708\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0615\n0.0444\n1.38\n0.1662\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0970\n0.0429\n2.26\n0.0238\n\n\n\na1 & Sex: Boys\n-0.0610\n0.0481\n-1.27\n0.2043\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0579\n0.1499\n0.39\n0.6991\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1218\n0.1518\n0.80\n0.4221\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3295\n0.1524\n-2.16\n0.0306\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.3374\n0.1478\n2.28\n0.0224\n\n\n\nTest: BPT\n\n\n\n\n0.9308\n\n\nTest: SLJ\n\n\n\n\n0.9665\n\n\nTest: Star_r\n\n\n\n\n0.9837\n\n\nTest: Run\n\n\n\n\n0.9738\n\n\nTest: S20_r\n\n\n\n\n0.9791\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.94830150\n0.97380773\n\n\n\n\n\n\n\nTest: Star_r\n0.96774071\n0.98373813\n+0.59\n\n\n\n\n\n\nTest: S20_r\n0.95859457\n0.97907843\n-0.07\n+0.48\n\n\n\n\n\nTest: SLJ\n0.93408384\n0.96648013\n+0.45\n+0.38\n+0.71\n\n\n\n\nTest: BPT\n0.86633442\n0.93077087\n+0.32\n+0.44\n+0.31\n+0.50\n\n\nResidual\n\n0.00000000\n0.00003484\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .     .\n Test: Star_r   0.59   1.0     .      .     .\n Test: S20_r   -0.07   0.48   1.0     .     .\n Test: SLJ      0.45   0.38   0.71   1.0    .\n Test: BPT      0.32   0.44   0.31   0.5   1.0\n\nNormalized cumulative variances:\n[0.5356, 0.763, 0.8908, 0.9998, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.37   0.69  -0.15  -0.36   0.48\n Test: Star_r  -0.48   0.24  -0.45   0.59  -0.41\n Test: S20_r   -0.42  -0.64  -0.29   0.08   0.57\n Test: SLJ     -0.52  -0.23   0.07  -0.64  -0.52\n Test: BPT     -0.43   0.05   0.83   0.34   0.11,)\n\n\n\nf_cpx_1 = @formula(\n  zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child)\n)\nm_cpx_1_SeqDiff =\nfit(MixedModel, f_cpx_1, dat; contrasts=contr1b)\n\nMinimizing 1917      Time: 0:00:12 ( 6.40 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0148\n0.0141\n-1.05\n0.2957\n0.7213\n\n\nTest: Star-Run\n-0.0265\n0.0440\n-0.60\n0.5468\n0.7508\n\n\nTest: S20-Star\n-0.0030\n0.0442\n-0.07\n0.9453\n0.6981\n\n\nTest: SLJ-S20\n0.0333\n0.0443\n0.75\n0.4514\n0.7384\n\n\nTest: BPT-SLJ\n-0.0373\n0.0430\n-0.87\n0.3861\n0.9282\n\n\na1\n0.2483\n0.0481\n5.16\n&lt;1e-06\n\n\n\nSex: Boys\n0.2075\n0.0141\n14.69\n&lt;1e-48\n\n\n\nTest: Star-Run & a1\n0.4755\n0.1492\n3.19\n0.0014\n\n\n\nTest: S20-Star & a1\n-0.0927\n0.1491\n-0.62\n0.5342\n\n\n\nTest: SLJ-S20 & a1\n-0.2262\n0.1516\n-1.49\n0.1357\n\n\n\nTest: BPT-SLJ & a1\n0.4814\n0.1480\n3.25\n0.0011\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1026\n0.0440\n-2.33\n0.0198\n\n\n\nTest: S20-Star & Sex: Boys\n0.0598\n0.0442\n1.35\n0.1760\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0643\n0.0443\n1.45\n0.1465\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0955\n0.0430\n2.22\n0.0263\n\n\n\na1 & Sex: Boys\n-0.0609\n0.0481\n-1.26\n0.2062\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0429\n0.1492\n0.29\n0.7736\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1430\n0.1491\n0.96\n0.3375\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.3429\n0.1516\n-2.26\n0.0237\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.3446\n0.1480\n2.33\n0.0199\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .      .     .\n Test: Star-Run   0.52   1.0     .      .     .\n Test: S20-Star  -0.11   0.67   1.0     .     .\n Test: SLJ-S20   -0.01  -0.6   -0.25   1.0    .\n Test: BPT-SLJ   -0.26   0.21   0.22  -0.5   1.0\n\nNormalized cumulative variances:\n[0.4554, 0.7398, 0.914, 0.9993, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4    PC5\n (Intercept)     -0.14   0.76  -0.33  -0.34  -0.43\n Test: Star-Run  -0.61   0.33   0.08  -0.0    0.72\n Test: S20-Star  -0.47  -0.06   0.75  -0.09  -0.45\n Test: SLJ-S20    0.52   0.17   0.44  -0.64   0.31\n Test: BPT-SLJ   -0.35  -0.54  -0.35  -0.68  -0.02,)"
  },
  {
    "objectID": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "href": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.4 PCA-based HypothesisCoding: contr4",
    "text": "2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principle components of the published LMM based on test scores, not test effects (contrasts) - coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 0 0 0 +1\n      -3 +2 +2 +2 -3\n      0 +2 -1 -1 0\n      0 0 +1 -1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = fit(MixedModel, f_cpx_1, dat; contrasts=contr4)\n\nMinimizing 1922      Time: 0:00:12 ( 6.42 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0151\n0.0141\n-1.07\n0.2860\n0.7079\n\n\nTest: c5.1\n-0.0372\n0.0432\n-0.86\n0.3885\n1.2585\n\n\nTest: c234.15\n0.0193\n0.1680\n0.11\n0.9087\n0.4572\n\n\nTest: c2.34\n-0.0067\n0.0770\n-0.09\n0.9307\n1.6258\n\n\nTest: c3.4\n-0.0289\n0.0448\n-0.65\n0.5185\n1.3738\n\n\na1\n0.2510\n0.0482\n5.21\n&lt;1e-06\n\n\n\nSex: Boys\n0.2082\n0.0141\n14.73\n&lt;1e-48\n\n\n\nTest: c5.1 & a1\n0.6298\n0.1465\n4.30\n&lt;1e-04\n\n\n\nTest: c234.15 & a1\n0.1670\n0.5723\n0.29\n0.7705\n\n\n\nTest: c2.34 & a1\n0.4414\n0.2615\n1.69\n0.0914\n\n\n\nTest: c3.4 & a1\n0.2489\n0.1537\n1.62\n0.1053\n\n\n\nTest: c5.1 & Sex: Boys\n0.1150\n0.0432\n2.66\n0.0077\n\n\n\nTest: c234.15 & Sex: Boys\n-0.6041\n0.1680\n-3.60\n0.0003\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1894\n0.0770\n-2.46\n0.0139\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0625\n0.0448\n-1.40\n0.1630\n\n\n\na1 & Sex: Boys\n-0.0634\n0.0482\n-1.32\n0.1881\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1723\n0.1465\n1.18\n0.2395\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.2561\n0.5723\n-0.45\n0.6545\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n0.1103\n0.2615\n0.42\n0.6731\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.3284\n0.1537\n2.14\n0.0326\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.50116627\n0.70793098\n\n\n\n\n\n\n\nTest: c5.1\n1.58388541\n1.25852509\n-0.01\n\n\n\n\n\n\nTest: c234.15\n0.20905401\n0.45722425\n-0.15\n+0.90\n\n\n\n\n\nTest: c2.34\n2.64336931\n1.62584418\n+0.26\n-0.10\n-0.51\n\n\n\n\nTest: c3.4\n1.88735563\n1.37381062\n+0.03\n+0.17\n+0.15\n+0.06\n\n\nResidual\n\n0.00000000\n0.00004837\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1     -0.01   1.0     .      .      .\n Test: c234.15  -0.15   0.9    1.0     .      .\n Test: c2.34     0.26  -0.1   -0.51   1.0     .\n Test: c3.4      0.03   0.17   0.15   0.06   1.0\n\nNormalized cumulative variances:\n[0.4297, 0.6775, 0.8598, 0.9999, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.18  -0.6    0.54   0.57  -0.03\n Test: c5.1      0.59  -0.34   0.21  -0.34   0.62\n Test: c234.15   0.67  -0.06   0.14  -0.01  -0.72\n Test: c2.34    -0.38  -0.53  -0.01  -0.69  -0.3\n Test: c3.4      0.15  -0.49  -0.81   0.29   0.02,)\n\n\nThere is a numerical interaction with a z-value &gt; 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger ‚Äúunfitness‚Äù gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related ‚Äúfitness‚Äù gains of girls than boys (i.e., in Run_r).\n\ncontr4b = merge(\n  Dict(nm =&gt; Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; HypothesisCoding(\n      [\n        0.49 -0.04 0.20 0.03 -0.85\n        0.70 -0.56 -0.21 -0.13 0.37\n        0.31 0.68 -0.56 -0.35 0.00\n        0.04 0.08 0.61 -0.78 0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    ),\n  ),\n);\n\n\nm_cpx_1_PC_2 = fit(MixedModel, f_cpx_1, dat; contrasts=contr4b)\n\nMinimizing 2698      Time: 0:00:18 ( 7.04 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0138\n0.0143\n-0.97\n0.3324\n0.7407\n\n\nTest: c5.1\n0.0245\n0.0305\n0.80\n0.4208\n0.9092\n\n\nTest: c234.15\n0.0084\n0.0311\n0.27\n0.7877\n0.5502\n\n\nTest: c12.34\n0.0029\n0.0314\n0.09\n0.9270\n0.8116\n\n\nTest: c3.4\n-0.0266\n0.0314\n-0.85\n0.3962\n0.4999\n\n\na1\n0.2362\n0.0487\n4.85\n&lt;1e-05\n\n\n\nSex: Boys\n0.2019\n0.0143\n14.14\n&lt;1e-44\n\n\n\nTest: c5.1 & a1\n-0.4240\n0.1035\n-4.10\n&lt;1e-04\n\n\n\nTest: c234.15 & a1\n-0.1930\n0.1056\n-1.83\n0.0675\n\n\n\nTest: c12.34 & a1\n0.0365\n0.1065\n0.34\n0.7317\n\n\n\nTest: c3.4 & a1\n0.2118\n0.1079\n1.96\n0.0496\n\n\n\nTest: c5.1 & Sex: Boys\n-0.1045\n0.0305\n-3.43\n0.0006\n\n\n\nTest: c234.15 & Sex: Boys\n0.1100\n0.0311\n3.54\n0.0004\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0535\n0.0314\n-1.70\n0.0884\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0324\n0.0314\n-1.03\n0.3016\n\n\n\na1 & Sex: Boys\n-0.0689\n0.0487\n-1.41\n0.1572\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.1121\n0.1035\n-1.08\n0.2788\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.0054\n0.1056\n-0.05\n0.9594\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0129\n0.1065\n-0.12\n0.9033\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.2546\n0.1079\n2.36\n0.0183\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.54870032\n0.74074308\n\n\n\n\n\n\n\nTest: c5.1\n0.82667150\n0.90921477\n+0.07\n\n\n\n\n\n\nTest: c234.15\n0.30276663\n0.55024234\n-0.50\n+0.34\n\n\n\n\n\nTest: c12.34\n0.65863108\n0.81156089\n-0.05\n+0.24\n+0.51\n\n\n\n\nTest: c3.4\n0.24989980\n0.49989979\n-0.13\n-0.44\n-0.59\n+0.05\n\n\nResidual\n\n0.00000000\n0.00001455\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1      0.07   1.0     .      .      .\n Test: c234.15  -0.5    0.34   1.0     .      .\n Test: c12.34   -0.05   0.24   0.51   1.0     .\n Test: c3.4     -0.13  -0.44  -0.59   0.05   1.0\n\nNormalized cumulative variances:\n[0.4248, 0.687, 0.8796, 0.9949, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.22  -0.7   -0.43   0.35   0.39\n Test: c5.1      0.44  -0.39  -0.18  -0.79   0.06\n Test: c234.15   0.64   0.23   0.14   0.26   0.67\n Test: c12.34    0.37   0.27  -0.78   0.22  -0.37\n Test: c3.4     -0.46   0.48  -0.4   -0.37   0.5,)\n\n\n\nf_zcp_1 = @formula(zScore ~ 1 + Test*a1*Sex + zerocorr(1 + Test | Child))\nm_zcp_1_PC_2 = fit(MixedModel, f_zcp_1, dat; contrasts=contr4b)\n\nMinimizing 628   Time: 0:00:02 ( 4.49 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0145\n0.0143\n-1.01\n0.3118\n0.7095\n\n\nTest: c5.1\n0.0209\n0.0303\n0.69\n0.4906\n0.5686\n\n\nTest: c234.15\n0.0088\n0.0314\n0.28\n0.7790\n0.8522\n\n\nTest: c12.34\n0.0003\n0.0314\n0.01\n0.9934\n0.6763\n\n\nTest: c3.4\n-0.0226\n0.0319\n-0.71\n0.4789\n0.8156\n\n\na1\n0.2373\n0.0488\n4.87\n&lt;1e-05\n\n\n\nSex: Boys\n0.2024\n0.0143\n14.16\n&lt;1e-44\n\n\n\nTest: c5.1 & a1\n-0.4234\n0.1032\n-4.10\n&lt;1e-04\n\n\n\nTest: c234.15 & a1\n-0.1986\n0.1065\n-1.86\n0.0622\n\n\n\nTest: c12.34 & a1\n0.0339\n0.1065\n0.32\n0.7504\n\n\n\nTest: c3.4 & a1\n0.2215\n0.1096\n2.02\n0.0432\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0964\n0.0303\n-3.18\n0.0015\n\n\n\nTest: c234.15 & Sex: Boys\n0.1101\n0.0314\n3.51\n0.0005\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0526\n0.0314\n-1.68\n0.0936\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0353\n0.0319\n-1.11\n0.2676\n\n\n\na1 & Sex: Boys\n-0.0688\n0.0488\n-1.41\n0.1582\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.1221\n0.1032\n-1.18\n0.2368\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.0007\n0.1065\n-0.01\n0.9950\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0138\n0.1065\n-0.13\n0.8971\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.2508\n0.1096\n2.29\n0.0220\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_zcp_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.50332408\n0.70945337\n\n\n\n\n\n\n\nTest: c5.1\n0.32327918\n0.56857645\n.\n\n\n\n\n\n\nTest: c234.15\n0.72619435\n0.85217038\n.\n.\n\n\n\n\n\nTest: c12.34\n0.45736595\n0.67628836\n.\n.\n.\n\n\n\n\nTest: c3.4\n0.66515879\n0.81557268\n.\n.\n.\n.\n\n\nResidual\n\n0.00000000\n0.00000373\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp_1_PC_2, m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13374\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13415\n-41\n10\nNaN"
  },
  {
    "objectID": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "href": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.1 Contrasts are re-parameterizations of the same model",
    "text": "3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13837.713608876566\n 13837.71360887659\n 13837.713608876602"
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.2 VCs and CPs depend on contrast coding",
    "text": "3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\nbegin\n  f_Child = @formula zScore ~\n    1 + Test * a1 * Sex + (1 + Test | Child)\n  m_Child_SDC = fit(MixedModel, f_Child, dat; contrasts=contr1)\n  m_Child_HeC = fit(MixedModel, f_Child, dat; contrasts=contr2)\n  m_Child_HyC = fit(MixedModel, f_Child, dat; contrasts=contr3)\n  m_Child_PCA = fit(MixedModel, f_Child, dat; contrasts=contr4)\nend\n\nMinimizing 1922      Time: 0:00:12 ( 6.47 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0151\n0.0141\n-1.07\n0.2860\n0.7079\n\n\nTest: c5.1\n-0.0372\n0.0432\n-0.86\n0.3885\n1.2585\n\n\nTest: c234.15\n0.0193\n0.1680\n0.11\n0.9087\n0.4572\n\n\nTest: c2.34\n-0.0067\n0.0770\n-0.09\n0.9307\n1.6258\n\n\nTest: c3.4\n-0.0289\n0.0448\n-0.65\n0.5185\n1.3738\n\n\na1\n0.2510\n0.0482\n5.21\n&lt;1e-06\n\n\n\nSex: Boys\n0.2082\n0.0141\n14.73\n&lt;1e-48\n\n\n\nTest: c5.1 & a1\n0.6298\n0.1465\n4.30\n&lt;1e-04\n\n\n\nTest: c234.15 & a1\n0.1670\n0.5723\n0.29\n0.7705\n\n\n\nTest: c2.34 & a1\n0.4414\n0.2615\n1.69\n0.0914\n\n\n\nTest: c3.4 & a1\n0.2489\n0.1537\n1.62\n0.1053\n\n\n\nTest: c5.1 & Sex: Boys\n0.1150\n0.0432\n2.66\n0.0077\n\n\n\nTest: c234.15 & Sex: Boys\n-0.6041\n0.1680\n-3.60\n0.0003\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1894\n0.0770\n-2.46\n0.0139\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0625\n0.0448\n-1.40\n0.1630\n\n\n\na1 & Sex: Boys\n-0.0634\n0.0482\n-1.32\n0.1881\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1723\n0.1465\n1.18\n0.2395\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.2561\n0.5723\n-0.45\n0.6545\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n0.1103\n0.2615\n0.42\n0.6731\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.3284\n0.1537\n2.14\n0.0326\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.58706403\n0.76620104\n\n\n\n\n\n\n\nTest: Star_r\n0.43756421\n0.66148636\n+0.29\n\n\n\n\n\n\nTest: S20_r\n0.29837071\n0.54623320\n+0.02\n+0.40\n\n\n\n\n\nTest: SLJ\n0.37701166\n0.61401275\n-0.10\n-0.79\n+0.22\n\n\n\n\nTest: BPT\n1.08386128\n1.04108659\n-0.28\n+0.17\n-0.14\n-0.36\n\n\nResidual\n\n0.00000000\n0.00002031\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.48326740\n0.69517437\n\n\n\n\n\n\n\nTest: Star_r\n0.17421904\n0.41739554\n+0.13\n\n\n\n\n\n\nTest: S20_r\n0.11921789\n0.34527943\n-0.03\n+0.38\n\n\n\n\n\nTest: SLJ\n0.03534182\n0.18799420\n+0.12\n-0.59\n+0.21\n\n\n\n\nTest: BPT\n0.03855727\n0.19636004\n-0.22\n+0.36\n+0.17\n+0.27\n\n\nResidual\n\n0.00000000\n0.00002335\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.49310561\n0.70221479\n\n\n\n\n\n\n\nTest: BPT-other\n1.52279747\n1.23401680\n+0.93\n\n\n\n\n\n\nTest: Star-End\n0.87853825\n0.93730371\n+0.03\n-0.17\n\n\n\n\n\nTest: S20-Star\n1.11096739\n1.05402438\n+0.06\n+0.35\n+0.01\n\n\n\n\nTest: SLJ-S20\n0.86156775\n0.92820674\n-0.17\n+0.06\n-0.93\n+0.15\n\n\nResidual\n\n0.00000000\n0.00001501\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.50116627\n0.70793098\n\n\n\n\n\n\n\nTest: c5.1\n1.58388541\n1.25852509\n-0.01\n\n\n\n\n\n\nTest: c234.15\n0.20905401\n0.45722425\n-0.15\n+0.90\n\n\n\n\n\nTest: c2.34\n2.64336931\n1.62584418\n+0.26\n-0.10\n-0.51\n\n\n\n\nTest: c3.4\n1.88735563\n1.37381062\n+0.03\n+0.17\n+0.15\n+0.06\n\n\nResidual\n\n0.00000000\n0.00004837\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nbegin\n  f_Child0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  m_Child_SDC0 = fit(MixedModel, f_Child0, dat; contrasts=contr1)\n  m_Child_HeC0 = fit(MixedModel, f_Child0, dat; contrasts=contr2)\n  m_Child_HyC0 = fit(MixedModel, f_Child0, dat; contrasts=contr3)\n  m_Child_PCA0 = fit(MixedModel, f_Child0, dat; contrasts=contr4)\nend\n\nMinimizing 789   Time: 0:00:03 ( 4.44 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0150\n0.0141\n-1.06\n0.2894\n0.7065\n\n\nTest: c5.1\n-0.0359\n0.0428\n-0.84\n0.4025\n1.2466\n\n\nTest: c234.15\n0.0157\n0.1678\n0.09\n0.9256\n0.7799\n\n\nTest: c2.34\n-0.0137\n0.0779\n-0.18\n0.8601\n2.0959\n\n\nTest: c3.4\n-0.0278\n0.0449\n-0.62\n0.5348\n1.1503\n\n\na1\n0.2508\n0.0481\n5.21\n&lt;1e-06\n\n\n\nSex: Boys\n0.2081\n0.0141\n14.73\n&lt;1e-48\n\n\n\nTest: c5.1 & a1\n0.6307\n0.1455\n4.34\n&lt;1e-04\n\n\n\nTest: c234.15 & a1\n0.1664\n0.5720\n0.29\n0.7711\n\n\n\nTest: c2.34 & a1\n0.4471\n0.2650\n1.69\n0.0915\n\n\n\nTest: c3.4 & a1\n0.2490\n0.1540\n1.62\n0.1059\n\n\n\nTest: c5.1 & Sex: Boys\n0.1164\n0.0428\n2.72\n0.0066\n\n\n\nTest: c234.15 & Sex: Boys\n-0.5988\n0.1678\n-3.57\n0.0004\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1882\n0.0779\n-2.42\n0.0157\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0595\n0.0449\n-1.33\n0.1848\n\n\n\na1 & Sex: Boys\n-0.0619\n0.0481\n-1.29\n0.1983\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1997\n0.1455\n1.37\n0.1698\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.2719\n0.5720\n-0.48\n0.6345\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n0.0878\n0.2650\n0.33\n0.7404\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.3360\n0.1540\n2.18\n0.0291\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13830\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13440\n390\n10\n&lt;1e-77\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13397\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13434\n-36\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13429\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13435\n-6\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13383\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13470\n-87\n10\nNaN\n\n\n\n\n\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nbegin\n  zcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\n  gof_summary = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM,\n      dof=dof.(mods),\n      deviance=deviance.(mods),\n      AIC=aic.(mods),\n      BIC=bic.(mods),\n    ),\n    :deviance,\n  )\nend\n\n4√ó5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nPCA0\n26\n13383.0\n13435.0\n13604.4\n\n\n2\nHeC0\n26\n13397.2\n13449.2\n13618.7\n\n\n3\nHyC0\n26\n13428.6\n13480.6\n13650.1\n\n\n4\nSDC0\n26\n13830.3\n13882.3\n14051.7\n\n\n\n\n\n\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM."
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.3 VCs and CPs depend on random factor",
    "text": "3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula zScore ~\n  1 + Test * a1 * Sex + (1 + Test + a1 | School);\nm_School_SeqDiff = fit(MixedModel, f_School, dat; contrasts=contr1);\nm_School_Helmert = fit(MixedModel, f_School, dat; contrasts=contr2);\nm_School_Hypo = fit(MixedModel, f_School, dat; contrasts=contr3);\nm_School_PCA = fit(MixedModel, f_School, dat; contrasts=contr4);\n\nMinimizing 587   Time: 0:00:00 ( 0.66 ms/it)\n  objective:  13790.097564610987\n\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.049545\n0.222586\n\n\n\n\n\n\n\n\nTest: Star_r\n0.165768\n0.407146\n-0.00\n\n\n\n\n\n\n\nTest: S20_r\n0.162184\n0.402721\n+0.06\n-0.82\n\n\n\n\n\n\nTest: SLJ\n0.041085\n0.202694\n-0.03\n+0.61\n-0.40\n\n\n\n\n\nTest: BPT\n0.180077\n0.424355\n-0.09\n-0.27\n-0.11\n-0.81\n\n\n\n\na1\n0.021409\n0.146318\n+0.01\n+0.68\n-0.14\n+0.60\n-0.65\n\n\nResidual\n\n0.843636\n0.918496\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0493980\n0.2222566\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0403715\n0.2009266\n-0.01\n\n\n\n\n\n\n\nTest: S20_r\n0.0066206\n0.0813669\n+0.10\n-0.58\n\n\n\n\n\n\nTest: SLJ\n0.0038584\n0.0621159\n+0.03\n+0.19\n+0.68\n\n\n\n\n\nTest: BPT\n0.0027634\n0.0525682\n-0.13\n-0.37\n-0.30\n-0.68\n\n\n\n\na1\n0.0187550\n0.1369489\n+0.01\n+0.68\n+0.19\n+0.84\n-0.61\n\n\nResidual\n\n0.8455138\n0.9195183\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.049548\n0.222593\n\n\n\n\n\n\n\n\nTest: BPT-other\n1.106945\n1.052115\n-0.13\n\n\n\n\n\n\n\nTest: Star-End\n0.165780\n0.407161\n-0.00\n-0.33\n\n\n\n\n\n\nTest: S20-Star\n0.162177\n0.402712\n+0.06\n+0.04\n-0.82\n\n\n\n\n\nTest: SLJ-S20\n0.041118\n0.202777\n-0.03\n-0.80\n+0.61\n-0.40\n\n\n\n\na1\n0.021410\n0.146321\n+0.01\n-0.55\n+0.68\n-0.14\n+0.60\n\n\nResidual\n\n0.843630\n0.918493\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0493482\n0.2221446\n\n\n\n\n\n\n\n\nTest: c5.1\n0.0005990\n0.0244735\n-1.00\n\n\n\n\n\n\n\nTest: c234.15\n2.8620321\n1.6917541\n+0.06\n-0.06\n\n\n\n\n\n\nTest: c2.34\n0.5437959\n0.7374252\n-0.06\n+0.06\n+0.02\n\n\n\n\n\nTest: c3.4\n0.0412790\n0.2031725\n+0.03\n-0.03\n-0.81\n-0.15\n\n\n\n\na1\n0.0172202\n0.1312258\n+0.01\n-0.01\n+0.97\n-0.07\n-0.67\n\n\nResidual\n\n0.8486443\n0.9212189\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nbegin\n  f_School0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School)\n  m_School_SDC0 = fit(MixedModel, f_School0, dat; contrasts=contr1)\n  m_School_HeC0 = fit(MixedModel, f_School0, dat; contrasts=contr2)\n  m_School_HyC0 = fit(MixedModel, f_School0, dat; contrasts=contr3)\n  m_School_PCA0 = fit(MixedModel, f_School0, dat; contrasts=contr4)\n  #\n  zcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods2 = [\n    m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n  ]\n  gof_summary2 = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM2,\n      dof=dof.(mods2),\n      deviance=deviance.(mods2),\n      AIC=aic.(mods2),\n      BIC=bic.(mods2),\n    ),\n    :deviance,\n  )\nend\n\nMinimizing 254   Time: 0:00:00 ( 0.51 ms/it)\n  objective:  13795.951105199356\n\n\n4√ó5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nPCA0\n27\n13796.0\n13850.0\n14025.9\n\n\n2\nHeC0\n27\n13801.5\n13855.5\n14031.5\n\n\n3\nHyC0\n27\n13806.1\n13860.1\n14036.1\n\n\n4\nSDC0\n27\n13812.4\n13866.4\n14042.4\n\n\n\n\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting."
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "",
    "text": "The speed of MixedModels.jl relative to its predecessors makes the parametric bootstrap much more computationally tractable. This is valuable because the parametric bootstrap can be used to produce more accurate confidence intervals than methods based on standard errors or profiling of the likelihood surface.\nThis page is adapted from the MixedModels.jl docs\n\n1 The parametric bootstrap\nBootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values. Bootstrapping also belongs to a larger family of procedures called resampling, which are based on creating new samples of data from an existing one, then computing statistics on the new samples, in order to examine the distribution of the relevant statistics.\nA parametric bootstrap is used with a parametric model, m, that has been fit to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.\nThe parameters of a LinearMixedModel object are the fixed-effects parameters, Œ≤, the standard deviation, œÉ, of the per-observation noise, and the covariance parameter, Œ∏, that defines the variance-covariance matrices of the random effects. A technical description of the covariance parameter can be found in the MixedModels.jl docs. Lisa Schwetlick and Daniel Backhaus have provided a more beginner-friendly description of the covariance parameter in the documentation for MixedModelsSim.jl. For today‚Äôs purposes ‚Äì looking at the uncertainty in the estimates from a fitted model ‚Äì we can simply use values from the fitted model, but we will revisit the parametric bootstrap as a convenient way to simulate new data, potentially with different parameter values, for power analysis.\nFor example, a simple linear mixed-effects model for the Dyestuff data in the lme4 package for R is fit by\n\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\nusing Random\n\nusing AlgebraOfGraphics: AlgebraOfGraphics as AoG\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\nProgressMeter.ijulia_behavior(:clear);\n\nNote that the precise stream of random numbers generated for a given seed can change between Julia versions. For exact reproducibility, you either need to have the exact same Julia version or use the StableRNGs package.\n\n\n2 A model of moderate complexity\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors.\n\ncontrasts = Dict(:spkr =&gt; EffectsCoding(),\n                 :prec =&gt; EffectsCoding(),\n                 :load =&gt; EffectsCoding(),\n                 :subj =&gt; Grouping(),\n                 :item =&gt; Grouping())\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :item =&gt; Grouping()\n  :spkr =&gt; EffectsCoding(nothing, nothing)\n  :load =&gt; EffectsCoding(nothing, nothing)\n  :prec =&gt; EffectsCoding(nothing, nothing)\n  :subj =&gt; Grouping()\n\n\nThe EffectsCoding contrast is used with these to create a ¬±1 encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\nWe can look at an initial fit of moderate complexity:\n\nform = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr + prec + load | subj) +\n                          (1 + spkr + prec + load | item))\nm0 = fit(MixedModel, form, kb07; contrasts)\n\nMinimizing 799   Time: 0:00:01 ( 1.91 ms/it)\n  objective:  28637.123623229592\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n\n\n(Intercept)\n2181.6729\n77.3136\n28.22\n&lt;1e-99\n301.8062\n362.2579\n\n\nspkr: old\n67.7491\n18.2664\n3.71\n0.0002\n42.3795\n40.6807\n\n\nprec: maintain\n-333.9205\n47.1558\n-7.08\n&lt;1e-11\n61.9630\n246.9158\n\n\nload: yes\n78.7702\n19.5298\n4.03\n&lt;1e-04\n64.9751\n42.3890\n\n\nspkr: old & prec: maintain\n-21.9655\n15.8074\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3837\n15.8074\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5333\n15.8074\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6073\n15.8074\n1.49\n0.1353\n\n\n\n\nResidual\n668.5542\n\n\n\n\n\n\n\n\n\n\nThe default display in Quarto uses the pretty MIME show method for the model and omits the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(m0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91087.0055\n301.8062\n\n\n\n\n\n\nspkr: old\n1796.0221\n42.3795\n+0.79\n\n\n\n\n\nprec: maintain\n3839.4126\n61.9630\n-0.59\n+0.02\n\n\n\n\nload: yes\n4221.7638\n64.9751\n+0.36\n+0.85\n+0.54\n\n\nitem\n(Intercept)\n131230.7914\n362.2579\n\n\n\n\n\n\nspkr: old\n1654.9232\n40.6807\n+0.44\n\n\n\n\n\nprec: maintain\n60967.4037\n246.9158\n-0.69\n+0.35\n\n\n\n\nload: yes\n1796.8284\n42.3890\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446964.7062\n668.5542\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\n# formula f4 from https://doi.org/10.33016/nextjournal.100002\nform = @formula(rt_trunc ~ 1 + spkr * prec * load + (1 | subj) + (1 + prec | item))\n\nm1 = fit(MixedModel, form, kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n&lt;1e-99\n364.7286\n298.1109\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n&lt;1e-11\n252.6687\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nspkr: old & prec: maintain\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.918\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.080\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(m0, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 | subj) + (1 + prec | item)\n13\n28658\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n21\n16\n0.1650\n\n\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, m1, to the more complex, m0.\n\n\n3 Bootstrap basics\nTo bootstrap the model parameters, first initialize a random number generator then create a bootstrap sample\n\nconst RNG = MersenneTwister(42)\nsamp = parametricbootstrap(RNG, 1_000, m1)\ndf = DataFrame(samp.allpars)\nfirst(df, 10)\n\n10√ó5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n2049.88\n\n\n2\n1\nŒ≤\nmissing\nspkr: old\n71.6398\n\n\n3\n1\nŒ≤\nmissing\nprec: maintain\n-268.333\n\n\n4\n1\nŒ≤\nmissing\nload: yes\n75.1566\n\n\n5\n1\nŒ≤\nmissing\nspkr: old & prec: maintain\n-17.8459\n\n\n6\n1\nŒ≤\nmissing\nspkr: old & load: yes\n-1.90968\n\n\n7\n1\nŒ≤\nmissing\nprec: maintain & load: yes\n13.1018\n\n\n8\n1\nŒ≤\nmissing\nspkr: old & prec: maintain & load: yes\n37.19\n\n\n9\n1\nœÉ\nitem\n(Intercept)\n363.568\n\n\n10\n1\nœÉ\nitem\nprec: maintain\n199.692\n\n\n\n\n\n\nEspecially for those with a background in R or pandas, the simplest way of accessing the parameter estimates in the parametric bootstrap object is to create a DataFrame from the allpars property as shown above.\nWe can use subset to subset out relevant rows of a dataframe. A density plot of the estimates of œÉ, the residual standard deviation, can be created as\n\nœÉres = subset(df, :type =&gt; ByRow(==(\"œÉ\")), :group =&gt; ByRow(==(\"residual\")); skipmissing=true)\n\nplt = data(œÉres) * mapping(:value) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ\"))\n\n\n\n\nA density plot of the estimates of the standard deviation of the random effects is obtained as\n\nœÉsubjitem = subset(df, :type =&gt; ByRow(==(\"œÉ\")), :group =&gt; ByRow(!=(\"residual\")); skipmissing=true)\n\nplt = data(œÉsubjitem) * mapping(:value; layout=:names, color=:group) * AoG.density()\ndraw(plt; figure=(;supertitle=\"Parametric bootstrap estimates of variance components\"))\n\n\n\n\nThe bootstrap sample can be used to generate intervals that cover a certain percentage of the bootstrapped values. We refer to these as ‚Äúcoverage intervals‚Äù, similar to a confidence interval. The shortest such intervals, obtained with the shortestcovint extractor, correspond to a highest posterior density interval in Bayesian inference.\nWe generate these for all random and fixed effects:\n\nshortestcovint(samp)\n\n13-element Vector{NamedTuple{(:type, :group, :names, :lower, :upper)}}:\n (type = \"Œ≤\", group = missing, names = \"(Intercept)\", lower = 2028.4504678827352, upper = 2335.4980383723514)\n (type = \"Œ≤\", group = missing, names = \"spkr: old\", lower = 33.9058405165279, upper = 97.1017123381832)\n (type = \"Œ≤\", group = missing, names = \"prec: maintain\", lower = -420.9759732561327, upper = -246.92630850502417)\n (type = \"Œ≤\", group = missing, names = \"load: yes\", lower = 49.93973144391102, upper = 108.72371421954583)\n (type = \"Œ≤\", group = missing, names = \"spkr: old & prec: maintain\", lower = -52.84340404834443, upper = 8.707634088469343)\n (type = \"Œ≤\", group = missing, names = \"spkr: old & load: yes\", lower = -11.990358741200508, upper = 49.090842992861646)\n (type = \"Œ≤\", group = missing, names = \"prec: maintain & load: yes\", lower = -25.20769992600972, upper = 36.98654052402985)\n (type = \"Œ≤\", group = missing, names = \"spkr: old & prec: maintain & load: yes\", lower = -5.88516757104554, upper = 54.77034471490767)\n (type = \"œÉ\", group = \"item\", names = \"(Intercept)\", lower = 267.2821922034617, upper = 462.82035004178374)\n (type = \"œÉ\", group = \"item\", names = \"prec: maintain\", lower = 178.14890728818813, upper = 316.22872563303554)\n (type = \"œÅ\", group = \"item\", names = \"(Intercept), prec: maintain\", lower = -0.9058573809671474, upper = -0.4814074554466317)\n (type = \"œÉ\", group = \"subj\", names = \"(Intercept)\", lower = 231.87600905769338, upper = 356.1555898742369)\n (type = \"œÉ\", group = \"residual\", names = missing, lower = 653.5649119908876, upper = 700.7662323651775)\n\n\nand convert it to a dataframe:\n\nDataFrame(shortestcovint(samp))\n\n13√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2028.45\n2335.5\n\n\n2\nŒ≤\nmissing\nspkr: old\n33.9058\n97.1017\n\n\n3\nŒ≤\nmissing\nprec: maintain\n-420.976\n-246.926\n\n\n4\nŒ≤\nmissing\nload: yes\n49.9397\n108.724\n\n\n5\nŒ≤\nmissing\nspkr: old & prec: maintain\n-52.8434\n8.70763\n\n\n6\nŒ≤\nmissing\nspkr: old & load: yes\n-11.9904\n49.0908\n\n\n7\nŒ≤\nmissing\nprec: maintain & load: yes\n-25.2077\n36.9865\n\n\n8\nŒ≤\nmissing\nspkr: old & prec: maintain & load: yes\n-5.88517\n54.7703\n\n\n9\nœÉ\nitem\n(Intercept)\n267.282\n462.82\n\n\n10\nœÉ\nitem\nprec: maintain\n178.149\n316.229\n\n\n11\nœÅ\nitem\n(Intercept), prec: maintain\n-0.905857\n-0.481407\n\n\n12\nœÉ\nsubj\n(Intercept)\n231.876\n356.156\n\n\n13\nœÉ\nresidual\nmissing\n653.565\n700.766\n\n\n\n\n\n\n\ndraw(\n  data(samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\nFor the fixed effects, MixedModelsMakie provides a convenience interface to plot the combined coverage intervals and density plots\n\nridgeplot(samp)\n\n\n\n\nOften the intercept will be on a different scale and potentially less interesting, so we can stop it from being included in the plot:\n\nridgeplot(samp; show_intercept=false, xlabel=\"Bootstrap density and 95%CI\")\n\n\n\n\n\n\n4 Singularity\nLet‚Äôs consider the classic dysetuff dataset:\n\ndyestuff = MixedModels.dataset(:dyestuff)\nmdye = fit(MixedModel, @formula(yield ~ 1 + (1 | batch)), dyestuff)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_batch\n\n\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n&lt;1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\n\nsampdye = parametricbootstrap(MersenneTwister(1234321), 10_000, mdye)\ndfdye = DataFrame(sampdye.allpars)\nfirst(dfdye, 10)\n\n‚îå Warning: NLopt was roundoff limited\n‚îî @ MixedModels ~/.julia/packages/MixedModels/1XFIm/src/optsummary.jl:180\n‚îå Warning: NLopt was roundoff limited\n‚îî @ MixedModels ~/.julia/packages/MixedModels/1XFIm/src/optsummary.jl:180\n‚îå Warning: NLopt was roundoff limited\n‚îî @ MixedModels ~/.julia/packages/MixedModels/1XFIm/src/optsummary.jl:180\n‚îå Warning: NLopt was roundoff limited\n‚îî @ MixedModels ~/.julia/packages/MixedModels/1XFIm/src/optsummary.jl:180\n‚îå Warning: NLopt was roundoff limited\n‚îî @ MixedModels ~/.julia/packages/MixedModels/1XFIm/src/optsummary.jl:180\n\n\n10√ó5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n1509.13\n\n\n2\n1\nœÉ\nbatch\n(Intercept)\n14.312\n\n\n3\n1\nœÉ\nresidual\nmissing\n67.4315\n\n\n4\n2\nŒ≤\nmissing\n(Intercept)\n1538.08\n\n\n5\n2\nœÉ\nbatch\n(Intercept)\n25.5673\n\n\n6\n2\nœÉ\nresidual\nmissing\n47.9831\n\n\n7\n3\nŒ≤\nmissing\n(Intercept)\n1508.02\n\n\n8\n3\nœÉ\nbatch\n(Intercept)\n21.7622\n\n\n9\n3\nœÉ\nresidual\nmissing\n50.1346\n\n\n10\n4\nŒ≤\nmissing\n(Intercept)\n1538.47\n\n\n\n\n\n\n\nœÉbatch = subset(dfdye, :type =&gt; ByRow(==(\"œÉ\")), :group =&gt; ByRow(==(\"batch\")); skipmissing=true)\n\nplt = data(œÉbatch) * mapping(:value) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ_batch\"))\n\n\n\n\nNotice that this density plot has a spike, or mode, at zero. Although this mode appears to be diffuse, this is an artifact of the way that density plots are created. In fact, it is a pulse, as can be seen from a histogram.\n\nplt = data(œÉbatch) * mapping(:value) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ_batch\"))\n\n\n\n\nA value of zero for the standard deviation of the random effects is an example of a singular covariance. It is easy to detect the singularity in the case of a scalar random-effects term. However, it is not as straightforward to detect singularity in vector-valued random-effects terms.\nFor example, if we bootstrap a model fit to the sleepstudy data\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\nmsleep = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)),\n             sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\nsampsleep = parametricbootstrap(MersenneTwister(666), 10_000, msleep);\ndfsleep = DataFrame(sampsleep.allpars);\nfirst(dfsleep, 10)\n\n10√ó5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n252.488\n\n\n2\n1\nŒ≤\nmissing\ndays\n11.0328\n\n\n3\n1\nœÉ\nsubj\n(Intercept)\n29.6185\n\n\n4\n1\nœÉ\nsubj\ndays\n6.33343\n\n\n5\n1\nœÅ\nsubj\n(Intercept), days\n0.233383\n\n\n6\n1\nœÉ\nresidual\nmissing\n22.4544\n\n\n7\n2\nŒ≤\nmissing\n(Intercept)\n260.763\n\n\n8\n2\nŒ≤\nmissing\ndays\n8.55352\n\n\n9\n2\nœÉ\nsubj\n(Intercept)\n20.8063\n\n\n10\n2\nœÉ\nsubj\ndays\n4.32895\n\n\n\n\n\n\nthe singularity can be exhibited as a standard deviation of zero or as a correlation of ¬±1.\n\nDataFrame(shortestcovint(sampsleep))\n\n6√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n237.905\n264.231\n\n\n2\nŒ≤\nmissing\ndays\n7.44545\n13.3516\n\n\n3\nœÉ\nsubj\n(Intercept)\n10.6756\n33.3567\n\n\n4\nœÉ\nsubj\ndays\n3.07053\n7.82204\n\n\n5\nœÅ\nsubj\n(Intercept), days\n-0.411665\n1.0\n\n\n6\nœÉ\nresidual\nmissing\n22.6345\n28.5125\n\n\n\n\n\n\nA histogram of the estimated correlations from the bootstrap sample has a spike at +1.\n\nœÅs = subset(dfsleep, :type =&gt; ByRow(==(\"œÅ\")), :group =&gt; ByRow(==(\"subj\")); skipmissing=true)\nplt = data(œÅs) * mapping(:value) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap samples of correlation of random effects\"))\n\n\n\n\nor, as a count,\n\ncount(œÅs.value .‚âà 1)\n\n293\n\n\nClose examination of the histogram shows a few values of -1.\n\ncount(œÅs.value .‚âà -1)\n\n2\n\n\nFurthermore there are even a few cases where the estimate of the standard deviation of the random effect for the intercept is zero.\n\nœÉs = subset(dfsleep, :type =&gt; ByRow(==(\"œÉ\")), :group =&gt; ByRow(==(\"subj\")), :names =&gt; ByRow(==(\"(Intercept)\")); skipmissing=true)\ncount(œÉs.value .‚âà 0)\n\n7\n\n\nThere is a general condition to check for singularity of an estimated covariance matrix or matrices in a bootstrap sample. The parameter optimized in the estimation is Œ∏, the relative covariance parameter. Some of the elements of this parameter vector must be non-negative and, when one of these components is approximately zero, one of the covariance matrices will be singular.\nThe issingular method for a MixedModel object that tests if a parameter vector Œ∏ corresponds to a boundary or singular fit.\nThis operation is encapsulated in a method for the issingular function that works on MixedModelBootstrap objects.\n\ncount(issingular(sampsleep))\n\n302\n\n\n\n\n5 References\n\n\nKronm√ºller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436‚Äì455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AoGPlots.html",
    "href": "AoGPlots.html",
    "title": "Creating multi-panel plots",
    "section": "",
    "text": "This notebook shows creating a multi-panel plot similar to Figure 2 of F√ºhner et al. (2021).\nThe data are available from the SMLP2023 example datasets.\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie   # for displaying static plots\nusing DataFrames\nusing Statistics\nusing StatsBase\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\n\n\n\ntbl = dataset(\"fggk21\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ntypeof(tbl)\n\nArrow.Table\n\n\n\ndf = DataFrame(tbl)\ntypeof(df)\n\nDataFrame\n\n\n\n1 Creating a summary data frame\nThe response to be plotted is the mean score by Test and Sex and age, rounded to the nearest 0.1 years.\nThe first task is to round the age to 1 digit after the decimal place, which can be done with select applied to a DataFrame. In some ways this is the most complicated expression in creating the plot so we will break it down. select is applied to DataFrame(dat), which is the conversion of the Arrow.Table, dat, to a DataFrame. This is necessary because an Arrow.Table is immutable but a DataFrame can be modified.\nThe arguments after the DataFrame describe how to modify the contents. The first : indicates that all the existing columns should be included. The other expression can be pairs (created with the =&gt; operator) of the form :col =&gt; function or of the form :col =&gt; function =&gt; :newname. (See the documentation of the DataFrames package for details.)\nIn this case the function is an anonymous function of the form round.(x, digits=1) where ‚Äúdot-broadcasting‚Äù is used to apply to the entire column (see this documentation for details).\n\ntransform!(df, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\nselect!(groupby(df, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\ntlabels = [     # establish order and labels of tbl.Test\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n];\n\nThe next stage is a group-apply-combine operation to group the rows by Sex, Test and rnd_age then apply mean to the zScore and also apply length to zScore to record the number in each group.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nString\nString\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nmale\nS20_r\n8.0\n-0.0265138\n1223\n\n\n2\nmale\nBPT\n8.0\n0.026973\n1227\n\n\n3\nmale\nSLJ\n8.0\n0.121609\n1227\n\n\n4\nmale\nStar_r\n8.0\n-0.0571726\n1186\n\n\n5\nmale\nRun\n8.0\n0.292695\n1210\n\n\n6\nfemale\nS20_r\n8.0\n-0.35164\n1411\n\n\n7\nfemale\nBPT\n8.0\n-0.610355\n1417\n\n\n8\nfemale\nSLJ\n8.0\n-0.279872\n1418\n\n\n9\nfemale\nStar_r\n8.0\n-0.268221\n1381\n\n\n10\nfemale\nRun\n8.0\n-0.245573\n1387\n\n\n11\nmale\nS20_r\n8.1\n0.0608397\n3042\n\n\n12\nmale\nBPT\n8.1\n0.0955413\n3069\n\n\n13\nmale\nSLJ\n8.1\n0.123099\n3069\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nmale\nStar_r\n9.0\n0.254973\n4049\n\n\n110\nmale\nRun\n9.0\n0.258082\n4034\n\n\n111\nfemale\nS20_r\n9.1\n-0.0286172\n1154\n\n\n112\nfemale\nBPT\n9.1\n-0.0752301\n1186\n\n\n113\nfemale\nSLJ\n9.1\n-0.094587\n1174\n\n\n114\nfemale\nStar_r\n9.1\n0.00276252\n1162\n\n\n115\nfemale\nRun\n9.1\n-0.235591\n1150\n\n\n116\nmale\nS20_r\n9.1\n0.325745\n1303\n\n\n117\nmale\nBPT\n9.1\n0.616416\n1320\n\n\n118\nmale\nSLJ\n9.1\n0.267577\n1310\n\n\n119\nmale\nStar_r\n9.1\n0.254342\n1297\n\n\n120\nmale\nRun\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n\n2 Creating the plot\nThe AlgebraOfGraphics package applies operators to the results of functions such as data (specify the data table to be used), mapping (designate the roles of columns), and visual (type of visual presentation).\n\nlet\n  design = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines = design * linear()\n  means = design * visual(Scatter; markersize=5)\n  draw(data(df2) * means + data(df) * lines)\nend\n\n\n\n\n\nTBD: Relabel factor levels (Boys, Girls; fitness components for Test)\nTBD: Relevel factors; why not levels from Tables?\nTBD: Set range (7.8 to 9.2 and tick marks (8, 8.5, 9) of axes.\nTBD: Move legend in plot?\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "arrow.html",
    "href": "arrow.html",
    "title": "Saving data with Arrow",
    "section": "",
    "text": "1 The Arrow storage format\nThe Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means ‚Äúsomething that looks like a data frame in R‚Äù. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather.\nInternally, the SMLP2023 package uses Arrow to store all of its datasets.\n\n\n2 The Emotikon data\nThe SMLP2021 repository contains a version of the data from F√ºhner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing RCall   # access R from within Julia\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density\n\n\n\n\n3 Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\n\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\n\ndfrm = rcopy(R\"readRDS($fn)\")\n\n525126√ó7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nCat‚Ä¶\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3077850\n\n\n\ndf = DataFrame(tbl)\n\n525126√ó7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nString\nString\nString\nString\nFloat64\nString\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\n\n\n4 Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design.\n\n\n5 Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n108295√ó5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1774946\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794058\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n525126√ó3 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\n\n\n\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n\n\n2\nC002352\nBPT\n3.7\n\n\n3\nC002352\nSLJ\n125.0\n\n\n4\nC002352\nStar_r\n2.47146\n\n\n5\nC002352\nRun\n1053.0\n\n\n6\nC002353\nS20_r\n5.0\n\n\n7\nC002353\nBPT\n4.1\n\n\n8\nC002353\nSLJ\n116.0\n\n\n9\nC002353\nStar_r\n1.76778\n\n\n10\nC002353\nRun\n1089.0\n\n\n11\nC002354\nS20_r\n4.54545\n\n\n12\nC002354\nBPT\n3.9\n\n\n13\nC002354\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n\n\n525116\nC117964\nRun\n864.0\n\n\n525117\nC117965\nS20_r\n4.65116\n\n\n525118\nC117965\nBPT\n3.8\n\n\n525119\nC117965\nSLJ\n123.0\n\n\n525120\nC117965\nStar_r\n1.52889\n\n\n525121\nC117965\nRun\n1080.0\n\n\n525122\nC117966\nS20_r\n4.54545\n\n\n525123\nC117966\nBPT\n3.8\n\n\n525124\nC117966\nSLJ\n100.0\n\n\n525125\nC117966\nStar_r\n2.18506\n\n\n525126\nC117966\nRun\n990.0\n\n\n\n\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius‚Äôs presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @bycol zscore(:score))\n\n525126√ó4 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\n\n\n\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\n\n\n5\nC002352\nRun\n1053.0\n0.331058\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\n\n\n7\nC002353\nBPT\n4.1\n0.498354\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\n\n\n10\nC002353\nRun\n1089.0\n0.574056\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\n\n\n12\nC002354\nBPT\n3.9\n0.218061\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\n\n\n\n\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n108295√ó5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n525126√ó8 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nFloat64\nFloat64\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\nS100067\n2013\nmale\n7.99452\n\n\n5\nC002352\nRun\n1053.0\n0.331058\nS100067\n2013\nmale\n7.99452\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\nS100067\n2013\nmale\n7.99452\n\n\n7\nC002353\nBPT\n4.1\n0.498354\nS100067\n2013\nmale\n7.99452\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\nS100067\n2013\nmale\n7.99452\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\nS100067\n2013\nmale\n7.99452\n\n\n10\nC002353\nRun\n1089.0\n0.574056\nS100067\n2013\nmale\n7.99452\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\nS100067\n2013\nmale\n7.99452\n\n\n12\nC002354\nBPT\n3.9\n0.218061\nS100067\n2013\nmale\n7.99452\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\nS100067\n2013\nmale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\nS401470\n2018\nmale\n9.10609\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\nS401470\n2018\nmale\n9.10609\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\nS401470\n2018\nfemale\n9.10609\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\nS401470\n2018\nfemale\n9.10609\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\nS401470\n2018\nfemale\n9.10609\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\nS401470\n2018\nfemale\n9.10609\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\nS401470\n2018\nfemale\n9.10609\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\nS800200\n2018\nmale\n9.10609\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\nS800200\n2018\nmale\n9.10609\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\nS800200\n2018\nmale\n9.10609\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\nS800200\n2018\nmale\n9.10609\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created.\n\n\n\n\n6 Discovering patterns in the data\nOne of the motivations for creating the Child table was be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow =&gt; :ntest)\n\n108295√ó2 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nntest\n\n\n\nString\nInt64\n\n\n\n\n1\nC002352\n5\n\n\n2\nC002353\n5\n\n\n3\nC002354\n5\n\n\n4\nC002355\n5\n\n\n5\nC002356\n5\n\n\n6\nC002357\n5\n\n\n7\nC002358\n5\n\n\n8\nC002359\n4\n\n\n9\nC002360\n5\n\n\n10\nC002361\n4\n\n\n11\nC002362\n5\n\n\n12\nC002363\n5\n\n\n13\nC002364\n5\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\n5\n\n\n108285\nC117944\n5\n\n\n108286\nC117945\n5\n\n\n108287\nC117946\n5\n\n\n108288\nC117956\n5\n\n\n108289\nC117957\n4\n\n\n108290\nC117958\n5\n\n\n108291\nC117959\n5\n\n\n108292\nC117962\n5\n\n\n108293\nC117964\n5\n\n\n108294\nC117965\n5\n\n\n108295\nC117966\n5\n\n\n\n\n\n\nNow create a table of the number of children with 1, 2, ‚Ä¶, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n5√ó2 DataFrame\n\n\n\nRow\nntest\nnrow\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n462\n\n\n2\n2\n729\n\n\n3\n3\n1739\n\n\n4\n4\n8836\n\n\n5\n5\n96529\n\n\n\n\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1437 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nString\nString\nString\nString\nFloat64\nInt64\n\n\n\n\n1\nC002452\nS101175\n2013\nmale\n7.99452\n1\n\n\n2\nC002625\nS103329\n2013\nmale\n7.99452\n1\n\n\n3\nC002754\nS104814\n2013\nfemale\n7.99452\n1\n\n\n4\nC003269\nS102258\n2012\nfemale\n7.99726\n1\n\n\n5\nC003599\nS105843\n2012\nfemale\n7.99726\n1\n\n\n6\nC003807\nS100754\n2011\nmale\n8.0\n1\n\n\n7\nC003985\nS102945\n2011\nmale\n8.0\n1\n\n\n8\nC004086\nS104255\n2011\nmale\n8.0\n1\n\n\n9\nC004657\nS101400\n2014\nmale\n8.03833\n1\n\n\n10\nC005036\nS105909\n2014\nmale\n8.03833\n1\n\n\n11\nC005440\nS101023\n2019\nmale\n8.05202\n1\n\n\n12\nC005523\nS101825\n2019\nfemale\n8.05202\n1\n\n\n13\nC005697\nS103615\n2019\nmale\n8.05202\n1\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n451\nC112638\nS103718\n2015\nfemale\n9.0486\n1\n\n\n452\nC114749\nS112938\n2017\nmale\n9.06502\n1\n\n\n453\nC115460\nS101953\n2015\nmale\n9.08145\n1\n\n\n454\nC115569\nS100572\n2017\nmale\n9.08419\n1\n\n\n455\nC115587\nS100754\n2017\nfemale\n9.08419\n1\n\n\n456\nC117108\nS103196\n2018\nfemale\n9.10609\n1\n\n\n457\nC117229\nS103615\n2018\nmale\n9.10609\n1\n\n\n458\nC117230\nS103615\n2018\nmale\n9.10609\n1\n\n\n459\nC117419\nS104954\n2018\nfemale\n9.10609\n1\n\n\n460\nC117437\nS105004\n2018\nmale\n9.10609\n1\n\n\n461\nC117539\nS105636\n2018\nmale\n9.10609\n1\n\n\n462\nC117659\nS106483\n2018\nfemale\n9.10609\n1\n\n\n\n‚ãÆ\n\n\nLast Group (96529 rows): ntest = 5\n\n\n96504 rows omitted\n\n\n\n\n\n\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nString\nString\nString\nString\nFloat64\nInt64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n5\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n5\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n5\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n5\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n5\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n5\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n5\n\n\n8\nC002360\nS100195\n2013\nfemale\n7.99452\n5\n\n\n9\nC002362\nS100237\n2013\nfemale\n7.99452\n5\n\n\n10\nC002363\nS100237\n2013\nfemale\n7.99452\n5\n\n\n11\nC002364\nS100250\n2013\nfemale\n7.99452\n5\n\n\n12\nC002365\nS100304\n2013\nmale\n7.99452\n5\n\n\n13\nC002366\nS100304\n2013\nmale\n7.99452\n5\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n96518\nC117942\nS130539\n2018\nmale\n9.10609\n5\n\n\n96519\nC117943\nS130539\n2018\nfemale\n9.10609\n5\n\n\n96520\nC117944\nS130539\n2018\nmale\n9.10609\n5\n\n\n96521\nC117945\nS130539\n2018\nmale\n9.10609\n5\n\n\n96522\nC117946\nS130539\n2018\nmale\n9.10609\n5\n\n\n96523\nC117956\nS400580\n2018\nmale\n9.10609\n5\n\n\n96524\nC117958\nS400919\n2018\nmale\n9.10609\n5\n\n\n96525\nC117959\nS400919\n2018\nmale\n9.10609\n5\n\n\n96526\nC117962\nS401250\n2018\nfemale\n9.10609\n5\n\n\n96527\nC117964\nS401470\n2018\nmale\n9.10609\n5\n\n\n96528\nC117965\nS401470\n2018\nfemale\n9.10609\n5\n\n\n96529\nC117966\nS800200\n2018\nmale\n9.10609\n5\n\n\n\n\n\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow =&gt; :nchild)\n\n2√ó2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nString\nInt64\n\n\n\n\n1\nmale\n230\n\n\n2\nfemale\n232\n\n\n\n\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow =&gt; :nchild)\n\n2√ó2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nString\nInt64\n\n\n\n\n1\nmale\n47552\n\n\n2\nfemale\n48977\n\n\n\n\n\n\nWhat about the distribution of ages?\n\n\"\"\"\n    ridgeplot!(ax::Axis, df::AbstractDataFrame, densvar::Symbol, group::Symbol; normalize=false)\n    ridgeplot!(f::Figure, args...; pos=(1,1) kwargs...)\n    ridgeplot(args...; kwargs...)\nCreate a \"ridge plot\".\nA ridge plot is stacked plot of densities for a given variable (`densvar`) grouped by a different variable (`group`). Because densities can very widely in scale, it is sometimes useful to `normalize` the densities so that each density has a maximum of 1.\nThe non-mutating method creates a Figure before calling the method for Figure.\nThe method for Figure places the ridge plot in the grid position specified by `pos`, default is (1,1).\n\"\"\"\nfunction ridgeplot!(\n  ax::Axis,\n  df::AbstractDataFrame,\n  densvar::Symbol,\n  group::Symbol;\n  normalize=false,\n)\n  # `normalize` makes it so that the max density is always 1\n  # `normalize` works on the density not the area/mass\n  gdf = groupby(df, group)\n  dens = combine(gdf, densvar =&gt; kde =&gt; :kde)\n  sort!(dens, group)\n  spacing = normalize ? 1.0 : 0.9 * maximum(dens[!, :kde]) do val\n    return maximum(val.density)\n  end\n\n  nticks = length(gdf)\n\n  for (idx, row) in enumerate(eachrow(dens))\n    dd = if normalize\n      row.kde.density ./ maximum(row.kde.density)\n    else\n      row.kde.density\n    end\n\n    offset = idx * spacing\n\n    lower = Observable(Point2f.(row.kde.x, offset))\n    upper = Observable(Point2f.(row.kde.x, dd .+ offset))\n    band!(ax, lower, upper; color=(:black, 0.3))\n    lines!(ax, upper; color=(:black, 1.0))\n  end\n\n  ax.yticks[] = (\n    1:spacing:(nticks * spacing), string.(dens[!, group])\n  )\n  ylims!(ax, 0, (nticks + 2) * spacing)\n  ax.xlabel[] = string(densvar)\n  ax.ylabel[] = string(group)\n\n  return ax\nend\n\n\nfunction ridgeplot!(f::Figure, args...; pos=(1, 1), kwargs...)\n  ridgeplot!(Axis(f[pos...]), args...; kwargs...)\n  return f\nend\n\n\n\"\"\"\n    ridgeplot(args...; kwargs...)\nSee [ridgeplot!](@ref).\n\"\"\"\nfunction ridgeplot(args...; kwargs...)\n  return ridgeplot!(Figure(), args...; kwargs...)\nend\n\n\nridgeplot(parent(gdf), :age, :ntest)\n\n\nparent(gdf)\n\n108295√ó6 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nString\nString\nString\nString\nFloat64\nInt64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n5\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n5\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n5\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n5\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n5\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n5\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n5\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n4\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n5\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n4\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n5\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n5\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n5\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n5\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n5\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n5\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n5\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n5\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n4\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n5\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n5\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n5\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n5\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n5\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n5\n\n\n\n\n\n\n\n\n7 Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow).\n#| eval: false\nimport pyarrow.feather: read_table\nread_table(\"./data/fggk21.arrow\")\n#| eval: false\nlibrary(\"arrow\")\nfggk21 &lt;- read_feather(\"./data/fggk21.arrow\")\nnrow(fggk21)\n\n\n8 References\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "check_emotikon_transform.html",
    "href": "check_emotikon_transform.html",
    "title": "Transformed and original metrics in Emotikon",
    "section": "",
    "text": "In F√ºhner et al. (2021) the original metric of two tasks (Star, S20) is time, but they were transformed to speed scores in the publication prior to computing z-scores. The critical result is the absence of evidence for the age x Sex x Test interaction. Is this interaction significant if we analyse all tasks in their original metric?\nFitting the LMM of the publication takes time, roughly 1 hour. However, if you save the model parameters (and other relevant information), you can restore the fitted model object very quickly. The notebook also illustrates this procedure."
  },
  {
    "objectID": "check_emotikon_transform.html#getting-the-packages-and-data",
    "href": "check_emotikon_transform.html#getting-the-packages-and-data",
    "title": "Transformed and original metrics in Emotikon",
    "section": "1 Getting the packages and data",
    "text": "1 Getting the packages and data\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nusing RCall\nusing Serialization\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\ndatadir = joinpath(@__DIR__, \"data\");\n\n\n\n1.1 Data and figure in publication\n\ndat = DataFrame(Arrow.Table(joinpath(datadir, \"fggk21.arrow\")))\n@transform!(dat, :a1 = :age - 8.5);\nselect!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore);\ndescribe(dat)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n2\nCohort\n\n2011\n\n2019\n0\nString\n\n\n3\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n4\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n5\nSex\n\nfemale\n\nmale\n0\nString\n\n\n6\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n9\nzScore\n-3.91914e-13\n-3.1542\n0.00031088\n3.55078\n0\nFloat64\n\n\n\n\n\n\n\n\n1.2 Data and figure with z-scores based on original metric\n\n# dat_om = rcopy(R\"readRDS('./data/fggk21_om.rds')\");  #Don't know what the _om is\n# @transform!(dat_om, :a1 = :age - 8.5);\n# select!(groupby(dat_om, :Test), :, :score =&gt; zscore =&gt; :zScore);\n# describe(dat_om)"
  },
  {
    "objectID": "check_emotikon_transform.html#lmms",
    "href": "check_emotikon_transform.html#lmms",
    "title": "Transformed and original metrics in Emotikon",
    "section": "2 LMMs",
    "text": "2 LMMs\n\n2.1 Contrasts\n\ncontrasts = Dict(\n  :Test =&gt; SeqDiffCoding(),\n  :Sex =&gt; HelmertCoding(),\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n);\n\n\n\n2.2 Formula\n\nf1 = @formula zScore ~\n  1 +\n  Test * a1 * Sex +\n  (1 + Test + a1 + Sex | School) +\n  (1 + Test | Child) +\n  zerocorr(1 + Test | Cohort);\n\n\n\n2.3 Restore LMM m1 from publication\n\nCommand for fitting LMM m1 = fit(MixedModel, f1, dat, contrasts=contr)\nFit statistics for LMM m1: Minimizing 5179 Time: 0 Time: 1:00:38 ( 0.70 s/it)\n\n\nm1x = LinearMixedModel(f1, dat; contrasts)\nrestoreoptsum!(m1x, \"./fits/fggk21_m1_optsum.json\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0383\n0.0108\n-3.56\n0.0004\n0.5939\n0.2024\n0.0157\n\n\nTest: Run\n-0.0228\n0.0274\n-0.83\n0.4052\n0.8384\n0.3588\n0.0651\n\n\nTest: S20_r\n-0.0147\n0.0405\n-0.36\n0.7171\n0.5825\n0.3596\n0.1107\n\n\nTest: SLJ\n0.0328\n0.0330\n0.99\n0.3198\n0.4127\n0.3027\n0.0896\n\n\nTest: Star_r\n0.0006\n0.0197\n0.03\n0.9763\n0.5574\n0.3620\n0.0313\n\n\na1\n0.2713\n0.0086\n31.63\n&lt;1e-99\n\n0.0966\n\n\n\nSex: male\n0.2064\n0.0024\n86.55\n&lt;1e-99\n\n0.0245\n\n\n\nTest: Run & a1\n-0.4464\n0.0131\n-34.05\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & a1\n0.1473\n0.0114\n12.97\n&lt;1e-37\n\n\n\n\n\nTest: SLJ & a1\n-0.0068\n0.0103\n-0.66\n0.5116\n\n\n\n\n\nTest: Star_r & a1\n0.0761\n0.0111\n6.84\n&lt;1e-11\n\n\n\n\n\nTest: Run & Sex: male\n-0.0900\n0.0037\n-24.10\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & Sex: male\n-0.0912\n0.0032\n-28.23\n&lt;1e-99\n\n\n\n\n\nTest: SLJ & Sex: male\n0.0330\n0.0029\n11.24\n&lt;1e-28\n\n\n\n\n\nTest: Star_r & Sex: male\n-0.0720\n0.0032\n-22.65\n&lt;1e-99\n\n\n\n\n\na1 & Sex: male\n0.0010\n0.0069\n0.14\n0.8876\n\n\n\n\n\nTest: Run & a1 & Sex: male\n-0.0154\n0.0126\n-1.22\n0.2233\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n0.0129\n0.0109\n1.18\n0.2380\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0098\n0.0100\n-0.98\n0.3256\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.0166\n0.0108\n1.54\n0.1241\n\n\n\n\n\nResidual\n0.5880\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3527294\n0.5939103\n\n\n\n\n\n\n\n\n\nTest: Run\n0.7029003\n0.8383915\n+0.11\n\n\n\n\n\n\n\n\nTest: S20_r\n0.3393356\n0.5825252\n+0.19\n-0.53\n\n\n\n\n\n\n\nTest: SLJ\n0.1702900\n0.4126621\n+0.05\n-0.14\n-0.29\n\n\n\n\n\n\nTest: Star_r\n0.3107227\n0.5574251\n-0.10\n+0.01\n-0.13\n-0.42\n\n\n\n\nSchool\n(Intercept)\n0.0409640\n0.2023957\n\n\n\n\n\n\n\n\n\nTest: Run\n0.1287690\n0.3588440\n+0.26\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1293351\n0.3596319\n+0.01\n-0.57\n\n\n\n\n\n\n\nTest: SLJ\n0.0916522\n0.3027411\n-0.13\n+0.01\n-0.53\n\n\n\n\n\n\nTest: Star_r\n0.1310575\n0.3620187\n+0.26\n+0.09\n-0.06\n-0.28\n\n\n\n\n\na1\n0.0093412\n0.0966499\n+0.48\n+0.25\n-0.15\n-0.01\n+0.12\n\n\n\n\nSex: male\n0.0005999\n0.0244934\n+0.09\n+0.13\n-0.01\n+0.05\n-0.19\n+0.25\n\n\nCohort\n(Intercept)\n0.0002452\n0.0156587\n\n\n\n\n\n\n\n\n\nTest: Run\n0.0042389\n0.0651068\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0122535\n0.1106954\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0080210\n0.0895599\n.\n.\n.\n\n\n\n\n\n\nTest: Star_r\n0.0009828\n0.0313498\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.3456872\n0.5879517\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Restore new LMM m1_om Star and S20 in original metric\n\nCommand for fitting LMM m1_om = fit(MixedModel, f1, dat_om, contrasts=contr)\nMinimizing 10502 Time: 0 Time: 2:09:40 ( 0.74 s/it)\nStore with: julia&gt; saveoptsum(‚Äú./fits/fggk21_m1_om_optsum.json‚Äù, m1_om)\nOnly for short-term and when desparate: julia&gt; serialize(‚Äú./fits/m1_om.jls‚Äù, m1_om);\n\n\n2.4.1 ‚Ä¶ restoreoptsum!()\n\nm1_om = LinearMixedModel(f1, dat; contrasts=contr);\nrestoreoptsum!(m1_om, \"./fits/fggk21_m1_om_optsum.json\");\n\n\n\n2.4.2 ‚Ä¶ deserialize()\n\nm1x_om = deserialize(\"./fits/m1_om.jls\")\n\n\nVarCorr(m1x_om)\n\n\n\n\n2.5 Residual diagnostics for LMM m1\nResidual plots for published LMM\n\n#scatter(fitted(m1x), residuals(m1x)\n\n\n#qqnorm(m1x)\n\n\n\n2.6 Residual diagnostics for LMM m1_om\nResidual plots for LMM with Star and Speed in original metric.\n\n#scatter(fitted(m1_om_v2), residuals(m1_om_v2)\n\n\n#qqnorm(m1_om_v2)\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4"
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Code\nusing Arrow\nusing Chain\nusing DataFrames\nusing MixedModels\nusing ProgressMeter\nusing SMLP2023: dataset\nusing StatsBase\nusing StatsModels\nusing StatsModels: ContrastsCoding\n\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.1 SeqDiffCoding",
    "text": "4.1 SeqDiffCoding\nThe SeqDiffCoding contrast corresponds to MASS::contr.sdif() in R. The assignment of random factors such as Subj to Grouping() is necessary when the sample size is very large. We recommend to include it always, but in this tutorial we do so only in the first example.\n\nform = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\nlevels = [\"val\", \"sod\", \"dos\", \"dod\"]\nm1 = let\n  contrasts = Dict(\n    :CTR =&gt; SeqDiffCoding(; levels),\n    :Subj =&gt; Grouping()\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\nMinimizing 206   Time: 0:00:00 ( 1.63 ms/it)\n  objective:  325809.5493500654\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0918\n54.96\n&lt;1e-99\n55.2039\n\n\nCTR: sod\n33.7817\n3.2875\n10.28\n&lt;1e-24\n23.2497\n\n\nCTR: dos\n13.9852\n2.3056\n6.07\n&lt;1e-08\n10.7517\n\n\nCTR: dod\n-2.7470\n2.2142\n-1.24\n0.2147\n9.5089\n\n\nResidual\n69.8348"
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.2 HypothesisCoding",
    "text": "4.2 HypothesisCoding\nHypothesisCoding is the most general option available. We can implement all ‚Äúcanned‚Äù contrasts ourselves. The next example reproduces the test statistcs from SeqDiffCoding - with a minor modification illustrating the flexibility of going beyond the default version.\n\nm1b = let\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1  1 0  0\n         0 -1 1  0\n         0  0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n&lt;1e-99\n55.2002\n\n\nCTR: spt\n33.7817\n3.2873\n10.28\n&lt;1e-24\n23.2477\n\n\nCTR: obj\n13.9852\n2.3059\n6.07\n&lt;1e-08\n10.7559\n\n\nCTR: grv\n2.7469\n2.2144\n1.24\n0.2148\n9.5118\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe difference to the preprogrammed SeqDiffCoding is that for the third contrast we changed the direction of the contrast such that the sign of the effect is positive when the result is in agreement with theoretical expectation, that is we subtract the fourth level from the third, not the third level from the fourth."
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.3 DummyCoding",
    "text": "4.3 DummyCoding\nThi contrast corresponds to contr.treatment() in R\n\nm2 = let\n  contrasts = Dict(:CTR =&gt; DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n358.0914\n6.1535\n58.19\n&lt;1e-99\n47.9065\n\n\nCTR: dod\n45.0200\n4.3634\n10.32\n&lt;1e-24\n32.2899\n\n\nCTR: dos\n47.7669\n3.5564\n13.43\n&lt;1e-40\n25.5354\n\n\nCTR: sod\n33.7817\n3.2873\n10.28\n&lt;1e-24\n23.2479\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe DummyCoding contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM."
  },
  {
    "objectID": "contrasts_kwdyz11.html#ychycaeitcoding",
    "href": "contrasts_kwdyz11.html#ychycaeitcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.4 YchycaeitCoding",
    "text": "4.4 YchycaeitCoding\nThe contrasts returned by DummyCoding may be exactly what we want. Can‚Äôt we have them, but also have the intercept estimate the GM, rather than the mean of the base level? Yes, we can! We call this ‚ÄúYou can have your cake and it eat, too‚Äù-Coding (YchycaeitCoding). And we use HpothesisCoding to achieve this outcome.\n\nm2b = let\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0876\n54.99\n&lt;1e-99\n55.1708\n\n\nCTR: sod\n33.7818\n3.2874\n10.28\n&lt;1e-24\n23.2485\n\n\nCTR: dos\n47.7669\n3.5567\n13.43\n&lt;1e-40\n25.5374\n\n\nCTR: dod\n45.0200\n4.3642\n10.32\n&lt;1e-24\n32.2966\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nWe can simply relevel the factor or move the column with -1s for a different base."
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding",
    "href": "contrasts_kwdyz11.html#effectscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.5 EffectsCoding",
    "text": "4.5 EffectsCoding\nThis contrast corresponds almost to contr.sum() in R.\n\nm3 = let\n  contrasts = Dict(:CTR =&gt; EffectsCoding(; base=\"dod\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0919\n54.95\n&lt;1e-99\n55.2042\n\n\nCTR: dos\n16.1248\n1.4404\n11.19\n&lt;1e-28\n7.3315\n\n\nCTR: sod\n2.1396\n1.3339\n1.60\n0.1087\n6.0087\n\n\nCTR: val\n-31.6422\n2.6421\n-11.98\n&lt;1e-32\n19.9488\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe ‚Äúalmost‚Äù qualification refers to the fact that contr.sum() uses the last factor levels as defaul base level; EffectsCoding uses the first level."
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.6 HelmertCoding",
    "text": "4.6 HelmertCoding\nHelmertCoding codes each level as the difference from the average of the lower levels. With the default order of CTR levels we get the following test statistics. These contrasts are othogonal.\n\nm4 = let\n  contrasts = Dict(:CTR =&gt; HelmertCoding())\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0919\n54.95\n&lt;1e-99\n55.2044\n\n\nCTR: dos\n1.3735\n1.1071\n1.24\n0.2148\n4.7550\n\n\nCTR: sod\n-4.2039\n0.6843\n-6.14\n&lt;1e-09\n3.3491\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n&lt;1e-32\n6.6502\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1: (2 - 1)/2           # (391 - 358)/2\n+ HeC2: (3 - (2+1)/2)/3     # (405 - (391 + 358)/2)/3\n+ HeC3: (4 - (3+2+1)/3)/4   # (402 - (405 + 391 + 358)/3)/4"
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.7 Reverse HelmertCoding",
    "text": "4.7 Reverse HelmertCoding\nReverse HelmertCoding codes each level as the difference from the average of the higher levels. To estimate these effects we simply reverse the order of factor levels. Of course, the contrasts are also orthogonal.\n\nm4b = let\n  levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR =&gt; HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0919\n54.95\n&lt;1e-99\n55.2044\n\n\nCTR: dos\n1.3735\n1.1071\n1.24\n0.2148\n4.7550\n\n\nCTR: sod\n-4.2039\n0.6843\n-6.14\n&lt;1e-09\n3.3491\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n&lt;1e-32\n6.6502\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1:(3 - 4)/2            # (405 - 402)/2\n+ HeC2:(2 - (3+4)/2)/3      # (391 - (405 + 402)/2)/3\n+ HeC3:(1 - (2+3+4)/3/4     # (356  -(391 + 405 + 402)/3)/4"
  },
  {
    "objectID": "contrasts_kwdyz11.html#anova-coding",
    "href": "contrasts_kwdyz11.html#anova-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.8 Anova Coding",
    "text": "4.8 Anova Coding\nFactorial designs (i.e., lab experiments) are traditionally analyzed with analysis of variance. The test statistics of main effects and interactions are based on an orthogonal set of contrasts. We specify them with HypothesisCoding.\n\n4.8.1 A(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n&lt;1e-99\n55.2001\n\n\nCTR: A\n59.0052\n5.1825\n11.39\n&lt;1e-29\n36.2073\n\n\nCTR: B\n31.0348\n4.6749\n6.64\n&lt;1e-10\n31.7124\n\n\nCTR: AxB\n-36.5287\n3.0928\n-11.81\n&lt;1e-31\n16.0057\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\n4.8.2 A(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1\n\n\n4.8.3 A(2) x B(2) x C(3)\nTO BE DONE"
  },
  {
    "objectID": "contrasts_kwdyz11.html#nested-coding",
    "href": "contrasts_kwdyz11.html#nested-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.9 Nested coding",
    "text": "4.9 Nested coding\nNested contrasts are often specified as follow up as post-hoc tests for ANOVA interactions. They are orthogonal. We specify them with HypothesisCoding.\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\nm8 = let\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1  0  0\n         0  0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0912\n54.96\n&lt;1e-99\n55.1986\n\n\nCTR: do_so\n59.0052\n5.1825\n11.39\n&lt;1e-29\n36.2067\n\n\nCTR: spt\n33.7817\n3.2874\n10.28\n&lt;1e-24\n23.2487\n\n\nCTR: grv\n2.7469\n2.2145\n1.24\n0.2148\n9.5133\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#standard-contrasts",
    "href": "contrasts_kwdyz11.html#standard-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.1 Standard contrasts",
    "text": "6.1 Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "href": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.2 ‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)",
    "text": "6.2 ‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)\nWe also provide HelmertCoding and SeqDiffCoding (corresponding to base R‚Äôs contr.helmert() and MASS::contr.sdif())."
  },
  {
    "objectID": "contrasts_kwdyz11.html#manual-contrasts",
    "href": "contrasts_kwdyz11.html#manual-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.3 Manual contrasts",
    "text": "6.3 Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it‚Äôs good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they‚Äôll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information)."
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "1 Matrix notation for the sleepstudy model",
    "text": "1 Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n180√ó3 DataFrame155 rows omitted\n\n\n\nRow\nsubj\ndays\nreaction\n\n\n\nString\nInt8\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n\n\n2\nS308\n1\n258.705\n\n\n3\nS308\n2\n250.801\n\n\n4\nS308\n3\n321.44\n\n\n5\nS308\n4\n356.852\n\n\n6\nS308\n5\n414.69\n\n\n7\nS308\n6\n382.204\n\n\n8\nS308\n7\n290.149\n\n\n9\nS308\n8\n430.585\n\n\n10\nS308\n9\n466.353\n\n\n11\nS309\n0\n222.734\n\n\n12\nS309\n1\n205.266\n\n\n13\nS309\n2\n202.978\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n169\nS371\n8\n350.781\n\n\n170\nS371\n9\n369.469\n\n\n171\nS372\n0\n269.412\n\n\n172\nS372\n1\n273.474\n\n\n173\nS372\n2\n297.597\n\n\n174\nS372\n3\n310.632\n\n\n175\nS372\n4\n287.173\n\n\n176\nS372\n5\n329.608\n\n\n177\nS372\n6\n334.482\n\n\n178\nS372\n7\n343.22\n\n\n179\nS372\n8\n369.142\n\n\n180\nS372\n9\n364.124\n\n\n\n\n\n\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nMinimizing 57    Time: 0:00:00 ( 3.83 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51066 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)  251.405      6.63226  37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, Œ≤, has 2 elements and the fixed-effects model matrix, X, is of size 180 √ó 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ‚ãÆ\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.Œ≤\n\n2-element Vector{Float64}:\n 251.40510484848417\n  10.467285959595896\n\n\n\nm1.X\n\n180√ó2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ‚ãÆ    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1‚Äôs.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The ‚Äúestimates‚Äù (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.815818033394916 -40.04844099287436 ‚Ä¶ 0.7232621645096935 12.118907745826883; 9.075511916305423 -8.644079548078082 ‚Ä¶ -0.9710526581529085 1.3106980682714398]\n\n\n\nfirst(m1.b)   # only one grouping factor\n\n2√ó18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   ‚Ä¶  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180√ó36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ‚ãÆ              ‚ãÆ              ‚ãÆ        ‚ã±     ‚ãÆ              ‚ãÆ              ‚ãÆ\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.Œ≤ + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.2209228818791\n 273.7637207577804\n 293.3065186336817\n 312.84931650958305\n 332.3921143854843\n 351.9349122613857\n 371.477710137287\n 391.0205080131883\n 410.56330588908963\n 430.10610376499096\n 211.3566638556098\n 213.17987026712763\n 215.0030766786454\n   ‚ãÆ\n 328.09823342453774\n 337.5944667259808\n 263.52401259431105\n 275.3019966221784\n 287.0799806500457\n 298.85796467791306\n 310.6359487057804\n 322.41393273364775\n 334.1919167615151\n 345.96990078938245\n 357.7478848172497\n 369.5258688451171\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.2209228818791\n 273.7637207577804\n 293.3065186336817\n 312.84931650958305\n 332.3921143854843\n 351.9349122613857\n 371.477710137287\n 391.02050801318836\n 410.56330588908963\n 430.10610376499096\n 211.3566638556098\n 213.17987026712763\n 215.0030766786454\n   ‚ãÆ\n 328.0982334245378\n 337.5944667259808\n 263.52401259431105\n 275.3019966221784\n 287.07998065004574\n 298.85796467791306\n 310.6359487057804\n 322.41393273364775\n 334.1919167615151\n 345.96990078938245\n 357.74788481724977\n 369.5258688451171\n\n\nIn symbols we would write the linear predictor expression as \\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\] where \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 √ó 2 and \\(\\bf Z\\) is of size 180 √ó 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 √ó 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 √ó 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.Œ∏\n\n3-element Vector{Float64}:\n 0.9292213081613828\n 0.01816836498823806\n 0.22264488151102485\n\n\n\nŒª = only(m1.Œª)  # with multiple grouping factors there will be multiple Œª's\n\n2√ó2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ‚ãÖ \n 0.0181684  0.222645\n\n\n\nŒ£ = varest(m1) * (Œª * Œª')\n\n2√ó2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51066\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "2 Linear predictors in LMMs and GLMMs",
    "text": "2 Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as \\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\] may seem like over-mathematization (or ‚Äúoverkill‚Äù, if you prefer) relative to expressions like \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\] but this more abstract form is necessary for generalizations.\nThe way that I read the first form is\n\n\n\n\n\n\nThe conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\n\n\n\nSo the only things that differ in the distributions of the \\(y_i\\)‚Äôs are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "3 Generalized Linear Mixed Models",
    "text": "3 Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e.¬†yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i‚Äôth response is again determined by the i‚Äôth element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of ‚Äúsuccess‚Äù for the i‚Äôth response and must be between 0 and 1. We can‚Äôt guarantee that the i‚Äôth component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren‚Äôt as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function, \\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\] (it‚Äôs called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic \\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\] This is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(Œ∑) = inv(increment(exp(-Œ∑)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don‚Äôt have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet ‚ÄúBeauty is truth, truth beauty - that is all ye know on earth and all ye need to know.‚Äù)\n\n3.1 An example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n1934√ó5 DataFrame1909 rows omitted\n\n\n\nRow\ndist\nurban\nlivch\nage\nuse\n\n\n\nString\nString\nString\nFloat64\nString\n\n\n\n\n1\nD01\nY\n3+\n18.44\nN\n\n\n2\nD01\nY\n0\n-5.56\nN\n\n\n3\nD01\nY\n2\n1.44\nN\n\n\n4\nD01\nY\n3+\n8.44\nN\n\n\n5\nD01\nY\n0\n-13.56\nN\n\n\n6\nD01\nY\n0\n-11.56\nN\n\n\n7\nD01\nY\n3+\n18.44\nN\n\n\n8\nD01\nY\n3+\n-3.56\nN\n\n\n9\nD01\nY\n1\n-5.56\nN\n\n\n10\nD01\nY\n3+\n1.44\nN\n\n\n11\nD01\nY\n0\n-11.56\nY\n\n\n12\nD01\nY\n0\n-2.56\nN\n\n\n13\nD01\nY\n1\n-4.56\nN\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n1923\nD61\nN\n0\n-11.56\nY\n\n\n1924\nD61\nN\n3+\n1.44\nN\n\n\n1925\nD61\nN\n1\n-5.56\nN\n\n\n1926\nD61\nN\n3+\n14.44\nN\n\n\n1927\nD61\nN\n3+\n19.44\nN\n\n\n1928\nD61\nN\n2\n-9.56\nY\n\n\n1929\nD61\nN\n2\n-2.56\nN\n\n\n1930\nD61\nN\n3+\n14.44\nN\n\n\n1931\nD61\nN\n2\n-4.56\nN\n\n\n1932\nD61\nN\n3+\n14.44\nN\n\n\n1933\nD61\nN\n0\n-13.56\nN\n\n\n1934\nD61\nN\n3+\n10.44\nN\n\n\n\n\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n60√ó2 DataFrame35 rows omitted\n\n\n\nRow\ndist\nnrow\n\n\n\nString\nInt64\n\n\n\n\n1\nD01\n117\n\n\n2\nD02\n20\n\n\n3\nD03\n2\n\n\n4\nD04\n30\n\n\n5\nD05\n39\n\n\n6\nD06\n65\n\n\n7\nD07\n18\n\n\n8\nD08\n37\n\n\n9\nD09\n23\n\n\n10\nD10\n13\n\n\n11\nD11\n21\n\n\n12\nD12\n29\n\n\n13\nD13\n24\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n49\nD49\n4\n\n\n50\nD50\n19\n\n\n51\nD51\n37\n\n\n52\nD52\n61\n\n\n53\nD53\n19\n\n\n54\nD55\n6\n\n\n55\nD56\n45\n\n\n56\nD57\n27\n\n\n57\nD58\n33\n\n\n58\nD59\n10\n\n\n59\nD60\n32\n\n\n60\nD61\n42\n\n\n\n\n\n\nThe information recorded included woman‚Äôs age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, drawn using lattice graphics in R, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20‚Äôs to early 30‚Äôs) and low again for older women (late 30‚Äôs to early 40‚Äôs in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn‚Äôt mean that there is no ‚Äúage effect‚Äù, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :numuse =&gt; \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist =&gt; Grouping(),\n  :urban =&gt; HelmertCoding(),\n  :livch =&gt; DummyCoding(), # default, but no harm in being explicit\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 199   Time: 0:00:00 ( 2.28 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n&lt;1e-04\n0.4787\n\n\nage\n0.0035\n0.0092\n0.38\n0.7023\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n&lt;1e-09\n\n\n\nurban: Y\n0.3483\n0.0600\n5.81\n&lt;1e-08\n\n\n\nlivch: 1\n0.8152\n0.1622\n5.02\n&lt;1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n&lt;1e-06\n\n\n\nlivch: 3+\n0.9154\n0.1858\n4.93\n&lt;1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229113\n0.478657\n\n\n\n\n\nNotice that there is no ‚Äúresidual‚Äù variance being estimated. This is because the Bernoulli distribution doesn‚Äôt have a scale parameter.\n\n\n3.2 Convert livch to a binary factor\n\n@transform!(contra, :children = :livch ‚â† \"0\")\n# add the associated contrast specifier\ncontrasts[:children] = EffectsCoding()\n\nEffectsCoding(nothing, nothing)\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 144   Time: 0:00:00 ( 1.39 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.84\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2√ó6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n2\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\n3.3 Using urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 140   Time: 0:00:00 ( 1.16 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm3\n7\n2353.82\n2368.48\n2407.45\n2368.54\n\n\n2\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n3\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.84\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age¬≤ (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\n3.4 Predictions for some subgroups\nFor a ‚Äútypical‚Äù district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e.¬†centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children =&gt; [true, false], :urban =&gt; [\"Y\", \"N\"], :age =&gt; [0.0]\n)\npreds = effects(design, gm3)\n\n4√ó7 DataFrame\n\n\n\nRow\nchildren\nage\nurban\nuse: Y\nerr\nlower\nupper\n\n\n\nBool\nFloat64\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ntrue\n0.0\nY\n0.658576\n0.150529\n0.508047\n0.809105\n\n\n2\nfalse\n0.0\nY\n-0.554337\n0.230477\n-0.784814\n-0.323861\n\n\n3\ntrue\n0.0\nN\n-0.128612\n0.113016\n-0.241628\n-0.0155953\n\n\n4\nfalse\n0.0\nN\n-1.34152\n0.221575\n-1.5631\n-1.11995\n\n\n\n\n\n\nConverting these Œ∑ values to probabilities yields\n\nlogistic.(preds[!, \"use: Y\"])\n\n4-element Vector{Float64}:\n 0.6589404203024364\n 0.364858723221093\n 0.46789132742910544\n 0.2072594086310126"
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "4 Summarizing the results",
    "text": "4 Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing ProgressMeter\nusing Random\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\n\n\nProvide a short alias for AlgebraOfGraphics.\n\nconst AOG = AlgebraOfGraphics;\n\n\n1 Data set and model\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\n\ncontrasts = merge(\n  Dict(nm =&gt; EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm =&gt; Grouping() for nm in (:subj, :item)),\n);\n\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 799   Time: 0:00:01 ( 1.72 ms/it)\n  objective:  28637.123623229592\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n\n\n(Intercept)\n2181.6729\n77.3136\n28.22\n&lt;1e-99\n301.8062\n362.2579\n\n\nspkr: old\n67.7491\n18.2664\n3.71\n0.0002\n42.3795\n40.6807\n\n\nprec: maintain\n-333.9205\n47.1558\n-7.08\n&lt;1e-11\n61.9630\n246.9158\n\n\nload: yes\n78.7702\n19.5298\n4.03\n&lt;1e-04\n64.9751\n42.3890\n\n\nspkr: old & prec: maintain\n-21.9655\n15.8074\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3837\n15.8074\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5333\n15.8074\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6073\n15.8074\n1.49\n0.1353\n\n\n\n\nResidual\n668.5542\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91087.0055\n301.8062\n\n\n\n\n\n\nspkr: old\n1796.0221\n42.3795\n+0.79\n\n\n\n\n\nprec: maintain\n3839.4126\n61.9630\n-0.59\n+0.02\n\n\n\n\nload: yes\n4221.7638\n64.9751\n+0.36\n+0.85\n+0.54\n\n\nitem\n(Intercept)\n131230.7914\n362.2579\n\n\n\n\n\n\nspkr: old\n1654.9232\n40.6807\n+0.44\n\n\n\n\n\nprec: maintain\n60967.4037\n246.9158\n-0.69\n+0.35\n\n\n\n\nload: yes\n1796.8284\n42.3890\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446964.7062\n668.5542\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7125\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.242\n364.713\n\n\n\n\nprec: maintain\n63766.937\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.438\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n27\n20\n0.1431\n\n\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, kbm02, to the more complex, kbm01.\n\n\n2 A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\n\nRandom.seed!(1234321)\nhide_progress = true\nkbm02samp = parametricbootstrap(2000, kbm02; hide_progress);\n\nOne of the uses of such a sample is to form ‚Äúconfidence intervals‚Äù on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nDataFrame(shortestcovint(kbm02samp))\n\n9√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2028.01\n2337.92\n\n\n2\nŒ≤\nmissing\nspkr: old\n38.431\n99.5944\n\n\n3\nŒ≤\nmissing\nprec: maintain\n-439.321\n-245.864\n\n\n4\nŒ≤\nmissing\nload: yes\n46.0262\n107.511\n\n\n5\nœÉ\nitem\n(Intercept)\n261.196\n448.512\n\n\n6\nœÉ\nitem\nprec: maintain\n175.489\n312.049\n\n\n7\nœÅ\nitem\n(Intercept), prec: maintain\n-0.897984\n-0.445598\n\n\n8\nœÉ\nsubj\n(Intercept)\n228.099\n357.789\n\n\n9\nœÉ\nresidual\nmissing\n655.249\n701.497\n\n\n\n\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure¬†1.\n\n\nCode\ndraw(\n  data(kbm02samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\ndraw(\n  data(\n    filter(\n      :column =&gt; ==(Symbol(\"(Intercept)\")), DataFrame(kbm02samp.œÉs)\n    ),\n  ) *\n  mapping(:œÉ; color=:group) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(filter(:type =&gt; ==(\"œÅ\"), DataFrame(kbm02samp.allpars))) *\n  mapping(:value =&gt; \"Correlation\"; color=:names) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†3: Density plot of correlation parameters in bootstrap sample from model kbm02\n\n\n\n\n\n\n3 References\n\n\nKronm√ºller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436‚Äì455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "",
    "text": "We take the kwdyz11.arrow dataset (Kliegl et al., 2010) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15.arrow from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl."
  },
  {
    "objectID": "kwdyz11.html#residual-over-fitted-plot",
    "href": "kwdyz11.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "5.1 Residual-over-fitted plot",
    "text": "5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f =&gt; \"Fitted values\", :r =&gt; \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\nFigure¬†3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f =&gt; \"Fitted log response time\", :r =&gt; \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†4: Heatmap of residuals versus fitted values for model m1"
  },
  {
    "objectID": "kwdyz11.html#q-q-plot",
    "href": "kwdyz11.html#q-q-plot",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "5.2 Q-Q plot",
    "text": "5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)"
  },
  {
    "objectID": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "href": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "5.3 Observed and theoretical normal distribution",
    "text": "5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure¬†5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value =&gt; \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure¬†5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian"
  },
  {
    "objectID": "kwdyz11.html#overlay",
    "href": "kwdyz11.html#overlay",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "6.1 Overlay",
    "text": "6.1 Overlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done"
  },
  {
    "objectID": "kwdyz11.html#caterpillar-plot",
    "href": "kwdyz11.html#caterpillar-plot",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "6.2 Caterpillar plot",
    "text": "6.2 Caterpillar plot\nThe caterpillar plot, Figure¬†6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n\n\nFigure¬†6: Prediction intervals on the random effects for Subj in model m1"
  },
  {
    "objectID": "kwdyz11.html#shrinkage-plot",
    "href": "kwdyz11.html#shrinkage-plot",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "6.3 Shrinkage plot",
    "text": "6.3 Shrinkage plot\nFigure¬†7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n\n\nFigure¬†7: Shrinkage plot of the conditional means of the random effects for model m1"
  },
  {
    "objectID": "kwdyz11.html#generate-a-bootstrap-sample",
    "href": "kwdyz11.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.1 Generate a bootstrap sample",
    "text": "7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1; hide_progress=true)\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n10√ó5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nŒ≤\nmissing\n(Intercept)\n5.93392\n\n\n2\n1\nŒ≤\nmissing\nCTR: sod\n0.0864217\n\n\n3\n1\nŒ≤\nmissing\nCTR: dos\n0.048889\n\n\n4\n1\nŒ≤\nmissing\nCTR: dod\n-0.0121835\n\n\n5\n1\nœÉ\nSubj\n(Intercept)\n0.132943\n\n\n6\n1\nœÉ\nSubj\nCTR: sod\n0.0497383\n\n\n7\n1\nœÅ\nSubj\n(Intercept), CTR: sod\n0.604981\n\n\n8\n1\nœÉ\nSubj\nCTR: dos\n0.0278819\n\n\n9\n1\nœÅ\nSubj\n(Intercept), CTR: dos\n-0.254403\n\n\n10\n1\nœÅ\nSubj\nCTR: sod, CTR: dos\n0.054989\n\n\n\n\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters\n\n37500"
  },
  {
    "objectID": "kwdyz11.html#shortest-coverage-interval",
    "href": "kwdyz11.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.2 Shortest coverage interval",
    "text": "7.2 Shortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nDataFrame(shortestcovint(samp))\n\n\n15√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n5.89917\n5.97256\n\n\n2\nŒ≤\nmissing\nCTR: sod\n0.0719305\n0.104588\n\n\n3\nŒ≤\nmissing\nCTR: dos\n0.0251176\n0.049167\n\n\n4\nŒ≤\nmissing\nCTR: dod\n-0.0207177\n0.00268285\n\n\n5\nœÉ\nSubj\n(Intercept)\n0.116567\n0.168574\n\n\n6\nœÉ\nSubj\nCTR: sod\n0.0455205\n0.0708044\n\n\n7\nœÅ\nSubj\n(Intercept), CTR: sod\n0.243756\n0.712983\n\n\n8\nœÉ\nSubj\nCTR: dos\n0.00941033\n0.040979\n\n\n9\nœÅ\nSubj\n(Intercept), CTR: dos\n-0.921164\n0.246926\n\n\n10\nœÅ\nSubj\nCTR: sod, CTR: dos\n-0.724795\n0.48168\n\n\n11\nœÉ\nSubj\nCTR: dod\n0.0133659\n0.0375967\n\n\n12\nœÅ\nSubj\n(Intercept), CTR: dod\n-0.13183\n0.739257\n\n\n13\nœÅ\nSubj\nCTR: sod, CTR: dod\n0.573682\n0.999995\n\n\n14\nœÅ\nSubj\nCTR: dos, CTR: dod\n-0.855027\n0.464009\n\n\n15\nœÉ\nresidual\nmissing\n0.19051\n0.19359"
  },
  {
    "objectID": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "7.3 Comparative density plots of bootstrapped parameter estimates"
  },
  {
    "objectID": "kwdyz11.html#residual",
    "href": "kwdyz11.html#residual",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.4 Residual",
    "text": "7.4 Residual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\", ismissing(:names))) *\n  mapping(:value =&gt; \"Residual standard deviation\") *\n  density();\n)\n\n\n\n\n\nFigure¬†8: ?(caption)"
  },
  {
    "objectID": "kwdyz11.html#fixed-effects-wo-gm",
    "href": "kwdyz11.html#fixed-effects-wo-gm",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.5 Fixed effects (w/o GM)",
    "text": "7.5 Fixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not inlcude its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept)\" =&gt; \"grand mean\",\n]\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value =&gt; \"Experimental effect size [ms]\";\n    color=:names =&gt; renamer(labels) =&gt; \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals."
  },
  {
    "objectID": "kwdyz11.html#variance-components-vcs",
    "href": "kwdyz11.html#variance-components-vcs",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.6 Variance components (VCs)",
    "text": "7.6 Variance components (VCs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\")) *\n  mapping(\n    :value =&gt; \"Standard deviations [ms]\";\n    color=:names =&gt; renamer(labels) =&gt; \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†10: Comparative density plots of the variance components for model m1\n\n\n\n\nThe VC are all very nicely defined."
  },
  {
    "objectID": "kwdyz11.html#correlation-parameters-cps",
    "href": "kwdyz11.html#correlation-parameters-cps",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "7.7 Correlation parameters (CPs)",
    "text": "7.7 Correlation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n    \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n    \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n    \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n  ]\n  draw(\n    data(@subset(dat2, :type == \"œÅ\")) *\n    mapping(\n      :value =&gt; \"Correlation\";\n      color=:names =&gt; renamer(labels) =&gt; \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\nFigure¬†11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015)."
  },
  {
    "objectID": "pkg.html",
    "href": "pkg.html",
    "title": "Package management and reproducible environments",
    "section": "",
    "text": "Julius Krumbiegel also has a great blog post with more details on Julia environments.\nJulia packages can be configured (in a file called Project.toml) on a per-project basis. The packaged sources and compiled versions are stored in a central location, e.g.¬†~/.julia/packages and ~/.julia/compiled on Linux systems, but the configuration of packages to be used can be local to a project. The Pkg package is used to modify the local project‚Äôs configuration. (An alternative is ‚Äúpackage mode‚Äù in the read-eval-print-loop or REPL, which we will show at the summer school.) Start julia in the directory of the cloned SMLP2022 repository\n\nusing Pkg        # there's a package called 'Pkg' to manipulate package configs\nPkg.activate(\".\")# activate the current directory as the project\n\nIf you‚Äôve recieved an environment from someone/somwhere else ‚Äì such as this course repository ‚Äì then you‚Äôll need to first ‚Äúinstantiate‚Äù it (i.e., install all the dependencies).\n\nPkg.instantiate()# only needed the first time you work in a project\nPkg.update()     # get the latest package versions compatible with the project\n\n\nPkg.status()\n\nOccasionally the Pkg.status function call will give info about new versions being available but blocked by requirements of other packages. This is to be expected - the package system is large and the web of dependencies are complex. Generally the Julia package system is very good at resolving dependencies.\n\n\n\n Back to top"
  },
  {
    "objectID": "shrinkageplot.html",
    "href": "shrinkageplot.html",
    "title": "More on shrinkage plots",
    "section": "",
    "text": "I have stated that the likelihood criterion used to fit linear mixed-effects can be considered as balancing fidelity to the data (i.e.¬†fits the observed data well) versus model complexity.\nThis is similar to some of the criterion used in Machine Learning (ML), except that the criterion for LMMs has a rigorous mathematical basis.\nIn the shrinkage plot we consider the values of the random-effects coefficients for the fitted values of the model versus those from a model in which there is no penalty for model complexity.\nIf there is strong subject-to-subject variation then the model fit will tend to values of the random effects similar to those without a penalty on complexity.\nIf the random effects term is not contributing much (i.e.¬†it is ‚Äúinert‚Äù) then the random effects will be shrunk considerably towards zero in some directions.\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing ProgressMeter\n\nProgressMeter.ijulia_behavior(:clear);\nLoad the kb07 data set (don‚Äôt tell Reinhold that I used these data).\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\ncontrasts = Dict(\n  :subj =&gt; Grouping(),\n  :item =&gt; Grouping(),\n  :spkr =&gt; HelmertCoding(),\n  :prec =&gt; HelmertCoding(),\n  :load =&gt; HelmertCoding(),\n)\nm1 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 799   Time: 0:00:01 ( 1.78 ms/it)\n  objective:  28637.123623229592\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n\n\n(Intercept)\n2181.6729\n77.3136\n28.22\n&lt;1e-99\n301.8062\n362.2579\n\n\nspkr: old\n67.7491\n18.2664\n3.71\n0.0002\n42.3795\n40.6807\n\n\nprec: maintain\n-333.9205\n47.1558\n-7.08\n&lt;1e-11\n61.9630\n246.9158\n\n\nload: yes\n78.7702\n19.5298\n4.03\n&lt;1e-04\n64.9751\n42.3890\n\n\nspkr: old & prec: maintain\n-21.9655\n15.8074\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3837\n15.8074\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5333\n15.8074\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6073\n15.8074\n1.49\n0.1353\n\n\n\n\nResidual\n668.5542\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91087.0055\n301.8062\n\n\n\n\n\n\nspkr: old\n1796.0221\n42.3795\n+0.79\n\n\n\n\n\nprec: maintain\n3839.4126\n61.9630\n-0.59\n+0.02\n\n\n\n\nload: yes\n4221.7638\n64.9751\n+0.36\n+0.85\n+0.54\n\n\nitem\n(Intercept)\n131230.7914\n362.2579\n\n\n\n\n\n\nspkr: old\n1654.9232\n40.6807\n+0.44\n\n\n\n\n\nprec: maintain\n60967.4037\n246.9158\n-0.69\n+0.35\n\n\n\n\nload: yes\n1796.8284\n42.3890\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446964.7062\n668.5542\nissingular(m1)\n\ntrue\nprint(m1)\n\nLinear mixed model fit by maximum likelihood\n rt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n    logLik   -2 logLik      AIC         AICc        BIC     \n -14318.5618  28637.1236  28695.1236  28696.1128  28854.3166\n\nVariance components:\n             Column       Variance  Std.Dev.   Corr.\nsubj     (Intercept)      91087.0055 301.8062\n         spkr: old         1796.0221  42.3795 +0.79\n         prec: maintain    3839.4126  61.9630 -0.59 +0.02\n         load: yes         4221.7638  64.9751 +0.36 +0.85 +0.54\nitem     (Intercept)     131230.7914 362.2579\n         spkr: old         1654.9232  40.6807 +0.44\n         prec: maintain   60967.4037 246.9158 -0.69 +0.35\n         load: yes         1796.8284  42.3890 +0.32 +0.16 -0.14\nResidual                 446964.7062 668.5542\n Number of obs: 1789; levels of grouping factors: 56, 32\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                             Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)                             2181.67        77.3136  28.22    &lt;1e-99\nspkr: old                                 67.7491      18.2664   3.71    0.0002\nprec: maintain                          -333.921       47.1558  -7.08    &lt;1e-11\nload: yes                                 78.7702      19.5298   4.03    &lt;1e-04\nspkr: old & prec: maintain               -21.9655      15.8074  -1.39    0.1647\nspkr: old & load: yes                     18.3837      15.8074   1.16    0.2448\nprec: maintain & load: yes                 4.53327     15.8074   0.29    0.7743\nspkr: old & prec: maintain & load: yes    23.6073      15.8074   1.49    0.1353\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  },
  {
    "objectID": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "href": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "title": "More on shrinkage plots",
    "section": "1 Expressing the covariance of random effects",
    "text": "1 Expressing the covariance of random effects\nEarlier today we mentioned that the parameters being optimized are from a ‚Äúmatrix square root‚Äù of the covariance matrix for the random effects. There is one such lower triangular matrix for each grouping factor.\n\nl1 = first(m1.Œª)   # Cholesky factor of relative covariance for subj\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.451431    ‚ãÖ           ‚ãÖ            ‚ãÖ \n  0.0502903  0.0385895    ‚ãÖ            ‚ãÖ \n -0.0550269  0.0745788   0.0           ‚ãÖ \n  0.035189   0.090593   -0.000214038  0.0\n\n\nNotice the zero on the diagonal. A triangular matrix with zeros on the diagonal is singular.\n\nl2 = last(m1.Œª)    # this one is not singular\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.541853    ‚ãÖ           ‚ãÖ          ‚ãÖ \n  0.0268744  0.0545925    ‚ãÖ          ‚ãÖ \n -0.253091   0.268002    0.0228756   ‚ãÖ \n  0.0199967  0.00128943  0.0601543  0.0\n\n\nTo regenerate the covariance matrix we need to know that the covariance is not the square of l1, it is l1 * l1' (so that the result is symmetric) and multiplied by œÉÃÇ¬≤\n\nŒ£‚ÇÅ = varest(m1) .* (l1 * l1')\n\n4√ó4 Matrix{Float64}:\n  91087.0   10147.3     -11103.0     7100.22\n  10147.3    1796.02        49.4502  2353.54\n -11103.0      49.4502    3839.41    2154.36\n   7100.22   2353.54      2154.36    4221.76\n\n\n\ndiag(Œ£‚ÇÅ)  # compare to the variance column in the VarCorr output\n\n4-element Vector{Float64}:\n 91087.0054784778\n  1796.0221392123499\n  3839.4125796885137\n  4221.763756192171\n\n\n\nsqrt.(diag(Œ£‚ÇÅ))\n\n4-element Vector{Float64}:\n 301.8062383027856\n  42.37950140353647\n  61.96299363078348\n  64.9751010479566"
  },
  {
    "objectID": "shrinkageplot.html#shrinkage-plots",
    "href": "shrinkageplot.html#shrinkage-plots",
    "title": "More on shrinkage plots",
    "section": "2 Shrinkage plots",
    "text": "2 Shrinkage plots\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\nFigure¬†1: Shrinkage plot of model m1\n\n\n\n\nThe upper left panel shows the perfect negative correlation for those two components of the random effects.\n\nshrinkageplot(m1, :item)\n\n\n\n\n\nX1 = Int.(m1.X')\n\n8√ó1789 Matrix{Int64}:\n  1   1   1   1   1  1   1   1   1   1  ‚Ä¶   1   1   1   1   1   1   1  1   1\n -1   1   1  -1  -1  1   1  -1  -1   1      1  -1  -1   1   1  -1  -1  1   1\n -1   1  -1   1  -1  1  -1   1  -1   1     -1   1  -1   1  -1   1  -1  1  -1\n  1  -1  -1  -1  -1  1   1   1   1  -1      1   1   1  -1  -1  -1  -1  1   1\n  1   1  -1  -1   1  1  -1  -1   1   1     -1  -1   1   1  -1  -1   1  1  -1\n -1  -1  -1   1   1  1   1  -1  -1  -1  ‚Ä¶   1  -1  -1  -1  -1   1   1  1   1\n -1  -1   1  -1   1  1  -1   1  -1  -1     -1   1  -1  -1   1  -1   1  1  -1\n  1  -1   1   1  -1  1  -1  -1   1  -1     -1  -1   1  -1   1   1  -1  1  -1\n\n\n\nX1 * X1'\n\n8√ó8 Matrix{Int64}:\n 1789    -1    -1     3    -3     1     1     3\n   -1  1789    -3     1    -1     3     3     1\n   -1    -3  1789     1    -1     3     3     1\n    3     1     1  1789     3    -1    -1    -3\n   -3    -1    -1     3  1789     1     1     3\n    1     3     3    -1     1  1789    -3    -1\n    1     3     3    -1     1    -3  1789    -1\n    3     1     1    -3     3    -1    -1  1789"
  },
  {
    "objectID": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "href": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "title": "More on shrinkage plots",
    "section": "3 How to interpret a shrinkage plot",
    "text": "3 How to interpret a shrinkage plot\n\nExtreme shrinkage (shrunk to a line or to a point) is easy to interpret - the term is not providing benefit and can be removed.\nWhen the range of the blue dots (shrunk values) is comparable to those of the red dots (unshrunk) it indicates that the term after shrinkage is about as strong as without shrinkage.\nBy itself, this doesn‚Äôt mean that the term is important. In some ways you need to get a feeling for the absolute magnitude of the random effects in addition to the relative magnitude.\nSmall magnitude and small relative magnitude indicate you can drop that term"
  },
  {
    "objectID": "shrinkageplot.html#conclusions-from-these-plots",
    "href": "shrinkageplot.html#conclusions-from-these-plots",
    "title": "More on shrinkage plots",
    "section": "4 Conclusions from these plots",
    "text": "4 Conclusions from these plots\n\nOnly the intercept for the subj appears to be contributing explanatory power\nFor the item both the intercept and the spkr appear to be contributing\n\n\nm2 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec * spkr * load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n&lt;1e-99\n364.7286\n298.1109\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n&lt;1e-11\n252.6687\n\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nprec: maintain & spkr: old\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & spkr: old & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.918\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.080\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m2)\n\n\n\n\n\nFigure¬†2: Shrinkage plot of model m2\n\n\n\n\n\nm3 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.242\n364.713\n\n\n\n\nprec: maintain\n63766.935\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.438\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\nrng = Random.seed!(1234321);\n\n\nm3btstrp = parametricbootstrap(rng, 2000, m3);\n\n\nDataFrame(shortestcovint(m3btstrp))\n\n9√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2013.95\n2319.53\n\n\n2\nŒ≤\nmissing\nprec: maintain\n-429.807\n-241.429\n\n\n3\nŒ≤\nmissing\nspkr: old\n35.3339\n95.7272\n\n\n4\nŒ≤\nmissing\nload: yes\n47.067\n111.04\n\n\n5\nœÉ\nitem\n(Intercept)\n267.788\n452.9\n\n\n6\nœÉ\nitem\nprec: maintain\n171.547\n314.702\n\n\n7\nœÅ\nitem\n(Intercept), prec: maintain\n-0.89308\n-0.457084\n\n\n8\nœÉ\nsubj\n(Intercept)\n235.922\n364.717\n\n\n9\nœÉ\nresidual\nmissing\n657.736\n703.054\n\n\n\n\n\n\n\nridgeplot(m3btstrp)\n\n\n\n\nFigure¬†3: Ridge plot of the fixed-effects coefficients from the bootstrap sample\n\n\n\n\n\n\nridgeplot(m3btstrp; show_intercept=false)\nFigure¬†4: ?(caption)\n\n\n\nm4 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 + prec | item) + (1 | subj)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nm4bstrp = parametricbootstrap(rng, 2000, m4);\n\n\nridgeplot(m4bstrp; show_intercept=false)\n\n\n\n\n\nDataFrame(shortestcovint(m4bstrp))\n\n9√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2008.72\n2319.8\n\n\n2\nŒ≤\nmissing\nprec: maintain\n-433.808\n-248.858\n\n\n3\nŒ≤\nmissing\nspkr: old\n35.4729\n97.9576\n\n\n4\nŒ≤\nmissing\nload: yes\n47.0078\n108.437\n\n\n5\nœÉ\nitem\n(Intercept)\n261.52\n444.426\n\n\n6\nœÉ\nitem\nprec: maintain\n177.436\n318.846\n\n\n7\nœÅ\nitem\n(Intercept), prec: maintain\n-0.898508\n-0.477346\n\n\n8\nœÉ\nsubj\n(Intercept)\n229.03\n356.407\n\n\n9\nœÉ\nresidual\nmissing\n656.918\n701.946\n\n\n\n\n\n\n\nVarCorr(m4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.242\n364.713\n\n\n\n\nprec: maintain\n63766.935\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.438\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\n\nCode\nlet mods = [m1, m2, m4]\n  DataFrame(;\n    geomdof=(sum ‚àò leverage).(mods),\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\ngeomdof\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n131.284\n29\n28637.1\n28695.1\n28854.3\n28696.1\n\n\n2\n107.543\n13\n28658.5\n28684.5\n28755.8\n28684.7\n\n\n3\n103.478\n9\n28663.9\n28681.9\n28731.3\n28682.0\n\n\n\n\n\n\n\nscatter(fitted(m4), residuals(m4))\n\n\n\n\nFigure¬†5: Residuals versus fitted values for model m4"
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ‚ÅÑ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\n\n\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\n\n\n\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\n\n\n\nThe authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\n\n\n\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#design",
    "href": "sleepstudy_speed.html#design",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00)."
  },
  {
    "objectID": "sleepstudy_speed.html#test-schedule-within-days",
    "href": "sleepstudy_speed.html#test-schedule-within-days",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper."
  },
  {
    "objectID": "sleepstudy_speed.html#statistical-analyses",
    "href": "sleepstudy_speed.html#statistical-analyses",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups."
  },
  {
    "objectID": "sleepstudy_speed.html#current-data",
    "href": "sleepstudy_speed.html#current-data",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-simple-regressions",
    "href": "sleepstudy_speed.html#within-subject-simple-regressions",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "5.1 Within-subject simple regressions",
    "text": "5.1 Within-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] =&gt; simplelinreg =&gt; :coef)\n\n18√ó2 DataFrame\n\n\n\nRow\nSubj\ncoef\n\n\n\nString\nTuple‚Ä¶\n\n\n\n\n1\nS308\n(3.94806, -0.194812)\n\n\n2\nS309\n(4.87022, -0.0475185)\n\n\n3\nS310\n(4.90606, -0.120054)\n\n\n4\nS330\n(3.4449, -0.0291309)\n\n\n5\nS331\n(3.47647, -0.0498047)\n\n\n6\nS332\n(3.84436, -0.105511)\n\n\n7\nS333\n(3.60159, -0.0917378)\n\n\n8\nS334\n(4.04528, -0.133527)\n\n\n9\nS335\n(3.80451, 0.0455771)\n\n\n10\nS337\n(3.34374, -0.137744)\n\n\n11\nS349\n(4.46855, -0.170885)\n\n\n12\nS350\n(4.21414, -0.20151)\n\n\n13\nS351\n(3.80469, -0.0728582)\n\n\n14\nS352\n(3.68634, -0.144957)\n\n\n15\nS369\n(3.85384, -0.120531)\n\n\n16\nS370\n(4.52679, -0.215965)\n\n\n17\nS371\n(3.853, -0.0936243)\n\n\n18\nS372\n(3.69208, -0.113292)\n\n\n\n\n\n\nFigure¬†1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n‚îå Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n‚îÇ   caller = top-level scope at In[7]:8\n‚îî @ Core ./In[7]:8\n\n\n\n\n\nFigure¬†1: Reaction speed (s‚Åª¬π) versus days of sleep deprivation by subject"
  }
]